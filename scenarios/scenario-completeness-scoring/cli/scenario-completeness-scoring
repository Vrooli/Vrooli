#!/bin/bash
# scenario-completeness-scoring - CLI for Scenario Completeness Scoring
# Provides completeness scoring with validation quality analysis

set -euo pipefail

readonly CLI_NAME="scenario-completeness-scoring"
readonly CLI_VERSION="2.0.0"
readonly CONFIG_DIR="$HOME/.${CLI_NAME}"
readonly CONFIG_FILE="$CONFIG_DIR/config.json"
readonly SCENARIO_ID="scenario-completeness-scoring"
readonly HEALTH_CHECK_TIMEOUT=60
readonly HEALTH_CHECK_INTERVAL=2

# Colors
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly CYAN='\033[0;36m'
readonly BOLD='\033[1m'
readonly DIM='\033[2m'
readonly NC='\033[0m'

# Section separators
readonly SECTION_SEP="===================================================================="
readonly SUBSECTION_SEP="â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

API_BASE=""
CONFIG_API_BASE=""
API_TOKEN=""

init_config() {
    if [[ ! -d "$CONFIG_DIR" ]]; then
        mkdir -p "$CONFIG_DIR"
    fi

    if [[ ! -f "$CONFIG_FILE" ]]; then
        cat > "$CONFIG_FILE" <<JSON
{
  "api_base": "",
  "api_token": "",
  "output_format": "human",
  "created_at": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
}
JSON
    fi
}

load_config() {
    if [[ -f "$CONFIG_FILE" ]]; then
        CONFIG_API_BASE=$(jq -r '.api_base // ""' "$CONFIG_FILE" 2>/dev/null || echo "")
        API_TOKEN=$(jq -r '.api_token // ""' "$CONFIG_FILE" 2>/dev/null || echo "")
    fi
}

detect_api_port() {
    if command -v vrooli >/dev/null 2>&1; then
        vrooli scenario port "$SCENARIO_ID" API_PORT 2>/dev/null || true
    fi
}

detect_api_base() {
    if [[ -n "${API_BASE_URL:-}" ]]; then
        echo "${API_BASE_URL%/}"
        return 0
    fi

    if [[ -n "${API_PORT:-}" ]]; then
        echo "http://${API_HOST:-localhost}:${API_PORT}/api/v1"
        return 0
    fi

    local detected_port
    detected_port="$(detect_api_port)"
    if [[ -n "$detected_port" ]]; then
        echo "http://localhost:${detected_port}/api/v1"
        return 0
    fi

    return 1
}

resolve_api_base() {
    local candidate="$1"

    if [[ -n "$candidate" && "$candidate" != "null" ]]; then
        echo "${candidate%/}"
        return 0
    fi

    local detected
    if detected=$(detect_api_base); then
        echo "$detected"
        return 0
    fi

    return 1
}

# Check if service is healthy
is_service_healthy() {
    local port="$1"
    if [[ -z "$port" ]]; then
        return 1
    fi

    local status
    status=$(curl -s -o /dev/null -w "%{http_code}" "http://localhost:${port}/health" 2>/dev/null || echo "000")
    [[ "$status" == "200" ]]
}

# Start the service if not running, and wait for it to be healthy
ensure_service_running() {
    # Check if vrooli is available
    if ! command -v vrooli >/dev/null 2>&1; then
        echo -e "${RED}Error:${NC} vrooli CLI not found. Cannot auto-start service." >&2
        return 1
    fi

    # Check if already running and healthy
    local port
    port=$(detect_api_port)
    if [[ -n "$port" ]] && is_service_healthy "$port"; then
        return 0
    fi

    # Start the service
    echo -e "${BLUE}Starting ${SCENARIO_ID} service...${NC}" >&2
    if ! vrooli scenario start "$SCENARIO_ID" >/dev/null 2>&1; then
        echo -e "${RED}Error:${NC} Failed to start ${SCENARIO_ID} service" >&2
        return 1
    fi

    # Wait for service to be healthy
    local elapsed=0
    echo -en "${DIM}Waiting for service to be ready...${NC}" >&2
    while [[ $elapsed -lt $HEALTH_CHECK_TIMEOUT ]]; do
        port=$(detect_api_port)
        if [[ -n "$port" ]] && is_service_healthy "$port"; then
            echo -e "\r${GREEN}âœ“${NC} Service ready                        " >&2
            return 0
        fi
        sleep $HEALTH_CHECK_INTERVAL
        elapsed=$((elapsed + HEALTH_CHECK_INTERVAL))
        echo -en "\r${DIM}Waiting for service to be ready... (${elapsed}s)${NC}" >&2
    done

    echo -e "\r${RED}âœ—${NC} Service failed to start within ${HEALTH_CHECK_TIMEOUT}s" >&2
    return 1
}

api_request() {
    local method="$1"
    local endpoint="$2"
    local url="${API_BASE}${endpoint}"

    if [[ -z "$API_BASE" ]]; then
        echo -e "${RED}Error:${NC} API not available. Start the scenario with: vrooli scenario start $SCENARIO_ID" >&2
        return 1
    fi

    local headers=(-H 'Content-Type: application/json')
    if [[ -n "${API_TOKEN:-}" ]]; then
        headers+=(-H "Authorization: Bearer $API_TOKEN")
    fi

    curl -s -X "$method" "${headers[@]}" "$url"
}

# ============================================================================
# Output Formatting Functions (matching completeness-output-formatter.js quality)
# ============================================================================

format_understanding_primer() {
    echo "$SECTION_SEP"
    echo "ðŸ“‹ UNDERSTANDING THIS REPORT"
    echo "$SECTION_SEP"
    echo ""
    echo "This completeness score measures how well your scenario is validated and"
    echo "implemented, not just whether basic features exist."
    echo ""
    echo "Validation penalties exist to prevent \"gaming\" behaviors observed in practice:"
    echo "  â€¢ Linking all requirements to the same few passing tests"
    echo "  â€¢ Using superficial tests that don't truly validate requirements"
    echo "  â€¢ Claiming comprehensive coverage without multi-layer validation"
    echo ""
    echo "These rules encourage proper test architecture and genuine verification."
    echo ""
}

format_validation_issues() {
    local analysis="$1"
    local verbose="${2:-false}"

    local has_issues
    has_issues=$(echo "$analysis" | jq -r '.has_issues')

    if [[ "$has_issues" != "true" ]]; then
        echo "$SECTION_SEP"
        echo -e "${GREEN}âœ… No Validation Issues Detected${NC}"
        echo "$SECTION_SEP"
        echo ""
        echo "All tests follow recommended patterns and best practices."
        echo ""
        return
    fi

    # Show primer when there are issues
    format_understanding_primer

    local total_penalty severity_upper
    total_penalty=$(echo "$analysis" | jq -r '.total_penalty')
    severity_upper=$(echo "$analysis" | jq -r '.overall_severity | ascii_upcase')

    local is_critical=false
    local is_high=false
    if [[ "$severity_upper" == "HIGH" && "$total_penalty" -ge 50 ]]; then
        is_critical=true
    elif [[ "$severity_upper" == "HIGH" && "$total_penalty" -ge 20 ]]; then
        is_high=true
    fi

    local severity_icon="âš ï¸"
    local severity_label=""
    if [[ "$is_critical" == "true" ]]; then
        severity_icon="ðŸš¨"
        severity_label="CRITICAL "
    fi

    echo "$SECTION_SEP"
    echo -e "${severity_icon} ${severity_label}VALIDATION ISSUES DETECTED"
    echo "$SECTION_SEP"
    echo ""

    if [[ "$is_critical" == "true" ]]; then
        echo -e "Overall Assessment: ${RED}HIGH SEVERITY${NC} gaming patterns detected (-${total_penalty}pts)"
        echo ""
        echo "This scenario shows signs of test gaming rather than genuine validation."
        echo "Tests appear created to inflate metrics rather than validate functionality."
        echo ""
        echo "â„¹ï¸  Gaming Prevention: These penalties detect anti-patterns where tests are created"
        echo "   to satisfy completeness scores rather than provide genuine validation."
    elif [[ "$is_high" == "true" ]]; then
        echo -e "Overall Assessment: ${YELLOW}MEDIUM-HIGH severity${NC} issues found (-${total_penalty}pts)"
        echo ""
        echo "This scenario has test quality issues that need attention."
        echo ""
        echo "â„¹ï¸  Gaming Prevention: These penalties encourage proper test structure and"
        echo "   multi-layer validation to ensure comprehensive coverage."
    else
        echo -e "Overall Assessment: ${YELLOW}MEDIUM severity${NC} issues found (-${total_penalty}pts)"
        echo ""
        echo "This scenario has a solid foundation but needs test quality improvements."
        echo ""
        echo "â„¹ï¸  Gaming Prevention: These penalties encourage best practices in test"
        echo "   organization and validation diversity."
    fi
    echo ""

    # Show high severity issues first
    local high_count
    high_count=$(echo "$analysis" | jq '[.issues[] | select(.severity == "high")] | length')

    if [[ "$high_count" -gt 0 ]]; then
        echo "Top Issues (Fix These First):"
        echo "$SUBSECTION_SEP"
        echo ""

        echo "$analysis" | jq -r '.issues[] | select(.severity == "high") | @json' | while read -r issue; do
            format_issue_detail "$issue" "$verbose"
        done
    fi

    # Show medium severity issues
    local medium_count
    medium_count=$(echo "$analysis" | jq '[.issues[] | select(.severity == "medium")] | length')

    if [[ "$medium_count" -gt 0 ]]; then
        echo "$SUBSECTION_SEP"
        echo ""
        local medium_penalty
        medium_penalty=$(echo "$analysis" | jq '[.issues[] | select(.severity == "medium") | .penalty] | add // 0')
        echo -e "ðŸŸ¡ ${high_count:+Minor }Issues (${medium_count} issues, -${medium_penalty}pts total)"
        echo ""

        if [[ "$verbose" == "true" || "$high_count" -eq 0 ]]; then
            echo "$analysis" | jq -r '.issues[] | select(.severity == "medium") | @json' | while read -r issue; do
                format_issue_detail "$issue" "$verbose"
            done
        else
            echo "$analysis" | jq -r '.issues[] | select(.severity == "medium") | @json' | while read -r issue; do
                local icon="ðŸŸ¡"
                local msg penalty
                msg=$(echo "$issue" | jq -r '.message' | head -1)
                penalty=$(echo "$issue" | jq -r '.penalty')
                echo -e "${icon} ${msg}"

                # Show valid sources if available
                local valid_sources
                valid_sources=$(echo "$issue" | jq -r '.valid_sources // [] | .[]')
                if [[ -n "$valid_sources" ]]; then
                    echo "   Valid test locations:"
                    echo "$valid_sources" | while read -r source; do
                        echo "     â€¢ $source"
                    done
                fi

                echo "   Penalty: -${penalty} pts"
                echo ""
            done
        fi
    fi

    if [[ "$verbose" != "true" ]]; then
        echo "Run with --verbose to see detailed explanations and per-requirement breakdown."
        echo ""
    fi
}

format_issue_detail() {
    local issue="$1"
    local verbose="${2:-false}"

    local severity icon msg penalty
    severity=$(echo "$issue" | jq -r '.severity')
    msg=$(echo "$issue" | jq -r '.message')
    penalty=$(echo "$issue" | jq -r '.penalty')

    if [[ "$severity" == "high" ]]; then
        icon="ðŸ”´"
    else
        icon="ðŸŸ¡"
    fi

    echo -e "${icon} ${msg}"

    # Show invalid paths if available
    local invalid_paths_count
    invalid_paths_count=$(echo "$issue" | jq '.invalid_paths | length // 0')
    if [[ "$invalid_paths_count" -gt 0 ]]; then
        echo ""
        echo "   Invalid paths found:"
        echo "$issue" | jq -r '.invalid_paths[:5][] | "     â€¢ \(.path) (referenced by \(.requirement_ids | length) requirements)"'
        if [[ "$invalid_paths_count" -gt 5 ]]; then
            echo "     ... and $((invalid_paths_count - 5)) more"
        fi
    fi

    # Show valid sources if available
    local valid_sources
    valid_sources=$(echo "$issue" | jq -r '.valid_sources // [] | .[]')
    if [[ -n "$valid_sources" ]]; then
        echo ""
        echo "   Valid test locations:"
        echo "$valid_sources" | while read -r source; do
            echo "     â€¢ $source"
        done
    fi

    # Show affected requirements if available
    local affected_count
    affected_count=$(echo "$issue" | jq '.affected_requirements | length // 0')
    if [[ "$affected_count" -gt 0 ]]; then
        echo ""
        echo "   Affected requirements (first 5):"
        echo "$issue" | jq -r '.affected_requirements[:5][] | "     â€¢ \(.id) (\(.title // "Untitled"))\n       has: \(if .current_layers then (.current_layers | join(", ")) else "none" end) â†’ needs: \(if .needed_layers then (.needed_layers | join(" + ")) else "unknown" end)"'
        if [[ "$affected_count" -gt 5 ]]; then
            echo "     ... and $((affected_count - 5)) more critical requirements"
        fi
    fi

    # Show worst offender if available
    local worst_offender
    worst_offender=$(echo "$issue" | jq -r '.worst_offender // empty')
    if [[ -n "$worst_offender" ]]; then
        echo ""
        echo "   Affected files:"
        local test_ref offender_count
        test_ref=$(echo "$issue" | jq -r '.worst_offender.test_ref')
        offender_count=$(echo "$issue" | jq -r '.worst_offender.count')
        echo "     â€¢ $test_ref (validates $offender_count requirements)"
        local violations
        violations=$(echo "$issue" | jq -r '.violations // 0')
        if [[ "$violations" -gt 1 ]]; then
            echo "     ... and $((violations - 1)) more test files"
        fi
    fi

    # Always show why it matters
    echo ""
    echo "   Why this matters:"
    echo "$issue" | jq -r '.why_it_matters // "" | split("\n")[] | "     \(.)"'

    # Show recommendation
    echo ""
    echo "   Next Steps:"
    echo "$issue" | jq -r '.recommendation // "" | "     â†’ \(.)"'

    if [[ "$verbose" == "true" ]]; then
        local description
        description=$(echo "$issue" | jq -r '.description // ""')
        if [[ -n "$description" ]]; then
            echo ""
            echo "   Background:"
            echo "   $description"
        fi
    fi

    echo ""
}

format_score_summary() {
    local data="$1"
    local validation_analysis="$2"

    local score classification base_score validation_penalty
    score=$(echo "$data" | jq -r '.score')
    classification=$(echo "$data" | jq -r '.classification')
    base_score=$(echo "$data" | jq -r '.base_score // .breakdown.base_score // 0')
    validation_penalty=$(echo "$data" | jq -r '.validation_penalty // .breakdown.validation_penalty // 0')

    echo "$SECTION_SEP"
    echo -e "ðŸ“Š COMPLETENESS SCORE: ${BOLD}${score}/100${NC} (${classification//_/ })"
    echo "$SECTION_SEP"
    echo ""
    echo "  Final Score:        ${score}/100"
    echo "  Base Score:         ${base_score}/100"

    local has_issues
    has_issues=$(echo "$validation_analysis" | jq -r '.has_issues // false')

    if [[ "$has_issues" == "true" ]]; then
        local overall_severity
        overall_severity=$(echo "$validation_analysis" | jq -r '.overall_severity')
        local severity_label=""
        if [[ "$overall_severity" == "high" ]]; then
            severity_label="âš ï¸  SEVERE"
        fi
        echo "  Validation Penalty: -${validation_penalty}pts ${severity_label}"
        echo ""
        echo "  Penalty breakdown:"
        echo "$validation_analysis" | jq -r '.issues[] | "    â€¢ \(.type | gsub("_"; " ")): -\(.penalty) pts"'
    fi

    echo ""
    echo "  Classification: ${classification//_/ }"
    echo "  Status: $(get_classification_description "$classification")"
    echo ""
}

get_classification_description() {
    local classification="$1"
    case "$classification" in
        production_ready) echo "Production ready, excellent validation coverage" ;;
        nearly_ready) echo "Nearly ready, final polish and edge cases" ;;
        mostly_complete) echo "Mostly complete, needs refinement and validation" ;;
        functional_incomplete) echo "Functional but incomplete, needs more features/tests" ;;
        foundation_laid) echo "Foundation laid, core features in progress" ;;
        early_stage) echo "Just starting, needs significant development" ;;
        *) echo "Status unclear" ;;
    esac
}

format_base_metrics() {
    local data="$1"

    local breakdown
    breakdown=$(echo "$data" | jq '.breakdown')

    # Quality Metrics
    local quality_score quality_max
    quality_score=$(echo "$breakdown" | jq -r '.quality.score')
    quality_max=$(echo "$breakdown" | jq -r '.quality.max')
    echo ""
    echo "Quality Metrics (${quality_score}/${quality_max}):"

    local req_rate req_passing req_total req_points
    req_rate=$(echo "$breakdown" | jq -r '.quality.requirement_pass_rate.rate')
    req_passing=$(echo "$breakdown" | jq -r '.quality.requirement_pass_rate.passing')
    req_total=$(echo "$breakdown" | jq -r '.quality.requirement_pass_rate.total')
    req_points=$(echo "$breakdown" | jq -r '.quality.requirement_pass_rate.points')
    local req_icon="âš ï¸ "
    [[ "$(echo "$req_rate >= 0.9" | bc -l)" == "1" ]] && req_icon="âœ…"
    local req_pct
    req_pct=$(echo "$req_rate * 100" | bc -l | cut -d. -f1)
    echo "  ${req_icon} Requirements: ${req_total} total, ${req_passing} passing (${req_pct}%) â†’ ${req_points}/20 pts$([[ "$req_pct" -lt 90 ]] && echo '  [Target: 90%+]')"

    local target_rate target_passing target_total target_points
    target_rate=$(echo "$breakdown" | jq -r '.quality.target_pass_rate.rate')
    target_passing=$(echo "$breakdown" | jq -r '.quality.target_pass_rate.passing')
    target_total=$(echo "$breakdown" | jq -r '.quality.target_pass_rate.total')
    target_points=$(echo "$breakdown" | jq -r '.quality.target_pass_rate.points')
    local target_icon="âš ï¸ "
    [[ "$(echo "$target_rate >= 0.9" | bc -l)" == "1" ]] && target_icon="âœ…"
    local target_pct
    target_pct=$(echo "$target_rate * 100" | bc -l | cut -d. -f1)
    echo "  ${target_icon} Op Targets: ${target_total} total, ${target_passing} passing (${target_pct}%) â†’ ${target_points}/15 pts$([[ "$target_pct" -lt 90 ]] && echo '  [Target: 90%+]')"

    local test_rate test_passing test_total test_points
    test_rate=$(echo "$breakdown" | jq -r '.quality.test_pass_rate.rate')
    test_passing=$(echo "$breakdown" | jq -r '.quality.test_pass_rate.passing')
    test_total=$(echo "$breakdown" | jq -r '.quality.test_pass_rate.total')
    test_points=$(echo "$breakdown" | jq -r '.quality.test_pass_rate.points')
    local test_icon="âš ï¸ "
    [[ "$(echo "$test_rate >= 0.9" | bc -l)" == "1" ]] && test_icon="âœ…"
    local test_pct
    test_pct=$(echo "$test_rate * 100" | bc -l | cut -d. -f1)
    echo "  ${test_icon} Tests: ${test_total} total, ${test_passing} passing (${test_pct}%) â†’ ${test_points}/15 pts$([[ "$test_pct" -lt 90 ]] && echo '  [Target: 90%+]')"

    # Coverage Metrics
    local coverage_score coverage_max
    coverage_score=$(echo "$breakdown" | jq -r '.coverage.score')
    coverage_max=$(echo "$breakdown" | jq -r '.coverage.max')
    echo ""
    echo "Coverage Metrics (${coverage_score}/${coverage_max}):"

    local cov_ratio cov_points
    cov_ratio=$(echo "$breakdown" | jq -r '.coverage.test_coverage_ratio.ratio')
    cov_points=$(echo "$breakdown" | jq -r '.coverage.test_coverage_ratio.points')
    local cov_icon="âš ï¸ "
    [[ "$(echo "$cov_ratio >= 2.0" | bc -l)" == "1" ]] && cov_icon="âœ…"
    local cov_display
    cov_display=$(printf "%.1f" "$cov_ratio")
    echo "  ${cov_icon} Test Coverage: ${cov_display}x â†’ ${cov_points}/8 pts$([[ "$(echo "$cov_ratio < 2.0" | bc -l)" == "1" ]] && echo '  [Target: 2.0x]')"

    local depth_avg depth_points
    depth_avg=$(echo "$breakdown" | jq -r '.coverage.depth_score.avg_depth')
    depth_points=$(echo "$breakdown" | jq -r '.coverage.depth_score.points')
    local depth_icon="âš ï¸ "
    [[ "$(echo "$depth_avg >= 3.0" | bc -l)" == "1" ]] && depth_icon="âœ…"
    local depth_display
    depth_display=$(printf "%.1f" "$depth_avg")
    echo "  ${depth_icon} Depth Score: ${depth_display} avg levels â†’ ${depth_points}/7 pts$([[ "$(echo "$depth_avg < 3.0" | bc -l)" == "1" ]] && echo '  [Target: 3.0+]')"

    # Quantity Metrics
    local quantity_score quantity_max
    quantity_score=$(echo "$breakdown" | jq -r '.quantity.score')
    quantity_max=$(echo "$breakdown" | jq -r '.quantity.max')
    echo ""
    echo "Quantity Metrics (${quantity_score}/${quantity_max}):"

    local req_count req_threshold req_qty_points
    req_count=$(echo "$breakdown" | jq -r '.quantity.requirements.count')
    req_threshold=$(echo "$breakdown" | jq -r '.quantity.requirements.threshold')
    req_qty_points=$(echo "$breakdown" | jq -r '.quantity.requirements.points')
    local req_qty_icon="âš ï¸ "
    [[ "$req_threshold" == "good" || "$req_threshold" == "excellent" ]] && req_qty_icon="âœ…"
    echo "  ${req_qty_icon} Requirements: ${req_count} (${req_threshold}) â†’ ${req_qty_points}/4 pts"

    local target_count target_threshold target_qty_points
    target_count=$(echo "$breakdown" | jq -r '.quantity.targets.count')
    target_threshold=$(echo "$breakdown" | jq -r '.quantity.targets.threshold')
    target_qty_points=$(echo "$breakdown" | jq -r '.quantity.targets.points')
    local target_qty_icon="âš ï¸ "
    [[ "$target_threshold" == "good" || "$target_threshold" == "excellent" ]] && target_qty_icon="âœ…"
    echo "  ${target_qty_icon} Targets: ${target_count} (${target_threshold}) â†’ ${target_qty_points}/3 pts"

    local tests_count tests_threshold tests_qty_points
    tests_count=$(echo "$breakdown" | jq -r '.quantity.tests.count')
    tests_threshold=$(echo "$breakdown" | jq -r '.quantity.tests.threshold')
    tests_qty_points=$(echo "$breakdown" | jq -r '.quantity.tests.points')
    local tests_qty_icon="âš ï¸ "
    [[ "$tests_threshold" == "good" || "$tests_threshold" == "excellent" ]] && tests_qty_icon="âœ…"
    echo "  ${tests_qty_icon} Tests: ${tests_count} (${tests_threshold}) â†’ ${tests_qty_points}/3 pts"

    # UI Metrics
    local ui_score ui_max
    ui_score=$(echo "$breakdown" | jq -r '.ui.score // 0')
    ui_max=$(echo "$breakdown" | jq -r '.ui.max // 25')
    if [[ "$ui_score" != "null" && "$ui_score" != "0" ]] || [[ "$ui_max" != "null" ]]; then
        echo ""
        echo "UI Metrics (${ui_score}/${ui_max}):"

        local is_template template_points
        is_template=$(echo "$breakdown" | jq -r '.ui.template_check.is_template')
        template_points=$(echo "$breakdown" | jq -r '.ui.template_check.points')
        local template_icon="âœ…"
        local template_status="Custom"
        if [[ "$is_template" == "true" ]]; then
            template_icon="âŒ"
            template_status="TEMPLATE"
        fi
        echo "  ${template_icon} Template: ${template_status} â†’ ${template_points}/10 pts$([[ "$is_template" == "true" ]] && echo '  [CRITICAL: Replace template UI]')"

        local file_count file_threshold file_points
        file_count=$(echo "$breakdown" | jq -r '.ui.component_complexity.file_count')
        file_threshold=$(echo "$breakdown" | jq -r '.ui.component_complexity.threshold')
        file_points=$(echo "$breakdown" | jq -r '.ui.component_complexity.points')
        local file_icon="âš ï¸ "
        [[ "$file_threshold" == "good" || "$file_threshold" == "excellent" ]] && file_icon="âœ…"
        echo "  ${file_icon} Files: ${file_count} files (${file_threshold}) â†’ ${file_points}/5 pts"

        local api_count api_points
        api_count=$(echo "$breakdown" | jq -r '.ui.api_integration.endpoint_count')
        api_points=$(echo "$breakdown" | jq -r '.ui.api_integration.points')
        local api_icon="âš ï¸ "
        [[ "$api_count" -ge 4 ]] && api_icon="âœ…"
        echo "  ${api_icon} API Integration: ${api_count} endpoints beyond /health â†’ ${api_points}/6 pts"

        local route_count route_points
        route_count=$(echo "$breakdown" | jq -r '.ui.routing.route_count')
        route_points=$(echo "$breakdown" | jq -r '.ui.routing.points')
        local route_icon="âš ï¸ "
        [[ "$route_count" -ge 3 ]] && route_icon="âœ…"
        echo "  ${route_icon} Routing: ${route_count} routes â†’ ${route_points}/1.5 pts"

        local loc_total loc_points
        loc_total=$(echo "$breakdown" | jq -r '.ui.code_volume.total_loc')
        loc_points=$(echo "$breakdown" | jq -r '.ui.code_volume.points')
        local loc_icon="âš ï¸ "
        [[ "$loc_total" -ge 600 ]] && loc_icon="âœ…"
        echo "  ${loc_icon} LOC: ${loc_total} total â†’ ${loc_points}/2.5 pts"
    fi
}

format_action_plan() {
    local data="$1"
    local validation_analysis="$2"

    echo ""
    echo "$SECTION_SEP"
    echo "ðŸŽ¯ RECOMMENDED ACTION PLAN"
    echo "$SECTION_SEP"
    echo ""

    local has_issues
    has_issues=$(echo "$validation_analysis" | jq -r '.has_issues // false')

    if [[ "$has_issues" == "true" ]]; then
        echo "To improve this score, fix validation issues first (highest ROI):"
        echo ""

        local phase=1

        # Check for invalid test locations
        local invalid_location
        invalid_location=$(echo "$validation_analysis" | jq -r '.issues[] | select(.type == "invalid_test_location") | @json' | head -1)
        if [[ -n "$invalid_location" ]]; then
            local count penalty
            count=$(echo "$invalid_location" | jq -r '.count // 0')
            penalty=$(echo "$invalid_location" | jq -r '.penalty')
            echo "Phase ${phase}: Fix Test Locations (+${penalty}pts estimated)"
            echo "  Current: ${count} requirements use invalid test locations"
            echo "  Target: Move all requirement validation refs to valid locations"
            echo ""
            echo "  Actions:"
            echo "    1. Audit each requirement to determine appropriate test layers:"
            echo "       - Business logic â†’ API tests (api/**/*_test.go)"
            echo "       - UI components â†’ UI tests (ui/src/**/*.test.tsx)"
            echo "       - User workflows â†’ e2e playbooks (test/playbooks/**/*.json)"
            echo ""
            echo "    2. Create tests in valid locations (or reference existing ones)"
            echo ""
            echo "    3. Update requirements/*/module.json validation refs"
            echo ""
            ((phase++))
        fi

        # Check for insufficient validation layers
        local multi_layer
        multi_layer=$(echo "$validation_analysis" | jq -r '.issues[] | select(.type == "insufficient_validation_layers") | @json' | head -1)
        if [[ -n "$multi_layer" ]]; then
            local count total penalty
            count=$(echo "$multi_layer" | jq -r '.count // 0')
            total=$(echo "$multi_layer" | jq -r '.total // 0')
            penalty=$(echo "$multi_layer" | jq -r '.penalty')
            echo "Phase ${phase}: Add Multi-Layer Validation (+${penalty}pts estimated)"
            echo "  Current: ${count}/${total} critical requirements have multi-layer validation"
            echo "  Target: All P0/P1 requirements validated at â‰¥2 layers"
            echo ""
            echo "  Actions:"
            echo "    â†’ For each P0 requirement:"
            echo "      Ensure validation at 2+ layers (API + UI, API + e2e, or all 3)"
            echo ""
            echo "    â†’ For each P1 requirement:"
            echo "      Ensure validation at 2+ layers where applicable"
            echo ""
            ((phase++))
        fi

        # Check for monolithic test files
        local monolithic
        monolithic=$(echo "$validation_analysis" | jq -r '.issues[] | select(.type == "monolithic_test_files") | @json' | head -1)
        if [[ -n "$monolithic" ]]; then
            local violations penalty
            violations=$(echo "$monolithic" | jq -r '.violations // 0')
            penalty=$(echo "$monolithic" | jq -r '.penalty')
            echo "Phase ${phase}: Create Focused Tests (+${penalty}pts estimated)"
            echo "  Current: ${violations} monolithic test files"
            echo "  Target: Focused tests per requirement"
            echo ""
            echo "  Actions:"
            echo "    â†’ Instead of test files that validate many requirements,"
            echo "      create focused tests that validate individual requirements"
            echo "    â†’ Use appropriate test types (API/UI/e2e) instead of CLI wrappers"
            echo ""
            ((phase++))
        fi

        local base_score
        base_score=$(echo "$data" | jq -r '.base_score // .breakdown.base_score // 0')
        local estimated_final=$((base_score > 100 ? 100 : base_score))
        echo "Estimated Score After Fixes: ~${estimated_final}/100"
    else
        echo "No priority actions needed. Continue maintaining quality!"
    fi
    echo ""
}

# ============================================================================
# Command Handlers
# ============================================================================

cmd_score() {
    local scenario_name="${1:-}"
    local format="human"
    local verbose=false
    local show_metrics=false

    shift || true
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --format)
                format="${2:-human}"
                shift 2
                ;;
            --json)
                format="json"
                shift
                ;;
            --verbose|-v)
                verbose=true
                shift
                ;;
            --metrics|-m)
                show_metrics=true
                verbose=true
                shift
                ;;
            --help|-h)
                cat <<'HELP'
Usage: scenario-completeness-scoring score <scenario-name> [options]

Calculate completeness score for a scenario with validation quality analysis.

Options:
  --format <type>    Output format: json (machine-readable) or human (default)
  --json             Shorthand for --format json
  --verbose, -v      Show detailed explanations and per-requirement breakdowns
  --metrics, -m      Show full detailed metrics breakdown (implies --verbose)
  --help, -h         Show this help message

Examples:
  scenario-completeness-scoring score deployment-manager
  scenario-completeness-scoring score deployment-manager --verbose
  scenario-completeness-scoring score deployment-manager --metrics
  scenario-completeness-scoring score deployment-manager --json

Score Components:
  Quality (50%):  Requirement, target, and test pass rates
  Coverage (15%): Test-to-requirement ratio, validation depth
  Quantity (10%): Absolute counts vs category thresholds
  UI (25%):       Template detection, component complexity, API integration

Validation Quality:
  The score includes penalties for test anti-patterns:
  â€¢ Invalid test locations        (up to -25pts)
  â€¢ Monolithic test files         (up to -15pts)
  â€¢ Ungrouped operational targets (up to -10pts)
  â€¢ Insufficient validation layers(up to -20pts)
  â€¢ Superficial test implementation (up to -10pts)
  â€¢ Missing test automation       (up to -15pts)
  â€¢ Suspicious 1:1 ratio          (-5pts)

HELP
                return 0
                ;;
            *)
                echo -e "${RED}Unknown option: $1${NC}" >&2
                return 1
                ;;
        esac
    done

    if [[ -z "$scenario_name" ]]; then
        echo -e "${RED}Error: Scenario name required${NC}" >&2
        echo "Usage: $CLI_NAME score <scenario-name> [--format json|human] [--verbose] [--metrics]" >&2
        return 1
    fi

    # Ensure API is available
    if [[ -z "$API_BASE" ]]; then
        if ! API_BASE=$(resolve_api_base "$CONFIG_API_BASE"); then
            echo -e "${RED}Error:${NC} API not available. Start the scenario with: vrooli scenario start $SCENARIO_ID" >&2
            return 1
        fi
    fi

    # Fetch score from API
    local response
    response=$(api_request GET "/scores/${scenario_name}")

    if [[ -z "$response" ]]; then
        echo -e "${RED}Error: Failed to get score for ${scenario_name}${NC}" >&2
        return 1
    fi

    # Check for error response
    local error_msg
    error_msg=$(echo "$response" | jq -r '.error // empty' 2>/dev/null)
    if [[ -n "$error_msg" ]]; then
        echo -e "${RED}Error: ${error_msg}${NC}" >&2
        return 1
    fi

    if [[ "$format" == "json" ]]; then
        echo "$response" | jq '.'
        return 0
    fi

    # Human-readable output
    local validation_analysis
    validation_analysis=$(echo "$response" | jq '.validation_analysis // {has_issues: false, issues: [], total_penalty: 0}')

    # Format validation issues first (if any)
    format_validation_issues "$validation_analysis" "$verbose"

    # Score summary
    format_score_summary "$response" "$validation_analysis"

    # Base metrics
    format_base_metrics "$response"

    # Action plan
    format_action_plan "$response" "$validation_analysis"

    # Comparison context
    local total_penalty
    total_penalty=$(echo "$validation_analysis" | jq -r '.total_penalty // 0')
    local score
    score=$(echo "$response" | jq -r '.score')

    echo "$SECTION_SEP"
    echo ""

    if [[ "$total_penalty" -gt 50 ]]; then
        echo "ðŸŽ“ Study browser-automation-studio as reference for proper test structure:"
        echo "   â€¢ Has API tests: api/**/*_test.go"
        echo "   â€¢ Has UI tests: ui/src/**/*.test.tsx"
        echo "   â€¢ Has e2e playbooks: test/playbooks/capabilities/**/ui/*.json"
        echo "   â€¢ Requirements reference appropriate test types"
    elif [[ "$score" -ge 80 && "$total_penalty" -lt 10 ]]; then
        echo "ðŸŒŸ Excellent work! This scenario demonstrates:"
        echo "   âœ“ Comprehensive multi-layer testing"
        echo "   âœ“ Proper test organization"
        echo "   âœ“ High pass rates across all metrics"
        echo "   âœ“ Minimal gaming patterns detected"
    elif [[ "$score" -ge 40 && "$total_penalty" -lt 30 ]]; then
        echo "âœ¨ This scenario has good test structure - continue improving:"
        echo "   â€¢ Proper use of test/playbooks/ for e2e testing"
        echo "   â€¢ Good mix of test types where present"
        echo "   â€¢ Focus on increasing test coverage and pass rates"
    fi
    echo ""
}

cmd_status() {
    echo -e "${BLUE}Checking system health...${NC}"
    response=$(api_request GET "/health" || true)
    if [[ -z "$response" ]]; then
        echo -e "${RED}âœ—${NC} API is not reachable"
        return 1
    fi

    status=$(echo "$response" | jq -r '.status // "unknown"' 2>/dev/null || echo "unknown")
    if [[ "$status" == "healthy" ]]; then
        echo -e "${GREEN}âœ“${NC} Service is healthy"
    else
        echo -e "${YELLOW}âš ${NC} Service status: $status"
    fi

    echo "$response" | jq -r '
        "  Service: \(.service // "unknown")",
        "  Version: \(.version // "unknown")"
    ' 2>/dev/null || true
}

cmd_configure() {
    local key="${1:-}"
    local value="${2:-}"

    if [[ -z "$key" ]]; then
        cat "$CONFIG_FILE" | jq '.' 2>/dev/null || cat "$CONFIG_FILE"
        return 0
    fi

    case "$key" in
        api|api_base)
            jq --arg v "$value" '.api_base = $v' "$CONFIG_FILE" > "${CONFIG_FILE}.tmp" && mv "${CONFIG_FILE}.tmp" "$CONFIG_FILE"
            echo -e "${GREEN}âœ“${NC} API base set to: $value"
            ;;
        token|api_token)
            jq --arg v "$value" '.api_token = $v' "$CONFIG_FILE" > "${CONFIG_FILE}.tmp" && mv "${CONFIG_FILE}.tmp" "$CONFIG_FILE"
            echo -e "${GREEN}âœ“${NC} API token updated"
            ;;
        *)
            echo -e "${RED}âœ—${NC} Unknown configuration key: $key" >&2
            echo "Valid keys: api_base, api_token" >&2
            return 1
            ;;
    esac
}

cmd_version() {
    echo "$CLI_NAME version $CLI_VERSION"
    if [[ -n "$API_BASE" ]]; then
        echo "API endpoint: $API_BASE"
    else
        echo "API endpoint: (auto-detect when scenario is running)"
    fi
}

cmd_help() {
    cat <<'EOF'
scenario-completeness-scoring - CLI for Scenario Completeness Scoring

Usage: scenario-completeness-scoring <command> [options]

Commands:
    score <scenario>    Calculate completeness score with validation analysis
    status              Check API and dependency health
    configure           View or update CLI configuration
    version             Show CLI version
    help                Show this help message

Score Command Options:
    --format <type>     Output format: json or human (default)
    --json              Shorthand for --format json
    --verbose, -v       Show detailed explanations
    --metrics, -m       Show full metrics breakdown

Examples:
    scenario-completeness-scoring score deployment-manager
    scenario-completeness-scoring score deployment-manager --verbose
    scenario-completeness-scoring score deployment-manager --json

The score command provides:
  â€¢ Completeness scoring across 4 dimensions (Quality, Coverage, Quantity, UI)
  â€¢ Validation quality analysis detecting 7 anti-patterns
  â€¢ Actionable recommendations for improvement
  â€¢ Human-readable output with visual indicators

EOF
}

main() {
    init_config
    load_config

    local command="${1:-help}"
    shift || true

    # Set up environment variables
    local env_prefix
    env_prefix=$(echo "$CLI_NAME" | tr '[:lower:]' '[:upper:]' | tr -c 'A-Z0-9' '_')
    local env_api_base_var="${env_prefix}_API_BASE"
    local env_api_token_var="${env_prefix}_API_TOKEN"

    local env_api_base="${!env_api_base_var:-}"
    local env_api_token="${!env_api_token_var:-}"

    if [[ -n "$env_api_token" ]]; then
        API_TOKEN="$env_api_token"
    fi

    if [[ -n "$env_api_base" ]]; then
        API_BASE="${env_api_base%/}"
    fi

    # For commands that need the API, ensure service is running
    if [[ "$command" == "score" || "$command" == "status" ]]; then
        if [[ -z "$API_BASE" ]]; then
            # Try to resolve existing API
            if ! API_BASE=$(resolve_api_base "$CONFIG_API_BASE" 2>/dev/null); then
                # Service not running - auto-start it
                if ! ensure_service_running; then
                    exit 1
                fi
                # Now try to resolve again
                if ! API_BASE=$(resolve_api_base "$CONFIG_API_BASE" 2>/dev/null); then
                    echo -e "${RED}Error:${NC} Failed to connect to service after starting" >&2
                    exit 1
                fi
            fi
        fi
    fi

    case "$command" in
        score)
            cmd_score "$@"
            ;;
        status)
            cmd_status
            ;;
        configure|config)
            cmd_configure "$@"
            ;;
        version|--version|-v)
            cmd_version
            ;;
        help|--help|-h)
            cmd_help
            ;;
        *)
            echo -e "${RED}âœ—${NC} Unknown command: $command" >&2
            echo "Run '$CLI_NAME help' for usage information" >&2
            exit 1
            ;;
    esac
}

main "$@"
