{
  "name": "Data Backup Orchestrator",
  "nodes": [
    {
      "parameters": {},
      "name": "Start",
      "type": "n8n-nodes-base.start",
      "typeVersion": 1,
      "position": [240, 300]
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "backup/trigger",
        "responseMode": "responseNode",
        "options": {}
      },
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [460, 300],
      "webhookId": "data-backup-manager-orchestrator"
    },
    {
      "parameters": {
        "functionCode": "// Extract backup parameters from request\nconst backupType = $json.type || 'full';\nconst targets = $json.targets || ['postgres'];\nconst retention = $json.retention_days || 7;\nconst description = $json.description || 'Automated backup';\n\n// Create job ID\nconst jobId = `backup-${Date.now()}`;\n\n// Return processed parameters\nreturn [{\n  json: {\n    job_id: jobId,\n    type: backupType,\n    targets: targets,\n    retention_days: retention,\n    description: description,\n    status: 'pending',\n    started_at: new Date().toISOString()\n  }\n}];"
      },
      "name": "Process Request",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [680, 300]
    },
    {
      "parameters": {
        "operation": "insert",
        "table": "backup_jobs",
        "columns": "id,type,target,target_identifier,status,description,retention_until",
        "additionalFields": {},
        "options": {}
      },
      "name": "Create Job Record",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 1,
      "position": [900, 300],
      "credentials": {
        "postgres": {
          "id": "postgres-main",
          "name": "PostgreSQL Main"
        }
      }
    },
    {
      "parameters": {
        "conditions": {
          "string": [
            {
              "value1": "={{$json.targets}}",
              "operation": "contains",
              "value2": "postgres"
            }
          ]
        }
      },
      "name": "Check Postgres Target",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [1120, 200]
    },
    {
      "parameters": {
        "command": "pg_dump --verbose --clean --if-exists --no-owner --no-privileges --format=custom {{$json.database_name || 'postgres'}} | gzip > /tmp/{{$json.job_id}}_postgres.sql.gz"
      },
      "name": "Backup PostgreSQL",
      "type": "n8n-nodes-base.executeCommand",
      "typeVersion": 1,
      "position": [1340, 180]
    },
    {
      "parameters": {
        "conditions": {
          "string": [
            {
              "value1": "={{$json.targets}}",
              "operation": "contains",
              "value2": "files"
            }
          ]
        }
      },
      "name": "Check Files Target",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [1120, 400]
    },
    {
      "parameters": {
        "command": "tar -czf /tmp/{{$json.job_id}}_files.tar.gz -C {{$json.backup_root || '/home/matthalloran8/Vrooli'}} scenarios/ resources/ data/"
      },
      "name": "Backup Files",
      "type": "n8n-nodes-base.executeCommand",
      "typeVersion": 1,
      "position": [1340, 380]
    },
    {
      "parameters": {
        "functionCode": "// Calculate file checksums and sizes\nconst fs = require('fs');\nconst crypto = require('crypto');\nconst jobId = $json.job_id;\nconst files = [];\n\n// Check for postgres backup\nconst pgFile = `/tmp/${jobId}_postgres.sql.gz`;\nif (fs.existsSync(pgFile)) {\n  const pgStats = fs.statSync(pgFile);\n  const pgHash = crypto.createHash('sha256');\n  const pgData = fs.readFileSync(pgFile);\n  pgHash.update(pgData);\n  \n  files.push({\n    path: pgFile,\n    type: 'postgres',\n    size: pgStats.size,\n    checksum: pgHash.digest('hex')\n  });\n}\n\n// Check for files backup\nconst filesFile = `/tmp/${jobId}_files.tar.gz`;\nif (fs.existsSync(filesFile)) {\n  const filesStats = fs.statSync(filesFile);\n  const filesHash = crypto.createHash('sha256');\n  const filesData = fs.readFileSync(filesFile);\n  filesHash.update(filesData);\n  \n  files.push({\n    path: filesFile,\n    type: 'files',\n    size: filesStats.size,\n    checksum: filesHash.digest('hex')\n  });\n}\n\nreturn [{\n  json: {\n    job_id: jobId,\n    files: files,\n    total_size: files.reduce((sum, f) => sum + f.size, 0),\n    file_count: files.length\n  }\n}];"
      },
      "name": "Calculate Checksums",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [1560, 300]
    },
    {
      "parameters": {
        "functionCode": "// Upload files to MinIO storage\nconst MinIO = require('minio');\nconst fs = require('fs');\nconst path = require('path');\n\nconst minioClient = new MinIO.Client({\n  endPoint: process.env.MINIO_ENDPOINT || 'localhost',\n  port: parseInt(process.env.MINIO_PORT) || 9000,\n  useSSL: false,\n  accessKey: process.env.MINIO_ACCESS_KEY || 'minioadmin',\n  secretKey: process.env.MINIO_SECRET_KEY || 'minioadmin'\n});\n\nconst bucketName = 'vrooli-backups';\nconst jobId = $json.job_id;\nconst files = $json.files || [];\nconst uploadedFiles = [];\n\n// Ensure bucket exists\ntry {\n  await minioClient.bucketExists(bucketName);\n} catch (error) {\n  await minioClient.makeBucket(bucketName);\n}\n\n// Upload each backup file\nfor (const file of files) {\n  try {\n    const fileName = path.basename(file.path);\n    const objectName = `${new Date().toISOString().split('T')[0]}/${fileName}`;\n    \n    await minioClient.fPutObject(bucketName, objectName, file.path, {\n      'Content-Type': 'application/gzip',\n      'X-Backup-Job-Id': jobId,\n      'X-Backup-Type': file.type,\n      'X-File-Checksum': file.checksum\n    });\n    \n    uploadedFiles.push({\n      ...file,\n      storage_path: `${bucketName}/${objectName}`,\n      uploaded_at: new Date().toISOString()\n    });\n    \n    // Clean up local file\n    fs.unlinkSync(file.path);\n    \n  } catch (error) {\n    console.error(`Failed to upload ${file.path}:`, error);\n  }\n}\n\nreturn [{\n  json: {\n    job_id: jobId,\n    uploaded_files: uploadedFiles,\n    upload_count: uploadedFiles.length,\n    total_size: uploadedFiles.reduce((sum, f) => sum + f.size, 0)\n  }\n}];"
      },
      "name": "Upload to Storage",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [1780, 300]
    },
    {
      "parameters": {
        "operation": "update",
        "table": "backup_jobs",
        "updateKey": "id",
        "columns": "status,completed_at,size_bytes,storage_path,checksum",
        "additionalFields": {},
        "options": {}
      },
      "name": "Update Job Status",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 1,
      "position": [2000, 300],
      "credentials": {
        "postgres": {
          "id": "postgres-main",
          "name": "PostgreSQL Main"
        }
      }
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "{\n  \"job_id\": \"{{$json.job_id}}\",\n  \"status\": \"completed\",\n  \"uploaded_files\": {{$json.upload_count}},\n  \"total_size_bytes\": {{$json.total_size}},\n  \"completed_at\": \"{{new Date().toISOString()}}\"\n}"
      },
      "name": "Respond",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [2220, 300]
    },
    {
      "parameters": {
        "functionCode": "// Handle backup errors\nconst error = $json.error || 'Unknown error occurred';\nconst jobId = $json.job_id;\n\nconsole.error(`Backup job ${jobId} failed:`, error);\n\n// Update job status to failed\nreturn [{\n  json: {\n    job_id: jobId,\n    status: 'failed',\n    error_message: error.toString(),\n    completed_at: new Date().toISOString()\n  }\n}];"
      },
      "name": "Handle Error",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [1780, 500]
    },
    {
      "parameters": {
        "operation": "update",
        "table": "backup_jobs",
        "updateKey": "id",
        "columns": "status,completed_at,error_message",
        "additionalFields": {},
        "options": {}
      },
      "name": "Update Failed Status",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 1,
      "position": [2000, 500],
      "credentials": {
        "postgres": {
          "id": "postgres-main",
          "name": "PostgreSQL Main"
        }
      }
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "{\n  \"job_id\": \"{{$json.job_id}}\",\n  \"status\": \"failed\",\n  \"error\": \"{{$json.error_message}}\",\n  \"completed_at\": \"{{$json.completed_at}}\"\n}",
        "responseCode": 500
      },
      "name": "Error Response",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [2220, 500]
    }
  ],
  "connections": {
    "Start": {
      "main": [[{"node": "Webhook", "type": "main", "index": 0}]]
    },
    "Webhook": {
      "main": [[{"node": "Process Request", "type": "main", "index": 0}]]
    },
    "Process Request": {
      "main": [[{"node": "Create Job Record", "type": "main", "index": 0}]]
    },
    "Create Job Record": {
      "main": [[
        {"node": "Check Postgres Target", "type": "main", "index": 0},
        {"node": "Check Files Target", "type": "main", "index": 0}
      ]]
    },
    "Check Postgres Target": {
      "main": [[{"node": "Backup PostgreSQL", "type": "main", "index": 0}]]
    },
    "Check Files Target": {
      "main": [[{"node": "Backup Files", "type": "main", "index": 0}]]
    },
    "Backup PostgreSQL": {
      "main": [[{"node": "Calculate Checksums", "type": "main", "index": 0}]]
    },
    "Backup Files": {
      "main": [[{"node": "Calculate Checksums", "type": "main", "index": 0}]]
    },
    "Calculate Checksums": {
      "main": [[{"node": "Upload to Storage", "type": "main", "index": 0}]]
    },
    "Upload to Storage": {
      "main": [[{"node": "Update Job Status", "type": "main", "index": 0}]]
    },
    "Update Job Status": {
      "main": [[{"node": "Respond", "type": "main", "index": 0}]]
    }
  },
  "active": false,
  "settings": {
    "saveManualExecutions": true
  },
  "id": "backup-orchestrator"
}