{
  "$schema": "../../../../.vrooli/schemas/service.schema.json",
  "version": "1.0.0",
  "service": {
    "name": "ai-model-orchestra-controller",
    "displayName": "AI Model Orchestra Controller",
    "description": "Intelligent AI model routing and resource management system for optimal performance and cost efficiency",
    "version": "1.0.0",
    "tags": [
      "vrooli-enhancement",
      "ai-infrastructure",
      "ai",
      "orchestration",
      "load-balancing",
      "resource-management",
      "model-routing",
      "failover-handling",
      "cost-optimization",
      "api-only",
      "node-red-flows",
      "ollama-models",
      "postgres-storage"
    ]
  },
  "ports": {
    "api": {
      "env_var": "API_PORT",
      "description": "Main AI orchestration API port",
      "range": "15000-19999"
    },
    "monitoring": {
      "env_var": "MONITORING_PORT",
      "description": "Monitoring and metrics endpoint port",
      "range": "23000-23999"
    },
    "ui": {
      "env_var": "UI_PORT",
      "description": "AI Model Orchestra Controller UI port",
      "range": "35000-39999"
    }
  },
  "resources": {
    "node_red": {
      "type": "node_red",
      "enabled": true,
      "required": true,
      "initialization": [
        {
          "file": "initialization/workflows/orchestrator-main.json",
          "type": "flow"
        },
        {
          "file": "initialization/workflows/resource-monitor.json",
          "type": "flow"
        }
      ]
    },
    "ollama": {
      "type": "ollama",
      "enabled": true,
      "required": true,
      "models": [
        "llama3.2:1b",
        "llama3.2:3b",
        "llama3.2:8b",
        "codellama:7b",
        "nomic-embed-text"
      ]
    },
    "postgres": {
      "type": "postgres",
      "enabled": true,
      "required": true,
      "purpose": "orchestration metrics and model performance data",
      "initialization": []
    },
    "redis": {
      "type": "redis",
      "enabled": true,
      "required": true,
      "purpose": "real-time orchestration events and caching"
    }
  },
  "endpoints": {
    "health": "/health",
    "api_health": "/api/health",
    "select_model": "/api/ai/select-model",
    "route_request": "/api/ai/route-request",
    "model_status": "/api/ai/models/status",
    "resource_metrics": "/api/ai/resources/metrics",
    "dashboard": "/dashboard"
  },
  "capabilities": {
    "model_selection": {
      "description": "Intelligent model selection based on task requirements and system resources",
      "features": [
        "capability-based-routing",
        "resource-awareness",
        "performance-optimization"
      ]
    },
    "load_balancing": {
      "description": "Dynamic load balancing across available AI models",
      "features": [
        "round-robin",
        "least-connections",
        "resource-based",
        "failover"
      ]
    },
    "resource_monitoring": {
      "description": "Real-time monitoring of system resources and model performance",
      "features": [
        "memory-tracking",
        "cpu-monitoring",
        "response-time-analysis",
        "health-checks"
      ]
    },
    "cost_optimization": {
      "description": "Automatic cost optimization through intelligent model routing",
      "features": [
        "cost-aware-routing",
        "efficiency-metrics",
        "usage-analytics"
      ]
    }
  },
  "environment": {
    "ORCHESTRATOR_PORT": "${API_PORT}",
    "ORCHESTRATOR_HOST": "localhost",
    "ORCHESTRATOR_LOG_LEVEL": "info",
    "RESOURCE_MONITOR_INTERVAL": "5000",
    "MODEL_HEALTH_CHECK_INTERVAL": "30000",
    "MAX_RETRY_ATTEMPTS": "3",
    "REQUEST_TIMEOUT": "30000"
  },
  "health_checks": [
    {
      "name": "api_endpoint",
      "url": "http://localhost:${API_PORT}/health",
      "interval": 30,
      "timeout": 10,
      "retries": 3
    },
    {
      "name": "model_availability",
      "url": "http://localhost:${API_PORT}/api/ai/models/status",
      "interval": 60,
      "timeout": 15,
      "retries": 2
    },
    {
      "name": "node_red_flows",
      "url": "http://localhost:${RESOURCE_PORTS[node_red]}/health",
      "interval": 45,
      "timeout": 10,
      "retries": 2
    }
  ],
  "lifecycle": {
    "version": "2.0.0",
    "health": {
      "description": "AI Model Orchestra Controller health check configuration",
      "endpoints": {
        "api": "/health"
      },
      "checks": [
        {
          "name": "api_endpoint",
          "type": "http",
          "target": "http://localhost:${API_PORT}/health",
          "critical": true,
          "timeout": 5000,
          "interval": 30000
        }
      ],
      "timeout": 5000,
      "interval": 30000,
      "startup_grace_period": 10000
    },
    "setup": {
      "description": "Initialize AI Model Orchestra Controller",
      "condition": {
        "checks": [
          {
            "type": "resources",
            "populated": true
          }
        ]
      },
      "steps": [
        {
          "name": "generate-embeddings",
          "run": "vrooli resource qdrant embeddings refresh --force || echo \"Embeddings generation skipped (not critical)\"",
          "description": "Generate code embeddings for AI assistance"
        },
        {
          "name": "add-data",
          "run": "../../scripts/resources/populate/populate.sh .",
          "description": "Add app data to resources"
        },
        {
          "name": "create-resource-urls",
          "run": "bash initialization/scripts/create-resource-urls.sh",
          "description": "Generate dynamic resource URLs configuration"
        },
        {
          "name": "install-api-deps",
          "run": "cd initialization/configuration && npm install",
          "description": "Install API server dependencies"
        },
        {
          "name": "start-api",
          "run": "cd initialization/configuration && node api-server.js &",
          "description": "Start AI orchestration API server",
          "background": true
        },
        {
          "name": "show-urls",
          "run": "echo 'ðŸš€ AI Model Orchestra Controller initialized\\n  API: http://localhost:${API_PORT}\\n  Dashboard: http://localhost:${API_PORT}/dashboard\\n  Node-RED: http://localhost:${RESOURCE_PORTS[node_red]}\\n  Health: curl http://localhost:${API_PORT}/health'",
          "description": "Display service access URLs"
        }
      ]
    },
    "develop": {
      "description": "Start AI orchestration development environment",
      "steps": [
        {
          "name": "install-deps",
          "run": "cd initialization/configuration && npm install",
          "description": "Install API dependencies",
          "condition": {
            "file_exists": "initialization/configuration/package.json"
          }
        },
        {
          "name": "start-orchestrator",
          "run": "cd initialization/configuration && PORT=$API_PORT node api-server.js",
          "description": "Start AI orchestration API server",
          "background": true,
          "condition": {
            "file_exists": "initialization/configuration/api-server.js"
          }
        },
        {
          "name": "show-running-services",
          "run": "echo 'ðŸš€ AI Model Orchestra Controller running:\\n  API: http://localhost:${API_PORT}\\n  Dashboard: http://localhost:${API_PORT}/dashboard\\n  Model Selection: POST http://localhost:${API_PORT}/api/ai/select-model\\n  Route Request: POST http://localhost:${API_PORT}/api/ai/route-request\\n  Node-RED: http://localhost:${RESOURCE_PORTS[node_red]}'",
          "description": "Display all running service URLs"
        }
      ]
    },
    "test": {
      "description": "Test AI orchestration capabilities",
      "steps": [
        {
          "name": "test-api-health",
          "run": "curl -sf http://localhost:$API_PORT/health",
          "description": "Test API health endpoint"
        },
        {
          "name": "test-model-selection",
          "run": "curl -sf -X POST http://localhost:$API_PORT/api/ai/select-model -H 'Content-Type: application/json' -d '{\"taskType\": \"completion\"}'",
          "description": "Test model selection endpoint"
        },
        {
          "name": "test-model-status",
          "run": "curl -sf http://localhost:$API_PORT/api/ai/models/status",
          "description": "Test model status endpoint"
        },
        {
          "name": "test-resource-metrics",
          "run": "curl -sf http://localhost:$API_PORT/api/ai/resources/metrics",
          "description": "Test resource metrics endpoint"
        }
      ]
    },
    "stop": {
      "description": "Stop services",
      "steps": [
        {
          "name": "stop-api",
          "run": "pkill -f ai-model-orchestra-controller.start-api 2>/dev/null || true",
          "description": "Stop API process"
        },
        {
          "name": "stop-ui",
          "run": "pkill -f ai-model-orchestra-controller.start-ui 2>/dev/null || true",
          "description": "Stop UI process"
        }
      ]
    }
  },
  "dependencies": {
    "node": ">=18.0.0",
    "npm_packages": [
      "express@^4.18.0",
      "cors@^2.8.5",
      "helmet@^6.0.0",
      "express-rate-limit@^6.6.0",
      "body-parser@^1.20.0",
      "pg@^8.8.0",
      "redis@^4.5.0",
      "dockerode@^3.3.0",
      "node-cron@^3.0.0"
    ]
  },
  "monitoring": {
    "metrics": [
      "requests_per_second",
      "average_response_time",
      "model_selection_accuracy",
      "resource_utilization",
      "failover_rate",
      "cost_savings_percentage"
    ],
    "alerts": [
      {
        "name": "high_error_rate",
        "condition": "error_rate > 5%",
        "severity": "warning"
      },
      {
        "name": "resource_pressure",
        "condition": "memory_usage > 90%",
        "severity": "critical"
      },
      {
        "name": "model_unavailable",
        "condition": "available_models = 0",
        "severity": "critical"
      }
    ]
  },
  "scaling": {
    "min_instances": 1,
    "max_instances": 3,
    "scale_triggers": [
      {
        "metric": "requests_per_second",
        "threshold": 100,
        "action": "scale_up"
      },
      {
        "metric": "cpu_usage",
        "threshold": 80,
        "action": "scale_up"
      }
    ]
  },
  "business_value": {
    "primary_benefits": [
      "99.9% AI service availability",
      "3-5x improved throughput",
      "40-60% cost reduction",
      "Automatic failover and recovery",
      "Resource-aware intelligent routing"
    ],
    "use_cases": [
      "High-volume AI inference workloads",
      "Multi-model AI applications",
      "Cost-sensitive AI deployments",
      "Mission-critical AI services",
      "Resource-constrained environments"
    ],
    "roi_metrics": [
      "Reduced infrastructure costs",
      "Improved system reliability",
      "Faster development cycles",
      "Better resource utilization",
      "Reduced operational overhead"
    ]
  }
}