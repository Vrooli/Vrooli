{
  "schema_version": "1.0",
  "last_updated": "2025-01-14T00:00:00Z",
  "description": "Model capability definitions for AI Model Orchestra Controller",
  
  "task_types": {
    "completion": {
      "description": "General text completion and generation",
      "complexity_levels": ["simple", "moderate", "complex"],
      "typical_tokens": [100, 500, 2000]
    },
    "embedding": {
      "description": "Text embedding and vector generation",
      "complexity_levels": ["simple"],
      "typical_tokens": [100, 1000, 5000]
    },
    "reasoning": {
      "description": "Complex reasoning and problem solving",
      "complexity_levels": ["moderate", "complex"],
      "typical_tokens": [200, 1000, 4000]
    },
    "code": {
      "description": "Code generation, analysis, and debugging",
      "complexity_levels": ["simple", "moderate", "complex"],
      "typical_tokens": [50, 500, 2000]
    },
    "analysis": {
      "description": "Text analysis, summarization, and extraction",
      "complexity_levels": ["simple", "moderate", "complex"],
      "typical_tokens": [200, 1000, 3000]
    },
    "conversation": {
      "description": "Conversational AI and chat interactions",
      "complexity_levels": ["simple", "moderate"],
      "typical_tokens": [50, 200, 1000]
    },
    "creative": {
      "description": "Creative writing and content generation",
      "complexity_levels": ["simple", "moderate", "complex"],
      "typical_tokens": [100, 500, 2000]
    }
  },
  
  "models": {
    "llama3.2:1b": {
      "capabilities": ["completion", "conversation", "creative"],
      "ram_required_gb": 1.3,
      "speed": "fast",
      "cost_per_1k_tokens": 0.0005,
      "max_context": 8192,
      "quality_tier": "basic",
      "best_for": ["quick_responses", "simple_tasks", "low_resource_environments"],
      "strengths": ["speed", "efficiency", "low_memory"],
      "weaknesses": ["limited_reasoning", "basic_knowledge"],
      "performance": {
        "avg_tokens_per_second": 50,
        "startup_time_seconds": 5,
        "memory_efficiency": "excellent"
      }
    },
    
    "llama3.2:3b": {
      "capabilities": ["completion", "reasoning", "conversation", "analysis", "creative"],
      "ram_required_gb": 2.0,
      "speed": "fast",
      "cost_per_1k_tokens": 0.001,
      "max_context": 8192,
      "quality_tier": "good",
      "best_for": ["general_purpose", "moderate_complexity", "balanced_performance"],
      "strengths": ["balanced", "versatile", "good_speed"],
      "weaknesses": ["limited_context", "not_specialized"],
      "performance": {
        "avg_tokens_per_second": 40,
        "startup_time_seconds": 8,
        "memory_efficiency": "very_good"
      }
    },
    
    "llama3.2:8b": {
      "capabilities": ["completion", "reasoning", "code", "analysis", "conversation", "creative"],
      "ram_required_gb": 4.9,
      "speed": "moderate",
      "cost_per_1k_tokens": 0.005,
      "max_context": 32768,
      "quality_tier": "very_good",
      "best_for": ["complex_reasoning", "code_generation", "detailed_analysis"],
      "strengths": ["strong_reasoning", "large_context", "versatile"],
      "weaknesses": ["slower_speed", "higher_memory"],
      "performance": {
        "avg_tokens_per_second": 25,
        "startup_time_seconds": 12,
        "memory_efficiency": "good"
      }
    },
    
    "llama3.1:8b": {
      "capabilities": ["completion", "reasoning", "code", "analysis", "conversation", "creative"],
      "ram_required_gb": 4.9,
      "speed": "moderate", 
      "cost_per_1k_tokens": 0.005,
      "max_context": 32768,
      "quality_tier": "very_good",
      "best_for": ["general_excellence", "reliability", "proven_performance"],
      "strengths": ["proven_stable", "good_reasoning", "reliable"],
      "weaknesses": ["not_latest", "moderate_speed"],
      "performance": {
        "avg_tokens_per_second": 25,
        "startup_time_seconds": 12,
        "memory_efficiency": "good"
      }
    },
    
    "codellama:13b": {
      "capabilities": ["code", "reasoning", "analysis"],
      "ram_required_gb": 7.4,
      "speed": "slow",
      "cost_per_1k_tokens": 0.01,
      "max_context": 16384,
      "quality_tier": "excellent",
      "best_for": ["complex_coding", "code_analysis", "software_engineering"],
      "strengths": ["code_expert", "detailed_analysis", "high_quality"],
      "weaknesses": ["slow", "high_memory", "specialized_only"],
      "performance": {
        "avg_tokens_per_second": 15,
        "startup_time_seconds": 20,
        "memory_efficiency": "moderate"
      }
    },
    
    "llama3.1:70b": {
      "capabilities": ["completion", "reasoning", "code", "analysis", "conversation", "creative"],
      "ram_required_gb": 39.0,
      "speed": "very_slow",
      "cost_per_1k_tokens": 0.05,
      "max_context": 32768,
      "quality_tier": "exceptional",
      "best_for": ["highest_quality", "complex_problems", "research_tasks"],
      "strengths": ["best_quality", "superior_reasoning", "comprehensive"],
      "weaknesses": ["very_slow", "very_high_memory", "expensive"],
      "performance": {
        "avg_tokens_per_second": 5,
        "startup_time_seconds": 60,
        "memory_efficiency": "poor"
      }
    },
    
    "nomic-embed-text": {
      "capabilities": ["embedding"],
      "ram_required_gb": 0.3,
      "speed": "very_fast",
      "cost_per_1k_tokens": 0.0001,
      "max_context": 8192,
      "quality_tier": "good",
      "dimensions": 768,
      "best_for": ["text_embeddings", "semantic_search", "clustering"],
      "strengths": ["very_fast", "low_memory", "specialized"],
      "weaknesses": ["embedding_only", "limited_dimensions"],
      "performance": {
        "avg_embeddings_per_second": 100,
        "startup_time_seconds": 3,
        "memory_efficiency": "excellent"
      }
    },
    
    "mxbai-embed-large": {
      "capabilities": ["embedding"],
      "ram_required_gb": 0.7,
      "speed": "fast",
      "cost_per_1k_tokens": 0.0002,
      "max_context": 8192,
      "quality_tier": "very_good",
      "dimensions": 1024,
      "best_for": ["high_quality_embeddings", "semantic_search", "rag_systems"],
      "strengths": ["high_quality", "larger_dimensions", "good_speed"],
      "weaknesses": ["embedding_only", "moderate_memory"],
      "performance": {
        "avg_embeddings_per_second": 80,
        "startup_time_seconds": 5,
        "memory_efficiency": "very_good"
      }
    }
  },
  
  "selection_strategies": {
    "speed_priority": {
      "description": "Prioritize fastest response time",
      "weights": {
        "speed": 0.7,
        "cost": 0.1,
        "quality": 0.1,
        "resource": 0.1
      }
    },
    
    "quality_priority": {
      "description": "Prioritize highest quality output",
      "weights": {
        "speed": 0.1,
        "cost": 0.1,
        "quality": 0.7,
        "resource": 0.1
      }
    },
    
    "cost_priority": {
      "description": "Prioritize lowest cost",
      "weights": {
        "speed": 0.1,
        "cost": 0.7,
        "quality": 0.1,
        "resource": 0.1
      }
    },
    
    "balanced": {
      "description": "Balanced approach across all factors",
      "weights": {
        "speed": 0.25,
        "cost": 0.25,
        "quality": 0.25,
        "resource": 0.25
      }
    },
    
    "resource_aware": {
      "description": "Heavily consider resource constraints",
      "weights": {
        "speed": 0.2,
        "cost": 0.2,
        "quality": 0.2,
        "resource": 0.4
      }
    }
  },
  
  "fallback_chains": {
    "completion": [
      "llama3.2:8b",
      "llama3.2:3b", 
      "llama3.2:1b"
    ],
    
    "reasoning": [
      "llama3.1:70b",
      "llama3.2:8b",
      "llama3.2:3b"
    ],
    
    "code": [
      "codellama:13b",
      "llama3.2:8b",
      "llama3.2:3b"
    ],
    
    "embedding": [
      "mxbai-embed-large",
      "nomic-embed-text"
    ],
    
    "analysis": [
      "llama3.2:8b",
      "llama3.2:3b",
      "llama3.2:1b"
    ],
    
    "conversation": [
      "llama3.2:3b",
      "llama3.2:8b",
      "llama3.2:1b"
    ],
    
    "creative": [
      "llama3.2:8b",
      "llama3.2:3b",
      "llama3.2:1b"
    ]
  },
  
  "memory_pressure_strategies": {
    "low": {
      "description": "Under 30% memory pressure",
      "preferred_models": ["llama3.1:70b", "codellama:13b", "llama3.2:8b"],
      "strategy": "use_best_available"
    },
    
    "moderate": {
      "description": "30-70% memory pressure", 
      "preferred_models": ["llama3.2:8b", "llama3.2:3b"],
      "strategy": "balanced_performance"
    },
    
    "high": {
      "description": "70-90% memory pressure",
      "preferred_models": ["llama3.2:3b", "llama3.2:1b", "nomic-embed-text"],
      "strategy": "prioritize_efficiency"
    },
    
    "critical": {
      "description": "Over 90% memory pressure",
      "preferred_models": ["llama3.2:1b", "nomic-embed-text"],
      "strategy": "emergency_only",
      "actions": ["reject_large_requests", "queue_requests", "alert_admin"]
    }
  },
  
  "quality_requirements": {
    "basic": {
      "description": "Basic quality sufficient",
      "suitable_models": ["llama3.2:1b", "llama3.2:3b", "nomic-embed-text"]
    },
    
    "good": {
      "description": "Good quality needed",
      "suitable_models": ["llama3.2:3b", "llama3.2:8b", "mxbai-embed-large"]
    },
    
    "high": {
      "description": "High quality required",
      "suitable_models": ["llama3.2:8b", "llama3.1:8b", "codellama:13b"]
    },
    
    "exceptional": {
      "description": "Exceptional quality needed",
      "suitable_models": ["llama3.1:70b", "codellama:13b"]
    }
  },
  
  "complexity_mapping": {
    "simple": {
      "description": "Simple tasks, quick responses",
      "token_range": [1, 200],
      "suitable_models": ["llama3.2:1b", "llama3.2:3b", "nomic-embed-text"]
    },
    
    "moderate": {
      "description": "Moderate complexity tasks", 
      "token_range": [200, 1000],
      "suitable_models": ["llama3.2:3b", "llama3.2:8b", "mxbai-embed-large"]
    },
    
    "complex": {
      "description": "Complex tasks requiring deep thinking",
      "token_range": [1000, 4000],
      "suitable_models": ["llama3.2:8b", "llama3.1:8b", "codellama:13b", "llama3.1:70b"]
    },
    
    "expert": {
      "description": "Expert-level tasks",
      "token_range": [2000, 8000],
      "suitable_models": ["llama3.1:70b", "codellama:13b"]
    }
  }
}