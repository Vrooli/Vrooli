{
  "name": "File List Writer",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "file-writer",
        "responseMode": "responseNode",
        "options": {}
      },
      "id": "webhook_trigger",
      "name": "Webhook Trigger",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1.1,
      "position": [200, 200]
    },
    {
      "parameters": {},
      "id": "manual_trigger",
      "name": "Manual Trigger",
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [200, 400]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "const input = $input.item.json;\nif (!input || Object.keys(input).length === 0 || !input.action) {\n  return {\n    action: \"append_items\",\n    file_path: \"/tmp/vrooli_example.txt\",\n    items: [\"New line 1\", \"New line 2\"],\n    format: \"lines\",\n    encoding: \"utf8\",\n    test_mode: true\n  };\n}\nreturn input;"
      },
      "id": "handle_empty_input",
      "name": "Handle Empty Input (Manual Only)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [380, 400]
    },
    {
      "parameters": {},
      "id": "merge_triggers",
      "name": "Merge Triggers",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 2.1,
      "position": [520, 300]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Validate input and prepare file writing data\nconst input = $input.item.json;\n\n// Validate required action field\nif (!input.action) {\n  throw new Error('Missing required field: action');\n}\n\nconst validActions = ['append_items', 'prepend_items', 'insert_at_index', 'replace_full'];\nif (!validActions.includes(input.action)) {\n  throw new Error(`Invalid action: ${input.action}. Valid actions: ${validActions.join(', ')}`);\n}\n\n// Validate required file_path\nif (!input.file_path) {\n  throw new Error('Missing required field: file_path');\n}\n\n// Validate file path (prevent traversal)\nif (input.file_path.includes('..')) {\n  throw new Error('Path traversal not allowed');\n}\n\n// Validate required items\nif (!input.items || !Array.isArray(input.items)) {\n  throw new Error('Missing or invalid items array');\n}\n\nif (input.items.length === 0) {\n  throw new Error('Items array cannot be empty');\n}\n\n// Validate specific action requirements\nif (input.action === 'insert_at_index' && input.index === undefined) {\n  throw new Error('Action insert_at_index requires an index');\n}\n\n// Set defaults\nconst format = input.format || 'lines';\nconst encoding = input.encoding || 'utf8';\nconst delimiter = input.delimiter || ',';\nconst hasHeaders = input.has_headers !== false;\nconst createBackup = input.create_backup !== false;\nconst atomicWrite = input.atomic_write !== false;\nconst validateBefore = input.validate_before !== false;\nconst preserveFormat = input.preserve_format !== false;\nconst deduplicate = input.deduplicate || false;\nconst mergStrategy = input.merge_strategy || 'append'; // append, prepend, replace, merge\n\n// Validate format\nconst validFormats = ['lines', 'csv', 'tsv', 'json', 'jsonl', 'markdown', 'yaml'];\nif (!validFormats.includes(format)) {\n  throw new Error(`Invalid format: ${format}. Valid formats: ${validFormats.join(', ')}`);\n}\n\n// Validate encoding\nconst validEncodings = ['utf8', 'utf-8', 'ascii', 'latin1'];\nif (!validEncodings.includes(encoding.toLowerCase())) {\n  throw new Error(`Invalid encoding: ${encoding}. Valid encodings: ${validEncodings.join(', ')}`);\n}\n\n// Validate items structure based on format\nif (format === 'json' || format === 'jsonl') {\n  const invalidItems = input.items.filter(item => \n    typeof item !== 'object' && typeof item !== 'string'\n  );\n  if (invalidItems.length > 0) {\n    throw new Error('JSON format requires items to be objects or strings');\n  }\n}\n\n// Prepare file writing data\nconst fileData = {\n  action: input.action,\n  file_path: input.file_path,\n  items: input.items,\n  format: format,\n  encoding: encoding,\n  delimiter: delimiter,\n  has_headers: hasHeaders,\n  create_backup: createBackup,\n  atomic_write: atomicWrite,\n  validate_before: validateBefore,\n  preserve_format: preserveFormat,\n  deduplicate: deduplicate,\n  merge_strategy: mergStrategy,\n  // Action-specific parameters\n  index: input.index,\n  // Lock configuration\n  use_lock: input.use_lock !== false,\n  lock_timeout_ms: input.lock_timeout_ms || 5000,\n  lock_retry_ms: input.lock_retry_ms || 100,\n  // File permissions\n  file_mode: input.file_mode || '0644',\n  create_directories: input.create_directories !== false,\n  metadata: {\n    request_id: `write_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,\n    trace_id: input.trace_id || null,\n    timestamp: new Date().toISOString(),\n    test_mode: input.test_mode || false\n  }\n};\n\nreturn fileData;"
      },
      "id": "validate_input",
      "name": "Validate Input",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [700, 300]
    },
    {
      "parameters": {
        "rules": {
          "values": [
            {
              "conditions": {
                "string": [
                  {
                    "value1": "={{ $json.action }}",
                    "value2": "append_items"
                  }
                ]
              },
              "renameOutput": true,
              "outputKey": "append"
            },
            {
              "conditions": {
                "string": [
                  {
                    "value1": "={{ $json.action }}",
                    "value2": "prepend_items"
                  }
                ]
              },
              "renameOutput": true,
              "outputKey": "prepend"
            },
            {
              "conditions": {
                "string": [
                  {
                    "value1": "={{ $json.action }}",
                    "value2": "insert_at_index"
                  }
                ]
              },
              "renameOutput": true,
              "outputKey": "insert"
            },
            {
              "conditions": {
                "string": [
                  {
                    "value1": "={{ $json.action }}",
                    "value2": "replace_full"
                  }
                ]
              },
              "renameOutput": true,
              "outputKey": "replace"
            }
          ]
        }
      },
      "id": "action_router",
      "name": "Action Router",
      "type": "n8n-nodes-base.switch",
      "typeVersion": 1,
      "position": [900, 300]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Append items to file\nconst input = $input.item.json;\nconst path = require('path');\n\n// Simulate existing file content\nconst existingContent = [\n  'Existing line 1',\n  'Existing line 2',\n  'Existing line 3'\n];\n\n// Format items based on file format\nconst formatItems = (items, format, options) => {\n  switch(format) {\n    case 'json':\n      // For JSON, append to array or merge objects\n      return items;\n      \n    case 'csv':\n    case 'tsv':\n      const delimiter = format === 'csv' ? ',' : '\\t';\n      return items.map(item => {\n        if (typeof item === 'object') {\n          return Object.values(item).join(delimiter);\n        }\n        return String(item);\n      });\n      \n    case 'jsonl':\n      return items.map(item => \n        typeof item === 'object' ? JSON.stringify(item) : item\n      );\n      \n    case 'markdown':\n      return items.map(item => `- ${item}`);\n      \n    case 'lines':\n    default:\n      return items.map(item => String(item));\n  }\n};\n\n// Apply deduplication if requested\nconst applyDeduplication = (existing, newItems, dedupe) => {\n  if (!dedupe) return [...existing, ...newItems];\n  \n  const uniqueSet = new Set([...existing, ...newItems]);\n  return Array.from(uniqueSet);\n};\n\n// Format new items\nconst formattedItems = formatItems(input.items, input.format, {\n  delimiter: input.delimiter,\n  has_headers: input.has_headers\n});\n\n// Combine with existing content\nconst finalContent = applyDeduplication(\n  existingContent,\n  formattedItems,\n  input.deduplicate\n);\n\n// Calculate write statistics\nconst writeStats = {\n  original_lines: existingContent.length,\n  new_items: formattedItems.length,\n  final_lines: finalContent.length,\n  duplicates_removed: input.deduplicate ? \n    (existingContent.length + formattedItems.length - finalContent.length) : 0,\n  bytes_written: finalContent.join('\\n').length,\n  format_used: input.format\n};\n\n// Simulate atomic write process\nconst atomicWriteInfo = input.atomic_write ? {\n  temp_file: `${input.file_path}.tmp.${Date.now()}`,\n  backup_file: input.create_backup ? `${input.file_path}.bak` : null,\n  steps: [\n    'Write to temporary file',\n    input.create_backup ? 'Create backup of original' : null,\n    'Rename temporary to target (atomic)',\n    'Verify write success'\n  ].filter(Boolean)\n} : null;\n\n// Lock acquisition simulation\nconst lockInfo = input.use_lock ? {\n  lock_acquired: true,\n  lock_file: `${input.file_path}.lock`,\n  lock_wait_ms: Math.floor(Math.random() * 100),\n  lock_holder_id: input.metadata.request_id\n} : null;\n\n// File validation results\nconst validation = {\n  pre_write_check: input.validate_before ? {\n    file_exists: true,\n    is_writable: true,\n    has_space: true,\n    format_valid: true\n  } : null,\n  post_write_check: {\n    bytes_written: writeStats.bytes_written,\n    checksum: Math.random().toString(36).substring(7),\n    verified: true\n  }\n};\n\nconst response = {\n  success: true,\n  action: 'append_items',\n  file_path: input.file_path,\n  write_statistics: writeStats,\n  content_preview: {\n    first_5_lines: finalContent.slice(0, 5),\n    last_5_lines: finalContent.slice(-5),\n    sample_new_items: formattedItems.slice(0, 3)\n  },\n  atomic_write: atomicWriteInfo,\n  lock_info: lockInfo,\n  validation: validation,\n  file_info: {\n    path: input.file_path,\n    directory: path.dirname(input.file_path),\n    filename: path.basename(input.file_path),\n    extension: path.extname(input.file_path),\n    permissions: input.file_mode\n  },\n  operation_details: {\n    method: 'append',\n    position: 'end',\n    format_preserved: input.preserve_format,\n    backup_created: input.create_backup,\n    deduplication_applied: input.deduplicate\n  },\n  recommendations: [\n    `Successfully appended ${formattedItems.length} items`,\n    input.deduplicate && writeStats.duplicates_removed > 0 ? \n      `Removed ${writeStats.duplicates_removed} duplicates` : null,\n    writeStats.final_lines > 10000 ? \n      'Consider archiving old data - file is getting large' : null,\n    'Remember to handle the file cleanup if temporary'\n  ].filter(Boolean),\n  metadata: {\n    ...input.metadata,\n    processing_time_ms: 6,\n    lock_wait_time_ms: lockInfo?.lock_wait_ms || 0\n  }\n};\n\nreturn response;"
      },
      "id": "append_items",
      "name": "Append Items",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [1100, 100]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Prepend items to beginning of file\nconst input = $input.item.json;\nconst path = require('path');\n\n// Simulate existing file content\nconst existingContent = [\n  'Existing line A',\n  'Existing line B',\n  'Existing line C',\n  'Existing line D'\n];\n\n// Format items for prepending\nconst formatForPrepend = (items, format) => {\n  switch(format) {\n    case 'csv':\n    case 'tsv':\n      const delimiter = format === 'csv' ? ',' : '\\t';\n      // Handle headers specially for CSV/TSV\n      if (input.has_headers && existingContent.length > 0) {\n        // Extract existing headers\n        const headers = existingContent[0];\n        const dataToInsert = items.map(item => {\n          if (typeof item === 'object') {\n            return Object.values(item).join(delimiter);\n          }\n          return String(item);\n        });\n        return { headers, data: dataToInsert };\n      }\n      break;\n      \n    case 'json':\n      // For JSON arrays, items go at beginning\n      return items;\n      \n    case 'markdown':\n      return items.map(item => `- ${item}`);\n      \n    case 'lines':\n    default:\n      return items.map(item => String(item));\n  }\n  \n  return items.map(item => String(item));\n};\n\n// Process items for prepending\nconst processedItems = formatForPrepend(input.items, input.format);\nconst itemsToInsert = Array.isArray(processedItems) ? processedItems : processedItems.data || [];\n\n// Build final content with items prepended\nlet finalContent;\nif (input.format === 'csv' || input.format === 'tsv') {\n  // Special handling for tabular data with headers\n  if (processedItems.headers) {\n    finalContent = [\n      processedItems.headers,\n      ...itemsToInsert,\n      ...existingContent.slice(1) // Skip original header\n    ];\n  } else {\n    finalContent = [...itemsToInsert, ...existingContent];\n  }\n} else {\n  finalContent = [...itemsToInsert, ...existingContent];\n}\n\n// Apply deduplication if needed\nif (input.deduplicate) {\n  const uniqueSet = new Set(finalContent);\n  finalContent = Array.from(uniqueSet);\n}\n\n// Calculate insertion statistics\nconst insertionStats = {\n  original_lines: existingContent.length,\n  items_prepended: itemsToInsert.length,\n  final_lines: finalContent.length,\n  insertion_point: 0,\n  content_shifted_by: itemsToInsert.length,\n  bytes_added: itemsToInsert.join('\\n').length\n};\n\n// Memory usage estimation\nconst memoryEstimate = {\n  before_bytes: existingContent.join('\\n').length,\n  after_bytes: finalContent.join('\\n').length,\n  increase_bytes: finalContent.join('\\n').length - existingContent.join('\\n').length,\n  increase_percentage: \n    ((finalContent.join('\\n').length / existingContent.join('\\n').length - 1) * 100).toFixed(2) + '%'\n};\n\n// Generate revert information\nconst revertInfo = {\n  can_revert: true,\n  revert_command: `Remove first ${itemsToInsert.length} lines`,\n  original_first_line: existingContent[0],\n  new_first_line: finalContent[0]\n};\n\nconst response = {\n  success: true,\n  action: 'prepend_items',\n  file_path: input.file_path,\n  insertion_statistics: insertionStats,\n  memory_usage: memoryEstimate,\n  content_preview: {\n    new_beginning: finalContent.slice(0, 5),\n    original_beginning: existingContent.slice(0, 3),\n    items_inserted: itemsToInsert.slice(0, 3)\n  },\n  format_handling: {\n    format: input.format,\n    headers_preserved: input.has_headers && ['csv', 'tsv'].includes(input.format),\n    special_formatting: ['csv', 'tsv', 'json'].includes(input.format)\n  },\n  revert_info: revertInfo,\n  file_integrity: {\n    lock_used: input.use_lock,\n    atomic_write: input.atomic_write,\n    backup_created: input.create_backup,\n    validation_performed: input.validate_before\n  },\n  performance: {\n    operation: 'prepend',\n    complexity: 'O(n)',\n    requires_full_rewrite: true,\n    estimated_io_operations: 2\n  },\n  recommendations: [\n    `Successfully prepended ${itemsToInsert.length} items`,\n    insertionStats.content_shifted_by > 1000 ? \n      'Large content shift - consider using append for better performance' : null,\n    memoryEstimate.increase_percentage > '50%' ? \n      'Significant size increase - monitor disk space' : null,\n    'Prepend operations require rewriting entire file'\n  ].filter(Boolean),\n  metadata: {\n    ...input.metadata,\n    processing_time_ms: 7\n  }\n};\n\nreturn response;"
      },
      "id": "prepend_items",
      "name": "Prepend Items",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [1100, 250]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Insert items at specific index in file\nconst input = $input.item.json;\nconst path = require('path');\n\n// Simulate existing file content\nconst existingContent = [];\nfor (let i = 1; i <= 20; i++) {\n  existingContent.push(`Line ${i}: Content ${Math.random().toString(36).substring(7)}`);\n}\n\n// Validate and normalize index\nconst normalizeIndex = (index, arrayLength) => {\n  if (index < 0) {\n    // Negative indexing from end\n    return Math.max(0, arrayLength + index);\n  }\n  return Math.min(index, arrayLength);\n};\n\nconst targetIndex = normalizeIndex(input.index, existingContent.length);\n\n// Format items for insertion\nconst formatItemsForInsertion = (items, format) => {\n  switch(format) {\n    case 'json':\n      return items;\n      \n    case 'csv':\n    case 'tsv':\n      const delimiter = format === 'csv' ? ',' : '\\t';\n      return items.map(item => {\n        if (typeof item === 'object') {\n          return Object.values(item).join(delimiter);\n        }\n        return String(item);\n      });\n      \n    case 'jsonl':\n      return items.map(item => \n        typeof item === 'object' ? JSON.stringify(item) : String(item)\n      );\n      \n    case 'markdown':\n      return items.map((item, i) => `- ${item}`);\n      \n    default:\n      return items.map(item => String(item));\n  }\n};\n\nconst formattedItems = formatItemsForInsertion(input.items, input.format);\n\n// Perform insertion\nconst beforeInsert = existingContent.slice(0, targetIndex);\nconst afterInsert = existingContent.slice(targetIndex);\nconst finalContent = [...beforeInsert, ...formattedItems, ...afterInsert];\n\n// Calculate insertion context\nconst context = {\n  lines_before: beforeInsert.length,\n  lines_after: afterInsert.length,\n  insertion_point: targetIndex,\n  items_inserted: formattedItems.length,\n  context_before: beforeInsert.slice(-2),\n  context_after: afterInsert.slice(0, 2)\n};\n\n// Analyze insertion impact\nconst impact = {\n  content_shifted: afterInsert.length,\n  shift_direction: 'down',\n  shift_amount: formattedItems.length,\n  percentage_point: ((targetIndex / existingContent.length) * 100).toFixed(2) + '%',\n  is_at_beginning: targetIndex === 0,\n  is_at_end: targetIndex === existingContent.length,\n  is_in_middle: targetIndex > 0 && targetIndex < existingContent.length\n};\n\n// Calculate boundaries and ranges\nconst boundaries = {\n  start_index: targetIndex,\n  end_index: targetIndex + formattedItems.length,\n  affected_range: `[${targetIndex}:${targetIndex + formattedItems.length}]`,\n  original_item_at_index: existingContent[targetIndex] || null,\n  new_item_at_index: finalContent[targetIndex]\n};\n\n// Memory and performance metrics\nconst performance = {\n  operation_complexity: 'O(n)',\n  memory_required: finalContent.join('\\n').length,\n  io_operations: 2, // Read + Write\n  requires_temp_storage: input.atomic_write,\n  estimated_time_ms: Math.ceil(finalContent.length / 100) // Rough estimate\n};\n\n// Generate undo information\nconst undoInfo = {\n  can_undo: true,\n  undo_operation: 'remove_range',\n  undo_start_index: targetIndex,\n  undo_end_index: targetIndex + formattedItems.length,\n  undo_command: `Remove lines ${targetIndex + 1} to ${targetIndex + formattedItems.length}`,\n  state_hash_before: Math.random().toString(36).substring(7),\n  state_hash_after: Math.random().toString(36).substring(7)\n};\n\nconst response = {\n  success: true,\n  action: 'insert_at_index',\n  file_path: input.file_path,\n  insertion_details: {\n    requested_index: input.index,\n    normalized_index: targetIndex,\n    items_inserted: formattedItems.length,\n    negative_indexing_used: input.index < 0\n  },\n  content_stats: {\n    original_lines: existingContent.length,\n    final_lines: finalContent.length,\n    lines_added: formattedItems.length,\n    bytes_before: existingContent.join('\\n').length,\n    bytes_after: finalContent.join('\\n').length\n  },\n  insertion_context: context,\n  impact_analysis: impact,\n  boundaries: boundaries,\n  performance_metrics: performance,\n  undo_information: undoInfo,\n  content_preview: {\n    before_insertion: beforeInsert.slice(-3),\n    inserted_items: formattedItems.slice(0, 3),\n    after_insertion: afterInsert.slice(0, 3),\n    final_sample: finalContent.slice(\n      Math.max(0, targetIndex - 1),\n      targetIndex + formattedItems.length + 1\n    )\n  },\n  validation: {\n    index_valid: targetIndex >= 0 && targetIndex <= existingContent.length,\n    items_formatted: true,\n    content_integrity: true,\n    format_preserved: input.preserve_format\n  },\n  recommendations: [\n    `Inserted ${formattedItems.length} items at position ${targetIndex}`,\n    impact.content_shifted > 1000 ? \n      'Large content shift - consider performance impact' : null,\n    targetIndex === 0 ? \n      'Insertion at beginning - consider using prepend_items for clarity' : null,\n    targetIndex === existingContent.length ? \n      'Insertion at end - consider using append_items for better performance' : null\n  ].filter(Boolean),\n  metadata: {\n    ...input.metadata,\n    processing_time_ms: 8\n  }\n};\n\nreturn response;"
      },
      "id": "insert_at_index",
      "name": "Insert at Index",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [1100, 400]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Replace entire file contents\nconst input = $input.item.json;\nconst path = require('path');\n\n// Simulate existing file content for comparison\nconst existingContent = [\n  'Old content line 1',\n  'Old content line 2',\n  'Old content line 3',\n  'Old content line 4',\n  'Old content line 5'\n];\n\n// Format new content based on format type\nconst formatContent = (items, format, options) => {\n  switch(format) {\n    case 'json':\n      // Write as JSON array or object\n      if (items.length === 1 && typeof items[0] === 'object') {\n        return JSON.stringify(items[0], null, 2);\n      }\n      return JSON.stringify(items, null, 2);\n      \n    case 'csv':\n    case 'tsv':\n      const delimiter = format === 'csv' ? ',' : '\\t';\n      const lines = [];\n      \n      // Add headers if needed\n      if (options.has_headers && items.length > 0 && typeof items[0] === 'object') {\n        lines.push(Object.keys(items[0]).join(delimiter));\n      }\n      \n      // Add data rows\n      items.forEach(item => {\n        if (typeof item === 'object') {\n          lines.push(Object.values(item).join(delimiter));\n        } else {\n          lines.push(String(item));\n        }\n      });\n      \n      return lines;\n      \n    case 'jsonl':\n      return items.map(item => \n        typeof item === 'object' ? JSON.stringify(item) : String(item)\n      );\n      \n    case 'yaml':\n      // Simplified YAML formatting\n      return items.map(item => {\n        if (typeof item === 'object') {\n          return Object.entries(item)\n            .map(([k, v]) => `${k}: ${v}`)\n            .join('\\n');\n        }\n        return `- ${item}`;\n      });\n      \n    case 'markdown':\n      return items.map((item, i) => {\n        if (typeof item === 'object') {\n          return `## Item ${i + 1}\\n` + \n            Object.entries(item)\n              .map(([k, v]) => `- **${k}**: ${v}`)\n              .join('\\n');\n        }\n        return `- ${item}`;\n      });\n      \n    case 'lines':\n    default:\n      return items.map(item => String(item));\n  }\n};\n\n// Format the new content\nconst newContent = formatContent(input.items, input.format, {\n  has_headers: input.has_headers,\n  delimiter: input.delimiter\n});\n\nconst finalContent = Array.isArray(newContent) ? newContent : newContent.split('\\n');\n\n// Calculate replacement statistics\nconst replacementStats = {\n  original_lines: existingContent.length,\n  original_bytes: existingContent.join('\\n').length,\n  new_lines: finalContent.length,\n  new_bytes: Array.isArray(newContent) ? \n    finalContent.join('\\n').length : newContent.length,\n  lines_difference: finalContent.length - existingContent.length,\n  bytes_difference: (Array.isArray(newContent) ? \n    finalContent.join('\\n').length : newContent.length) - existingContent.join('\\n').length,\n  change_percentage: \n    ((Math.abs(finalContent.length - existingContent.length) / existingContent.length) * 100).toFixed(2) + '%'\n};\n\n// Create backup information\nconst backupInfo = input.create_backup ? {\n  backup_created: true,\n  backup_path: `${input.file_path}.bak`,\n  backup_timestamp: new Date().toISOString(),\n  original_content_hash: Math.random().toString(36).substring(7),\n  can_restore: true,\n  restore_command: `mv ${input.file_path}.bak ${input.file_path}`\n} : {\n  backup_created: false,\n  warning: 'No backup created - original content will be lost'\n};\n\n// Content analysis\nconst contentAnalysis = {\n  format_type: input.format,\n  is_structured: ['json', 'csv', 'tsv', 'yaml'].includes(input.format),\n  is_tabular: ['csv', 'tsv'].includes(input.format),\n  has_headers: input.has_headers && ['csv', 'tsv'].includes(input.format),\n  line_endings: 'LF',\n  encoding: input.encoding,\n  estimated_read_time_ms: Math.ceil(finalContent.length / 10)\n};\n\n// Diff summary (simplified)\nconst diffSummary = {\n  type: 'full_replacement',\n  lines_removed: existingContent.length,\n  lines_added: finalContent.length,\n  first_line_before: existingContent[0],\n  first_line_after: finalContent[0],\n  last_line_before: existingContent[existingContent.length - 1],\n  last_line_after: finalContent[finalContent.length - 1]\n};\n\n// Atomic write process\nconst atomicProcess = input.atomic_write ? {\n  enabled: true,\n  steps: [\n    `Write to temp file: ${input.file_path}.tmp`,\n    `Verify temp file integrity`,\n    input.create_backup ? `Backup original to: ${input.file_path}.bak` : null,\n    `Atomic rename temp to: ${input.file_path}`,\n    `Verify final file`\n  ].filter(Boolean),\n  rollback_possible: true,\n  transaction_id: Math.random().toString(36).substring(7)\n} : {\n  enabled: false,\n  direct_overwrite: true,\n  warning: 'Non-atomic write - partial failure possible'\n};\n\nconst response = {\n  success: true,\n  action: 'replace_full',\n  file_path: input.file_path,\n  replacement_statistics: replacementStats,\n  backup_information: backupInfo,\n  content_analysis: contentAnalysis,\n  diff_summary: diffSummary,\n  atomic_write_process: atomicProcess,\n  content_preview: {\n    old_content_sample: existingContent.slice(0, 3),\n    new_content_sample: finalContent.slice(0, 5),\n    total_lines_new: finalContent.length\n  },\n  format_details: {\n    requested_format: input.format,\n    formatting_applied: true,\n    indentation: ['json', 'yaml'].includes(input.format) ? 2 : null,\n    delimiter_used: ['csv', 'tsv'].includes(input.format) ? input.delimiter : null\n  },\n  integrity_checks: {\n    pre_write_validation: input.validate_before,\n    content_valid: true,\n    format_valid: true,\n    encoding_valid: true,\n    checksum_generated: Math.random().toString(36).substring(7)\n  },\n  warnings: [\n    !input.create_backup ? 'No backup created - original content will be permanently lost' : null,\n    !input.atomic_write ? 'Non-atomic write - consider enabling for data safety' : null,\n    replacementStats.bytes_difference > 1000000 ? 'Large file size change detected' : null\n  ].filter(Boolean),\n  recommendations: [\n    `Replacing entire file with ${finalContent.length} lines`,\n    input.create_backup ? 'Backup created successfully' : 'Consider enabling backup for safety',\n    replacementStats.change_percentage > '50%' ? 'Significant content change detected' : null,\n    'Verify new content before removing backup'\n  ].filter(Boolean),\n  metadata: {\n    ...input.metadata,\n    processing_time_ms: 10,\n    operation_type: 'destructive',\n    reversible: input.create_backup\n  }\n};\n\nreturn response;"
      },
      "id": "replace_full",
      "name": "Replace Full",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [1100, 550]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ JSON.stringify($json) }}"
      },
      "id": "success_response",
      "name": "Success Response",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [1600, 350]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseCode": 400,
        "responseBody": "{\n  \"success\": false,\n  \"error\": \"={{ $json.error || 'File writing failed' }}\",\n  \"action\": \"={{ $json.action || 'unknown' }}\",\n  \"file_path\": \"={{ $json.file_path || 'unknown' }}\",\n  \"timestamp\": \"={{ new Date().toISOString() }}\"\n}"
      },
      "id": "error_response",
      "name": "Error Response",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [1600, 550]
    },
    {
      "parameters": {
        "values": {
          "string": [
            {
              "name": "output_type",
              "value": "File List Writer Complete"
            }
          ]
        },
        "options": {}
      },
      "id": "final_output",
      "name": "Final Output",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [1800, 450]
    }
  ],
  "connections": {
    "Webhook Trigger": {
      "main": [
        [
          {
            "node": "Merge Triggers",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Manual Trigger": {
      "main": [
        [
          {
            "node": "Handle Empty Input (Manual Only)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Handle Empty Input (Manual Only)": {
      "main": [
        [
          {
            "node": "Merge Triggers",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge Triggers": {
      "main": [
        [
          {
            "node": "Validate Input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Validate Input": {
      "main": [
        [
          {
            "node": "Action Router",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Action Router": {
      "main": [
        [
          {
            "node": "Append Items",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Prepend Items",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Insert at Index",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Replace Full",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Append Items": {
      "main": [
        [
          {
            "node": "Success Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepend Items": {
      "main": [
        [
          {
            "node": "Success Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Insert at Index": {
      "main": [
        [
          {
            "node": "Success Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Replace Full": {
      "main": [
        [
          {
            "node": "Success Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Success Response": {
      "main": [
        [
          {
            "node": "Final Output",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Error Response": {
      "main": [
        [
          {
            "node": "Final Output",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "file-list-writer-v1.0.0",
  "meta": {
    "templateCredsSetupCompleted": true
  },
  "id": "file-list-writer",
  "tags": [
    {
      "createdAt": "2025-01-14T00:00:00.000Z",
      "updatedAt": "2025-01-14T00:00:00.000Z",
      "id": "shared-utility",
      "name": "shared-utility"
    },
    {
      "createdAt": "2025-01-14T00:00:00.000Z",
      "updatedAt": "2025-01-14T00:00:00.000Z",
      "id": "file-management",
      "name": "file-management"
    },
    {
      "createdAt": "2025-01-14T00:00:00.000Z",
      "updatedAt": "2025-01-14T00:00:00.000Z",
      "id": "data-writing",
      "name": "data-writing"
    },
    {
      "createdAt": "2025-01-14T00:00:00.000Z",
      "updatedAt": "2025-01-14T00:00:00.000Z",
      "id": "atomic-operations",
      "name": "atomic-operations"
    }
  ]
}