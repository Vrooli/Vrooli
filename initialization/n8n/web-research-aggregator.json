{
  "name": "Web Research Aggregator",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "research",
        "responseMode": "responseNode",
        "options": {}
      },
      "id": "webhook_trigger",
      "name": "Webhook Trigger",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1.1,
      "position": [200, 200]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Validate and prepare input for web research\nconst input = $input.item.json;\n\n// Required field validation\nif (!input.query) {\n  throw new Error('Missing required field: query');\n}\n\n// Extract and validate query\nconst query = input.query.trim();\nif (query.length === 0) {\n  throw new Error('Query cannot be empty');\n}\n\nif (query.length > 500) {\n  throw new Error('Query exceeds maximum length of 500 characters');\n}\n\n// Set default configuration\nconst defaultConfig = {\n  search_engines: ['google', 'bing', 'duckduckgo'],\n  max_results_per_engine: 10,\n  extraction_mode: 'smart',\n  synthesis_length: 'summary',\n  time_limit: 300,\n  include_academic: false,\n  sentiment_analysis: false,\n  extract_quotes: false\n};\n\n// Merge user config with defaults\nconst config = { ...defaultConfig, ...(input.config || {}) };\n\n// Validate configuration\nconst validEngines = ['google', 'bing', 'duckduckgo', 'scholar', 'news', 'twitter'];\nconfig.search_engines = config.search_engines.filter(engine => validEngines.includes(engine));\n\nif (config.search_engines.length === 0) {\n  config.search_engines = ['google', 'bing'];\n}\n\nconfig.max_results_per_engine = Math.min(Math.max(config.max_results_per_engine, 1), 20);\nconfig.time_limit = Math.min(Math.max(config.time_limit, 30), 600);\n\nconst validExtractionModes = ['comprehensive', 'headlines_only', 'smart'];\nif (!validExtractionModes.includes(config.extraction_mode)) {\n  config.extraction_mode = 'smart';\n}\n\nconst validSynthesisLengths = ['detailed', 'summary', 'bullet_points'];\nif (!validSynthesisLengths.includes(config.synthesis_length)) {\n  config.synthesis_length = 'summary';\n}\n\n// Prepare metadata\nconst metadata = {\n  start_time: new Date().toISOString(),\n  request_id: `research_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,\n  processing_start: Date.now()\n};\n\nreturn {\n  query: query,\n  config: config,\n  metadata: metadata\n};"
      },
      "id": "validate_input",
      "name": "Validate Input",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [400, 300]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "${service.n8n.url}/webhook/ollama",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "bodyContentType": "json",
        "jsonBody": "{\n  \"prompt\": \"You are a search strategy expert. Given this research query, generate 3-5 optimized search terms that would find the most relevant information. Focus on different angles and synonyms.\\n\\nOriginal Query: ={{ $json.query }}\\n\\nGenerate search terms as a JSON array with this exact format: {\\\"search_terms\\\": [\\\"term1\\\", \\\"term2\\\", \\\"term3\\\"]}\\n\\nConsider:\\n- Technical vs. general language\\n- Current vs. historical context\\n- Different perspectives on the topic\\n- Related concepts and synonyms\\n\\nProvide only the JSON response with search_terms array.\",\n  \"model\": \"llama3.2:3b\",\n  \"type\": \"reasoning\",\n  \"quiet\": true,\n  \"timeout_seconds\": 35\n}",
        "options": {
          "timeout": 35000
        }
      },
      "id": "search_strategy_planning",
      "name": "Search Strategy Planning",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [600, 300]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Parse Ollama response and execute searches\nconst planningResponse = $input.item.json;\nconst prevData = $('Validate Input').item.json;\n\n// Extract search terms from Ollama response\nlet searchTerms = [];\ntry {\n  const responseText = planningResponse.response || planningResponse.content || planningResponse;\n  const parsed = JSON.parse(responseText);\n  searchTerms = parsed.search_terms || [];\n} catch (error) {\n  console.warn('Failed to parse search strategy, using query as search term:', error.message);\n  searchTerms = [prevData.query];\n}\n\n// Fallback if no terms generated\nif (searchTerms.length === 0) {\n  searchTerms = [prevData.query];\n}\n\n// Prepare all search requests\nconst allResults = [];\nconst searchPromises = [];\nconst maxConcurrent = 3; // Limit concurrent requests\n\nconst engines = prevData.config.search_engines;\nconst maxResults = prevData.config.max_results_per_engine;\n\n// Execute searches for each engine/term combination\nfor (const engine of engines) {\n  for (const term of searchTerms.slice(0, 3)) { // Limit to 3 terms per engine\n    const searchUrl = `${service.searxng.url}/search`;\n    const params = new URLSearchParams({\n      q: term,\n      categories: engine === 'scholar' ? 'science' : 'general',\n      engines: engine,\n      format: 'json',\n      pageno: '1'\n    });\n    \n    try {\n      const response = await fetch(`${searchUrl}?${params}`, {\n        method: 'GET',\n        headers: {\n          'Accept': 'application/json',\n          'User-Agent': 'Vrooli-Research-Agent/1.0'\n        },\n        timeout: 15000\n      });\n      \n      if (response.ok) {\n        const data = await response.json();\n        if (data.results && Array.isArray(data.results)) {\n          const results = data.results\n            .slice(0, maxResults)\n            .map(result => ({\n              url: result.url,\n              title: result.title || '',\n              snippet: result.content || result.template || '',\n              engine: engine,\n              search_term: term,\n              publishedDate: result.publishedDate || null\n            }));\n          allResults.push(...results);\n        }\n      }\n    } catch (error) {\n      console.warn(`Search failed for ${engine}/${term}:`, error.message);\n    }\n  }\n}\n\n// Remove duplicates based on URL\nconst uniqueResults = [];\nconst seenUrls = new Set();\n\nfor (const result of allResults) {\n  if (!seenUrls.has(result.url) && result.url && result.title) {\n    seenUrls.add(result.url);\n    uniqueResults.push(result);\n  }\n}\n\n// Limit total results\nconst finalResults = uniqueResults.slice(0, 50);\n\nreturn {\n  query: prevData.query,\n  config: prevData.config,\n  metadata: {\n    ...prevData.metadata,\n    search_terms_used: searchTerms,\n    engines_used: engines,\n    raw_results_count: allResults.length,\n    unique_results_count: finalResults.length\n  },\n  search_results: finalResults\n};"
      },
      "id": "multi_source_search",
      "name": "Multi-Source Search",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [800, 300]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "${service.n8n.url}/webhook/ollama",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "bodyContentType": "json",
        "jsonBody": "{\n  \"prompt\": \"You are a relevance filtering expert. Evaluate these search results for the query and score their relevance.\\n\\nQuery: ={{ $json.query }}\\n\\nSearch Results:\\n={{ JSON.stringify($json.search_results.slice(0, 20), null, 2) }}\\n\\nFor each result, provide a relevance score (0.0-1.0) and decide if it should be extracted for content. Respond with this exact JSON format:\\n\\n{\\n  \\\"filtered_results\\\": [\\n    {\\n      \\\"url\\\": \\\"original_url\\\",\\n      \\\"title\\\": \\\"original_title\\\",\\n      \\\"snippet\\\": \\\"original_snippet\\\",\\n      \\\"relevance_score\\\": 0.85,\\n      \\\"should_extract\\\": true,\\n      \\\"reason\\\": \\\"why relevant or not\\\"\\n    }\\n  ]\\n}\\n\\nOnly include results with relevance_score >= 0.3. Mark should_extract=true for top results that would provide valuable content.\",\n  \"model\": \"llama3.2:3b\",\n  \"type\": \"reasoning\",\n  \"quiet\": true,\n  \"timeout_seconds\": 50\n}",
        "options": {
          "timeout": 50000
        }
      },
      "id": "relevance_filtering",
      "name": "Relevance Filtering",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [1000, 300]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Parse filtering response and extract content from relevant URLs\nconst filteringResponse = $input.item.json;\nconst prevData = $('Multi-Source Search').item.json;\n\n// Parse filtered results\nlet filteredResults = [];\ntry {\n  const responseText = filteringResponse.response || filteringResponse.content || filteringResponse;\n  const parsed = JSON.parse(responseText);\n  filteredResults = parsed.filtered_results || [];\n} catch (error) {\n  console.warn('Failed to parse filtering response, using top search results:', error.message);\n  // Fallback: use top 10 search results\n  filteredResults = prevData.search_results.slice(0, 10).map(result => ({\n    ...result,\n    relevance_score: 0.5,\n    should_extract: true,\n    reason: 'Fallback selection'\n  }));\n}\n\n// Filter for extraction candidates\nconst extractionCandidates = filteredResults\n  .filter(result => result.should_extract && result.relevance_score >= 0.3)\n  .slice(0, 8); // Limit to 8 extractions max\n\nconst extractedContent = [];\nconst browserlessUrl = '${service.browserless.url}';\n\n// Extract content from each candidate URL\nfor (const candidate of extractionCandidates) {\n  try {\n    const extractResponse = await fetch(`${browserlessUrl}/content`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({\n        url: candidate.url,\n        waitFor: 2000,\n        timeout: 15000,\n        selector: 'body',\n        rejectResourceTypes: ['image', 'stylesheet', 'font']\n      }),\n      timeout: 20000\n    });\n    \n    if (extractResponse.ok) {\n      const content = await extractResponse.text();\n      \n      // Clean and limit content\n      let cleanContent = content\n        .replace(/<[^>]*>/g, ' ') // Remove HTML tags\n        .replace(/\\s+/g, ' ') // Normalize whitespace\n        .trim();\n      \n      // Limit content size based on extraction mode\n      const maxLength = prevData.config.extraction_mode === 'comprehensive' ? 5000 :\n                       prevData.config.extraction_mode === 'headlines_only' ? 1000 : 2500;\n      \n      if (cleanContent.length > maxLength) {\n        cleanContent = cleanContent.substring(0, maxLength) + '...';\n      }\n      \n      if (cleanContent.length > 100) {\n        extractedContent.push({\n          ...candidate,\n          content: cleanContent,\n          extraction_success: true,\n          content_length: cleanContent.length\n        });\n      }\n    }\n  } catch (error) {\n    console.warn(`Content extraction failed for ${candidate.url}:`, error.message);\n    // Keep the result but mark extraction as failed\n    extractedContent.push({\n      ...candidate,\n      content: candidate.snippet || 'Content extraction failed',\n      extraction_success: false,\n      error: error.message\n    });\n  }\n}\n\nreturn {\n  query: prevData.query,\n  config: prevData.config,\n  metadata: {\n    ...prevData.metadata,\n    filtered_count: filteredResults.length,\n    extraction_attempted: extractionCandidates.length,\n    extraction_successful: extractedContent.filter(r => r.extraction_success).length\n  },\n  filtered_results: filteredResults,\n  extracted_content: extractedContent\n};"
      },
      "id": "content_extraction",
      "name": "Content Extraction",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [1200, 300]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "${service.n8n.url}/webhook/ollama",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "bodyContentType": "json",
        "jsonBody": "{\n  \"prompt\": \"You are a research synthesis expert. Create a comprehensive research summary based on the extracted content.\\n\\nOriginal Query: ={{ $json.query }}\\n\\nExtracted Content:\\n={{ JSON.stringify($json.extracted_content.map(item => ({ title: item.title, url: item.url, relevance_score: item.relevance_score, content: item.content.substring(0, 1000) })), null, 2) }}\\n\\nSynthesis Requirements:\\n- Length: ={{ $json.config.synthesis_length }}\\n- Include academic sources: ={{ $json.config.include_academic }}\\n- Include sentiment analysis: ={{ $json.config.sentiment_analysis }}\\n- Extract quotes: ={{ $json.config.extract_quotes }}\\n\\nCreate a research summary with this exact JSON format:\\n\\n{\\n  \\\"summary\\\": \\\"Comprehensive summary of findings\\\",\\n  \\\"key_findings\\\": [\\n    \\\"Key finding 1\\\",\\n    \\\"Key finding 2\\\",\\n    \\\"Key finding 3\\\"\\n  ],\\n  \\\"confidence_score\\\": 0.85,\\n  \\\"gaps_identified\\\": [\\n    \\\"Information gap 1\\\",\\n    \\\"Information gap 2\\\"\\n  ],\\n  \\\"sentiment\\\": \\\"positive|neutral|negative\\\",\\n  \\\"quotes\\\": [\\n    {\\n      \\\"text\\\": \\\"Notable quote\\\",\\n      \\\"source\\\": \\\"Source title or URL\\\"\\n    }\\n  ]\\n}\\n\\nEnsure the summary is well-structured, factual, and directly addresses the original query.\",\n  \"model\": \"llama3.2:3b\",\n  \"type\": \"reasoning\",\n  \"quiet\": true,\n  \"timeout_seconds\": 65\n}",
        "options": {
          "timeout": 65000
        }
      },
      "id": "synthesis_summary",
      "name": "Synthesis & Summary",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [1400, 300]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Process synthesis response and create final output\nconst synthesisResponse = $input.item.json;\nconst prevData = $('Content Extraction').item.json;\n\n// Parse synthesis results\nlet synthesis = {\n  summary: 'Research completed but synthesis failed',\n  key_findings: [],\n  confidence_score: 0.5,\n  gaps_identified: [],\n  sentiment: 'neutral',\n  quotes: []\n};\n\ntry {\n  const responseText = synthesisResponse.response || synthesisResponse.content || synthesisResponse;\n  const parsed = JSON.parse(responseText);\n  synthesis = { ...synthesis, ...parsed };\n} catch (error) {\n  console.warn('Failed to parse synthesis response:', error.message);\n  \n  // Fallback summary\n  const sourceCount = prevData.extracted_content.filter(r => r.extraction_success).length;\n  synthesis.summary = `Research completed on \"${prevData.query}\". Found ${sourceCount} relevant sources with content extracted from ${prevData.metadata.extraction_successful} pages. Due to synthesis processing issues, detailed analysis is not available.`;\n  synthesis.key_findings = [\n    `${prevData.metadata.unique_results_count} search results found`,\n    `${prevData.metadata.filtered_count} results passed relevance filtering`,\n    `${prevData.metadata.extraction_successful} pages successfully extracted`\n  ];\n}\n\n// Calculate processing time\nconst processingTime = Date.now() - prevData.metadata.processing_start;\n\n// Create final response\nconst response = {\n  success: true,\n  query: prevData.query,\n  config: prevData.config,\n  summary: synthesis.summary,\n  key_findings: synthesis.key_findings || [],\n  confidence_score: synthesis.confidence_score || 0.5,\n  gaps_identified: synthesis.gaps_identified || [],\n  sentiment: synthesis.sentiment || 'neutral',\n  quotes: synthesis.quotes || [],\n  sources: prevData.extracted_content.map(item => ({\n    url: item.url,\n    title: item.title,\n    relevance_score: item.relevance_score,\n    extraction_success: item.extraction_success,\n    content_snippet: item.content ? item.content.substring(0, 200) + '...' : item.snippet\n  })),\n  processing_time_ms: processingTime,\n  stats: {\n    searches_performed: prevData.metadata.search_terms_used?.length * prevData.metadata.engines_used?.length || 0,\n    results_found: prevData.metadata.raw_results_count,\n    results_filtered: prevData.metadata.filtered_count,\n    pages_extracted: prevData.metadata.extraction_successful,\n    confidence_score: synthesis.confidence_score\n  },\n  metadata: {\n    request_id: prevData.metadata.request_id,\n    completed_at: new Date().toISOString(),\n    processing_time_ms: processingTime\n  }\n};\n\nreturn response;"
      },
      "id": "format_response",
      "name": "Format Response",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [1600, 300]
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{ $json.success }}",
              "value2": true
            }
          ]
        }
      },
      "id": "check_success",
      "name": "Success Check",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [1800, 300]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ JSON.stringify($json) }}"
      },
      "id": "success_response",
      "name": "Success Response",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [2000, 200]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseCode": 500,
        "responseBody": "{\n  \"success\": false,\n  \"error\": \"={{ $json.error || 'Web research aggregation failed' }}\",\n  \"query\": \"={{ $json.query || 'unknown' }}\",\n  \"timestamp\": \"={{ new Date().toISOString() }}\",\n  \"request_id\": \"={{ $json.metadata?.request_id || 'unknown' }}\"\n}"
      },
      "id": "error_response",
      "name": "Error Response",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [2000, 400]
    },
    {
      "parameters": {},
      "id": "manual_trigger",
      "name": "Manual Trigger",
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [200, 400]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "const input = $input.item.json;\nif (!input || Object.keys(input).length === 0 || !input.query) {\n  return {\n    query: \"latest developments in renewable energy\",\n    max_results: 5,\n    search_depth: \"balanced\",\n    include_summaries: true\n  };\n}\nreturn input;"
      },
      "id": "handle_empty_input",
      "name": "Handle Empty Input (Manual Only)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [380, 400]
    },
    {
      "parameters": {},
      "id": "merge_triggers",
      "name": "Merge Triggers",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 2.1,
      "position": [520, 300]
    },
    {
      "parameters": {
        "values": {
          "string": [
            {
              "name": "output_type",
              "value": "Web Research Aggregator Complete"
            }
          ]
        },
        "options": {}
      },
      "id": "final_output",
      "name": "Final Output",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [2200, 300]
    }
  ],
  "connections": {
    "Webhook Trigger": {
      "main": [
        [
          {
            "node": "Merge Triggers",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Manual Trigger": {
      "main": [
        [
          {
            "node": "Handle Empty Input (Manual Only)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Handle Empty Input (Manual Only)": {
      "main": [
        [
          {
            "node": "Merge Triggers",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge Triggers": {
      "main": [
        [
          {
            "node": "Validate Input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Validate Input": {
      "main": [
        [
          {
            "node": "Search Strategy Planning",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Search Strategy Planning": {
      "main": [
        [
          {
            "node": "Multi-Source Search",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Multi-Source Search": {
      "main": [
        [
          {
            "node": "Relevance Filtering",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Relevance Filtering": {
      "main": [
        [
          {
            "node": "Content Extraction",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Content Extraction": {
      "main": [
        [
          {
            "node": "Synthesis & Summary",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Synthesis & Summary": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Response": {
      "main": [
        [
          {
            "node": "Success Check",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Success Check": {
      "main": [
        [
          {
            "node": "Success Response",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Error Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Success Response": {
      "main": [
        [
          {
            "node": "Final Output",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Error Response": {
      "main": [
        [
          {
            "node": "Final Output",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "research-v1.0.0",
  "meta": {
    "templateCredsSetupCompleted": true
  },
  "id": "web-research-aggregator",
  "tags": [
    {
      "createdAt": "2025-01-14T00:00:00.000Z",
      "updatedAt": "2025-01-14T00:00:00.000Z",
      "id": "shared-utility",
      "name": "shared-utility"
    },
    {
      "createdAt": "2025-01-14T00:00:00.000Z",
      "updatedAt": "2025-01-14T00:00:00.000Z",
      "id": "research",
      "name": "research"
    }
  ]
}