{
  "id": "7829564732190847201",
  "publicId": "data-cleaner-v1",
  "resourceType": "Routine",
  "isPrivate": false,
  "permissions": "{}",
  "isInternal": false,
  "tags": [],
  "versions": [
    {
      "id": "7829564732190847202",
      "publicId": "clean-data-v1",
      "versionLabel": "1.0.0",
      "versionNotes": "Initial version",
      "isComplete": true,
      "isPrivate": false,
      "versionIndex": 0,
      "isAutomatable": true,
      "resourceSubType": "RoutineCode",
      "config": {
        "__version": "1.0",
        "callDataCode": {
          "__version": "1.0",
          "schema": {
            "language": "python",
            "code": "import json\nimport re\nfrom collections import Counter\nfrom datetime import datetime\n\ndef clean_and_preprocess_data():\n    # Get inputs\n    raw_data = globals().get('raw_data', '')\n    data_format = globals().get('data_format', 'CSV')\n    cleaning_preferences = globals().get('cleaning_preferences', '')\n    data_purpose = globals().get('data_purpose', 'Analysis')\n    quality_threshold = globals().get('quality_threshold', 'Medium')\n    \n    def parse_data_format(data_text, format_type):\n        \"\"\"Parse data based on format type\"\"\"\n        if not data_text.strip():\n            # Return sample data for demonstration\n            return [\n                {'name': 'John Doe', 'email': 'john@email.com', 'age': '25', 'salary': '50000', 'date': '2024-01-15'},\n                {'name': 'Jane Smith', 'email': 'JANE@EMAIL.COM', 'age': '30', 'salary': '60,000', 'date': '01/20/2024'},\n                {'name': '', 'email': 'bob@email', 'age': 'thirty-five', 'salary': '$75000', 'date': '2024-02-30'},\n                {'name': 'Alice Johnson', 'email': 'alice@email.com', 'age': '28', 'salary': '', 'date': '2024-01-25'},\n                {'name': 'John Doe', 'email': 'john@email.com', 'age': '25', 'salary': '50000', 'date': '2024-01-15'},  # Duplicate\n            ]\n        \n        data_rows = []\n        lines = data_text.strip().split('\\n')\n        \n        if format_type.upper() == 'CSV':\n            if len(lines) > 1:\n                headers = [h.strip() for h in lines[0].split(',')]\n                for line in lines[1:]:\n                    if line.strip():\n                        values = [v.strip() for v in line.split(',')]\n                        row = {}\n                        for i, header in enumerate(headers):\n                            row[header] = values[i] if i < len(values) else ''\n                        data_rows.append(row)\n        elif format_type.upper() == 'JSON':\n            try:\n                data_rows = json.loads(data_text)\n                if isinstance(data_rows, dict):\n                    data_rows = [data_rows]\n            except:\n                data_rows = []\n        else:\n            # Try to parse as key-value pairs\n            for line in lines:\n                if ':' in line:\n                    key, value = line.split(':', 1)\n                    data_rows.append({key.strip(): value.strip()})\n        \n        return data_rows if data_rows else [{'sample': 'No data provided'}]\n    \n    def identify_data_issues(data):\n        \"\"\"Identify common data quality issues\"\"\"\n        issues = {\n            'missing_values': [],\n            'duplicates': [],\n            'format_inconsistencies': [],\n            'invalid_values': [],\n            'outliers': [],\n            'data_type_issues': []\n        }\n        \n        if not data:\n            return issues\n        \n        # Get all field names\n        all_fields = set()\n        for row in data:\n            all_fields.update(row.keys())\n        \n        # Check for missing values\n        for field in all_fields:\n            missing_count = sum(1 for row in data if not row.get(field, '').strip())\n            if missing_count > 0:\n                issues['missing_values'].append({\n                    'field': field,\n                    'missing_count': missing_count,\n                    'percentage': round((missing_count / len(data)) * 100, 1)\n                })\n        \n        # Check for duplicates\n        seen_rows = set()\n        for i, row in enumerate(data):\n            row_signature = tuple(sorted(row.items()))\n            if row_signature in seen_rows:\n                issues['duplicates'].append({\n                    'row_index': i,\n                    'duplicate_of': next(j for j, r in enumerate(data[:i]) \n                                       if tuple(sorted(r.items())) == row_signature)\n                })\n            seen_rows.add(row_signature)\n        \n        # Check for format inconsistencies\n        for field in all_fields:\n            values = [row.get(field, '') for row in data if row.get(field, '').strip()]\n            if values:\n                # Check date formats\n                if any(char in field.lower() for char in ['date', 'time']):\n                    date_formats = set()\n                    for value in values:\n                        if re.match(r'\\d{4}-\\d{2}-\\d{2}', value):\n                            date_formats.add('YYYY-MM-DD')\n                        elif re.match(r'\\d{2}/\\d{2}/\\d{4}', value):\n                            date_formats.add('MM/DD/YYYY')\n                        elif re.match(r'\\d{2}-\\d{2}-\\d{4}', value):\n                            date_formats.add('MM-DD-YYYY')\n                    \n                    if len(date_formats) > 1:\n                        issues['format_inconsistencies'].append({\n                            'field': field,\n                            'issue': 'Multiple date formats',\n                            'formats_found': list(date_formats)\n                        })\n                \n                # Check email formats\n                if 'email' in field.lower():\n                    invalid_emails = [v for v in values if '@' not in v or '.' not in v.split('@')[-1]]\n                    if invalid_emails:\n                        issues['invalid_values'].append({\n                            'field': field,\n                            'issue': 'Invalid email format',\n                            'invalid_count': len(invalid_emails),\n                            'examples': invalid_emails[:3]\n                        })\n                \n                # Check numeric fields\n                if any(char in field.lower() for char in ['age', 'salary', 'price', 'amount', 'count']):\n                    non_numeric = []\n                    for value in values:\n                        cleaned_value = re.sub(r'[,$%]', '', str(value))\n                        try:\n                            float(cleaned_value)\n                        except:\n                            non_numeric.append(value)\n                    \n                    if non_numeric:\n                        issues['data_type_issues'].append({\n                            'field': field,\n                            'issue': 'Non-numeric values in numeric field',\n                            'invalid_count': len(non_numeric),\n                            'examples': non_numeric[:3]\n                        })\n        \n        return issues\n    \n    def clean_data_records(data, cleaning_preferences, quality_threshold):\n        \"\"\"Clean data based on preferences and quality threshold\"\"\"\n        cleaned_data = []\n        cleaning_log = []\n        \n        for i, row in enumerate(data):\n            cleaned_row = {}\n            row_changes = []\n            \n            for field, value in row.items():\n                original_value = value\n                cleaned_value = str(value).strip() if value else ''\n                \n                # Standard cleaning operations\n                if cleaned_value:\n                    # Remove extra whitespace\n                    cleaned_value = re.sub(r'\\s+', ' ', cleaned_value)\n                    \n                    # Email cleaning\n                    if 'email' in field.lower():\n                        cleaned_value = cleaned_value.lower()\n                    \n                    # Numeric cleaning\n                    if any(char in field.lower() for char in ['age', 'salary', 'price', 'amount', 'count']):\n                        # Remove currency symbols and commas\n                        numeric_value = re.sub(r'[,$%]', '', cleaned_value)\n                        if numeric_value.replace('.', '').isdigit():\n                            cleaned_value = numeric_value\n                    \n                    # Date standardization\n                    if any(char in field.lower() for char in ['date', 'time']):\n                        # Try to standardize to YYYY-MM-DD\n                        date_match = re.match(r'(\\d{1,2})[/-](\\d{1,2})[/-](\\d{4})', cleaned_value)\n                        if date_match:\n                            month, day, year = date_match.groups()\n                            try:\n                                # Validate date\n                                datetime(int(year), int(month), int(day))\n                                cleaned_value = f\"{year}-{month.zfill(2)}-{day.zfill(2)}\"\n                            except:\n                                # Invalid date, keep original\n                                pass\n                    \n                    # Name cleaning\n                    if 'name' in field.lower():\n                        cleaned_value = cleaned_value.title()\n                \n                if original_value != cleaned_value:\n                    row_changes.append({\n                        'field': field,\n                        'original': original_value,\n                        'cleaned': cleaned_value,\n                        'change_type': 'standardization'\n                    })\n                \n                cleaned_row[field] = cleaned_value\n            \n            # Record changes\n            if row_changes:\n                cleaning_log.append({\n                    'row_index': i,\n                    'changes': row_changes\n                })\n            \n            cleaned_data.append(cleaned_row)\n        \n        return cleaned_data, cleaning_log\n    \n    def remove_duplicates(data):\n        \"\"\"Remove duplicate records\"\"\"\n        seen = set()\n        unique_data = []\n        duplicates_removed = []\n        \n        for i, row in enumerate(data):\n            row_signature = tuple(sorted((k, v) for k, v in row.items() if v))\n            \n            if row_signature not in seen:\n                seen.add(row_signature)\n                unique_data.append(row)\n            else:\n                duplicates_removed.append(i)\n        \n        return unique_data, duplicates_removed\n    \n    def handle_missing_values(data, strategy='remove'):\n        \"\"\"Handle missing values based on strategy\"\"\"\n        if strategy == 'remove':\n            # Remove rows with any missing values\n            complete_data = [row for row in data if all(v.strip() for v in row.values())]\n            removed_count = len(data) - len(complete_data)\n            return complete_data, f\"Removed {removed_count} rows with missing values\"\n        \n        elif strategy == 'fill':\n            # Fill missing values with defaults\n            filled_data = []\n            for row in data:\n                filled_row = {}\n                for field, value in row.items():\n                    if not value.strip():\n                        # Use field-specific defaults\n                        if any(char in field.lower() for char in ['age', 'count']):\n                            filled_row[field] = '0'\n                        elif 'email' in field.lower():\n                            filled_row[field] = 'unknown@example.com'\n                        elif 'name' in field.lower():\n                            filled_row[field] = 'Unknown'\n                        else:\n                            filled_row[field] = 'N/A'\n                    else:\n                        filled_row[field] = value\n                filled_data.append(filled_row)\n            return filled_data, \"Filled missing values with defaults\"\n        \n        return data, \"No missing value handling applied\"\n    \n    def validate_cleaned_data(data):\n        \"\"\"Validate the cleaned data quality\"\"\"\n        validation_results = {\n            'total_records': len(data),\n            'fields_count': len(data[0].keys()) if data else 0,\n            'completeness_score': 0,\n            'consistency_score': 0,\n            'validity_score': 0,\n            'overall_quality_score': 0\n        }\n        \n        if not data:\n            return validation_results\n        \n        # Calculate completeness score\n        total_cells = len(data) * len(data[0].keys())\n        filled_cells = sum(1 for row in data for value in row.values() if value.strip())\n        validation_results['completeness_score'] = round((filled_cells / total_cells) * 100, 1)\n        \n        # Calculate consistency score (simplified)\n        consistency_issues = 0\n        for field in data[0].keys():\n            values = [row[field] for row in data if row[field].strip()]\n            if values:\n                # Check for format consistency\n                if 'email' in field.lower():\n                    invalid_emails = sum(1 for v in values if '@' not in v)\n                    consistency_issues += invalid_emails\n        \n        consistency_score = max(0, 100 - (consistency_issues / len(data)) * 100)\n        validation_results['consistency_score'] = round(consistency_score, 1)\n        \n        # Calculate validity score (simplified)\n        validity_issues = 0\n        for field in data[0].keys():\n            values = [row[field] for row in data if row[field].strip()]\n            if 'date' in field.lower():\n                for value in values:\n                    try:\n                        datetime.strptime(value, '%Y-%m-%d')\n                    except:\n                        validity_issues += 1\n        \n        validity_score = max(0, 100 - (validity_issues / len(data)) * 100)\n        validation_results['validity_score'] = round(validity_score, 1)\n        \n        # Overall quality score\n        validation_results['overall_quality_score'] = round(\n            (validation_results['completeness_score'] + \n             validation_results['consistency_score'] + \n             validation_results['validity_score']) / 3, 1\n        )\n        \n        return validation_results\n    \n    def generate_cleaning_report(original_data, cleaned_data, issues, cleaning_log, duplicates_removed, validation_results):\n        \"\"\"Generate comprehensive cleaning report\"\"\"\n        report = {\n            'data_summary': {\n                'original_records': len(original_data),\n                'cleaned_records': len(cleaned_data),\n                'records_removed': len(original_data) - len(cleaned_data),\n                'data_reduction_percentage': round(((len(original_data) - len(cleaned_data)) / len(original_data)) * 100, 1) if original_data else 0\n            },\n            'issues_identified': {\n                'missing_values': len(issues['missing_values']),\n                'duplicates_found': len(issues['duplicates']),\n                'format_inconsistencies': len(issues['format_inconsistencies']),\n                'invalid_values': len(issues['invalid_values']),\n                'data_type_issues': len(issues['data_type_issues'])\n            },\n            'cleaning_actions': {\n                'standardizations_applied': len(cleaning_log),\n                'duplicates_removed': len(duplicates_removed),\n                'format_corrections': sum(len(log['changes']) for log in cleaning_log)\n            },\n            'quality_metrics': validation_results,\n            'recommendations': []\n        }\n        \n        # Generate recommendations\n        if validation_results['completeness_score'] < 80:\n            report['recommendations'].append('Consider investigating data source for missing value issues')\n        \n        if validation_results['consistency_score'] < 80:\n            report['recommendations'].append('Implement data validation rules at point of entry')\n        \n        if len(duplicates_removed) > len(original_data) * 0.1:\n            report['recommendations'].append('Review data collection process to prevent duplicates')\n        \n        if not report['recommendations']:\n            report['recommendations'].append('Data quality is good - maintain current data collection standards')\n        \n        return report\n    \n    # Main execution\n    try:\n        # Parse input data\n        original_data = parse_data_format(raw_data, data_format)\n        \n        # Identify issues\n        issues = identify_data_issues(original_data)\n        \n        # Clean data\n        cleaned_data, cleaning_log = clean_data_records(original_data, cleaning_preferences, quality_threshold)\n        \n        # Remove duplicates\n        cleaned_data, duplicates_removed = remove_duplicates(cleaned_data)\n        \n        # Handle missing values based on preferences\n        if 'remove missing' in cleaning_preferences.lower():\n            cleaned_data, missing_action = handle_missing_values(cleaned_data, 'remove')\n        elif 'fill missing' in cleaning_preferences.lower():\n            cleaned_data, missing_action = handle_missing_values(cleaned_data, 'fill')\n        else:\n            missing_action = 'No missing value handling applied'\n        \n        # Validate cleaned data\n        validation_results = validate_cleaned_data(cleaned_data)\n        \n        # Generate report\n        cleaning_report = generate_cleaning_report(\n            original_data, cleaned_data, issues, cleaning_log, duplicates_removed, validation_results\n        )\n        \n        # Create comprehensive output\n        output = {\n            'data_cleaning_results': {\n                'input_summary': {\n                    'original_record_count': len(original_data),\n                    'data_format': data_format,\n                    'cleaning_preferences': cleaning_preferences,\n                    'quality_threshold': quality_threshold\n                },\n                'issues_identified': issues,\n                'cleaned_dataset': cleaned_data[:10],  # Show first 10 records\n                'cleaning_transformations': cleaning_log[:10],  # Show first 10 transformations\n                'quality_assessment': validation_results\n            },\n            'cleaning_report': cleaning_report,\n            'recommendations': {\n                'immediate_actions': [\n                    'Review cleaned dataset for accuracy',\n                    'Validate critical fields manually',\n                    'Test cleaned data with intended analysis'\n                ],\n                'long_term_improvements': [\n                    'Implement data validation at source',\n                    'Regular data quality monitoring',\n                    'Standardize data entry procedures',\n                    'Train data collectors on quality standards'\n                ]\n            },\n            'export_options': {\n                'csv_ready': 'Data can be exported as clean CSV',\n                'json_ready': 'Data can be exported as clean JSON',\n                'analysis_ready': f'Dataset ready for {data_purpose.lower()}',\n                'backup_original': 'Original data preserved for reference'\n            }\n        }\n        \n        return json.dumps(output, indent=2)\n        \n    except Exception as e:\n        return json.dumps({\n            'error': f'Error in data cleaning process: {str(e)}',\n            'basic_guidance': {\n                'data_format': 'Ensure data is in CSV or JSON format',\n                'cleaning_steps': [\n                    'Remove duplicates',\n                    'Handle missing values',\n                    'Standardize formats',\n                    'Validate data types'\n                ]\n            }\n        }, indent=2)\n\n# Execute the function\nresult = clean_and_preprocess_data()\nprint(result)"
          }
        },
        "formInput": {
          "__version": "1.0",
          "schema": {
            "containers": [],
            "elements": [
              {
                "fieldName": "raw_data",
                "id": "raw_data_input",
                "label": "Raw Data",
                "type": "Text",
                "isRequired": false,
                "props": {
                  "placeholder": "Paste your raw data here (CSV format with headers, or JSON):\n\nExample CSV:\nname,email,age,salary,date\nJohn Doe,john@email.com,25,50000,2024-01-15\nJane Smith,JANE@EMAIL.COM,30,60,000,01/20/2024\n\nLeave blank to use sample data for demonstration.",
                  "minRows": 8
                }
              },
              {
                "fieldName": "data_format",
                "id": "data_format_input",
                "label": "Data Format",
                "type": "Selector",
                "isRequired": true,
                "props": {
                  "options": ["CSV", "JSON", "Tab-separated", "Key-value pairs"],
                  "defaultValue": "CSV"
                }
              },
              {
                "fieldName": "cleaning_preferences",
                "id": "cleaning_preferences_input",
                "label": "Cleaning Preferences",
                "type": "Text",
                "isRequired": true,
                "props": {
                  "placeholder": "What cleaning operations should be performed? (remove duplicates, fill missing values, standardize formats, remove outliers...)",
                  "minRows": 2,
                  "defaultValue": "Remove duplicates, standardize formats, handle missing values"
                }
              },
              {
                "fieldName": "data_purpose",
                "id": "data_purpose_input",
                "label": "Data Purpose",
                "type": "Selector",
                "isRequired": true,
                "props": {
                  "options": ["Analysis", "Machine learning", "Reporting", "Data migration", "Quality assessment", "Integration"],
                  "defaultValue": "Analysis"
                }
              },
              {
                "fieldName": "quality_threshold",
                "id": "quality_threshold_input",
                "label": "Quality Threshold",
                "type": "Selector",
                "isRequired": true,
                "props": {
                  "options": ["Low (basic cleaning)", "Medium (standard cleaning)", "High (strict cleaning)", "Very high (research quality)"],
                  "defaultValue": "Medium (standard cleaning)"
                }
              },
              {
                "fieldName": "missingValueStrategy",
                "id": "missing_value_strategy_input",
                "label": "Missing Value Strategy",
                "type": "Selector",
                "isRequired": true,
                "props": {
                  "options": ["Remove rows with missing values", "Fill with default values", "Fill with calculated values", "Keep as-is", "Flag for manual review"],
                  "defaultValue": "Remove rows with missing values"
                }
              },
              {
                "fieldName": "validationRules",
                "id": "validation_rules_input",
                "label": "Validation Rules (Optional)",
                "type": "Text",
                "isRequired": false,
                "props": {
                  "placeholder": "Any specific validation rules? (email format, date ranges, numeric limits...)",
                  "minRows": 2
                }
              }
            ]
          }
        },
        "formOutput": {
          "__version": "1.0",
          "schema": {
            "containers": [],
            "elements": [
              {
                "fieldName": "response",
                "id": "complete_cleaning_results_output",
                "label": "Complete Data Cleaning Results",
                "type": "Text"
              },
              {
                "fieldName": "cleanedDataset",
                "id": "cleaned_dataset_output",
                "label": "Cleaned Dataset Preview",
                "type": "Text"
              },
              {
                "fieldName": "issuesReport",
                "id": "issues_report_output",
                "label": "Data Issues Report",
                "type": "Text"
              },
              {
                "fieldName": "qualityAssessment",
                "id": "quality_assessment_output",
                "label": "Quality Assessment & Recommendations",
                "type": "Text"
              }
            ]
          }
        },
        "executionStrategy": "deterministic"
      },
      "translations": [
        {
          "id": "7829564732190847203",
          "language": "en",
          "name": "Data Cleaner",
          "description": "Cleans and preprocesses raw data for analysis. Handles missing values, duplicates, format standardization with comprehensive issue reports and quality assessment metrics.",
          "instructions": "1. Paste your raw data (or use sample data)\n2. Select data format\n3. Specify cleaning preferences\n4. Choose data purpose\n5. Set quality threshold\n6. Select missing value strategy\n7. Add validation rules (optional)\n\nThe cleaner will:\n- Identify data quality issues\n- Remove duplicates and handle missing values\n- Standardize formats and data types\n- Validate data integrity\n- Generate quality assessment metrics\n- Provide cleaned dataset preview\n- Create comprehensive cleaning report\n- Suggest long-term data quality improvements\n\nPerfect for data analysts, researchers, and anyone who needs clean, reliable data for analysis or machine learning."
        }
      ]
    }
  ]
}