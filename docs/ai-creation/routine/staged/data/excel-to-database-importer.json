{
  "id": "7829564732190847207",
  "publicId": "excel-db-import-v1",
  "resourceType": "Routine",
  "isPrivate": false,
  "permissions": "{}",
  "isInternal": false,
  "tags": [],
  "versions": [
    {
      "id": "7829564732190847208",
      "publicId": "db-importer-v1",
      "versionLabel": "1.0.0",
      "versionNotes": "Initial version",
      "isComplete": true,
      "isPrivate": false,
      "versionIndex": 0,
      "isAutomatable": true,
      "resourceSubType": "RoutineCode",
      "config": {
        "__version": "1.0",
        "callDataCode": {
          "__version": "1.0",
          "schema": {
            "language": "python",
            "code": "import json\nimport csv\nimport io\nimport re\nfrom datetime import datetime\n\ndef convert_excel_to_database():\n    # Get inputs\n    spreadsheet_data = globals().get('spreadsheet_data', '')\n    table_name = globals().get('table_name', 'imported_data')\n    column_mapping = globals().get('column_mapping', '')\n    data_types = globals().get('data_types', 'Auto-detect')\n    primary_key = globals().get('primary_key', '')\n    import_mode = globals().get('import_mode', 'Create new table')\n    \n    def parse_csv_data(data_text):\n        \"\"\"Parse CSV data from input text\"\"\"\n        if not data_text.strip():\n            # Sample data for demonstration\n            return [\n                ['customer_id', 'name', 'email', 'purchase_date', 'amount', 'status'],\n                ['1001', 'John Doe', 'john@email.com', '2024-01-15', '250.00', 'completed'],\n                ['1002', 'Jane Smith', 'jane@email.com', '2024-01-16', '175.50', 'completed'],\n                ['1003', 'Bob Johnson', 'bob@email.com', '2024-01-17', '320.00', 'pending'],\n                ['1004', 'Alice Brown', 'alice@email.com', '2024-01-18', '150.00', 'completed'],\n                ['1005', 'Charlie Davis', 'charlie@email.com', '2024-01-19', '500.00', 'refunded']\n            ]\n        \n        rows = []\n        reader = csv.reader(io.StringIO(data_text))\n        for row in reader:\n            rows.append(row)\n        \n        return rows if rows else [['column1'], ['value1']]\n    \n    def detect_data_types(data_rows):\n        \"\"\"Automatically detect SQL data types from data\"\"\"\n        if not data_rows or len(data_rows) < 2:\n            return {}\n        \n        headers = data_rows[0]\n        type_detection = {header: {'type': 'VARCHAR(255)', 'samples': []} for header in headers}\n        \n        # Analyze data to detect types\n        for row_idx, row in enumerate(data_rows[1:]):\n            if row_idx >= 100:  # Sample first 100 rows\n                break\n            \n            for col_idx, value in enumerate(row):\n                if col_idx < len(headers):\n                    header = headers[col_idx]\n                    if value and value.strip():\n                        type_detection[header]['samples'].append(value)\n        \n        # Determine final types based on samples\n        for header, info in type_detection.items():\n            samples = info['samples']\n            if not samples:\n                continue\n            \n            # Check if all samples are integers\n            if all(re.match(r'^-?\\d+$', s) for s in samples):\n                max_val = max(int(s) for s in samples)\n                min_val = min(int(s) for s in samples)\n                if min_val >= -128 and max_val <= 127:\n                    type_detection[header]['type'] = 'TINYINT'\n                elif min_val >= -32768 and max_val <= 32767:\n                    type_detection[header]['type'] = 'SMALLINT'\n                elif min_val >= -2147483648 and max_val <= 2147483647:\n                    type_detection[header]['type'] = 'INT'\n                else:\n                    type_detection[header]['type'] = 'BIGINT'\n            \n            # Check if all samples are decimals\n            elif all(re.match(r'^-?\\d+\\.\\d+$', s) for s in samples):\n                max_decimals = max(len(s.split('.')[1]) for s in samples)\n                max_total = max(len(s.replace('.', '').replace('-', '')) for s in samples)\n                type_detection[header]['type'] = f'DECIMAL({max_total + 2},{max_decimals})'\n            \n            # Check if all samples are dates\n            elif all(re.match(r'^\\d{4}-\\d{2}-\\d{2}$', s) for s in samples):\n                type_detection[header]['type'] = 'DATE'\n            \n            # Check if all samples are timestamps\n            elif all(re.match(r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', s) for s in samples):\n                type_detection[header]['type'] = 'TIMESTAMP'\n            \n            # Check if samples are boolean-like\n            elif all(s.lower() in ['true', 'false', '1', '0', 'yes', 'no'] for s in samples):\n                type_detection[header]['type'] = 'BOOLEAN'\n            \n            # Default to VARCHAR with appropriate length\n            else:\n                max_length = max(len(s) for s in samples)\n                if max_length <= 50:\n                    type_detection[header]['type'] = 'VARCHAR(50)'\n                elif max_length <= 255:\n                    type_detection[header]['type'] = 'VARCHAR(255)'\n                else:\n                    type_detection[header]['type'] = 'TEXT'\n        \n        return {k: v['type'] for k, v in type_detection.items()}\n    \n    def clean_column_name(name):\n        \"\"\"Clean column name for SQL compatibility\"\"\"\n        # Replace spaces and special characters with underscores\n        cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)\n        # Remove leading/trailing underscores\n        cleaned = cleaned.strip('_')\n        # Ensure it doesn't start with a number\n        if cleaned and cleaned[0].isdigit():\n            cleaned = 'col_' + cleaned\n        # Limit length\n        if len(cleaned) > 64:\n            cleaned = cleaned[:64]\n        return cleaned.lower() if cleaned else 'column'\n    \n    def generate_create_table_sql(table_name, columns, data_types, primary_key):\n        \"\"\"Generate CREATE TABLE SQL statement\"\"\"\n        sql_parts = [f'CREATE TABLE {table_name} (']\n        \n        column_definitions = []\n        for col in columns:\n            clean_col = clean_column_name(col)\n            col_type = data_types.get(col, 'VARCHAR(255)')\n            \n            col_def = f'  {clean_col} {col_type}'\n            \n            # Add NOT NULL for primary key\n            if col == primary_key:\n                col_def += ' NOT NULL'\n            \n            column_definitions.append(col_def)\n        \n        sql_parts.append(',\\n'.join(column_definitions))\n        \n        # Add primary key constraint\n        if primary_key and primary_key in columns:\n            clean_pk = clean_column_name(primary_key)\n            sql_parts.append(f',\\n  PRIMARY KEY ({clean_pk})')\n        \n        sql_parts.append('\\n);')\n        \n        return '\\n'.join(sql_parts)\n    \n    def generate_insert_sql(table_name, columns, data_rows):\n        \"\"\"Generate INSERT SQL statements\"\"\"\n        if not data_rows or len(data_rows) < 2:\n            return []\n        \n        clean_columns = [clean_column_name(col) for col in columns]\n        insert_statements = []\n        \n        # Generate INSERT statements in batches\n        batch_size = 100\n        for i in range(1, len(data_rows), batch_size):\n            batch_rows = data_rows[i:i + batch_size]\n            \n            values_parts = []\n            for row in batch_rows:\n                values = []\n                for j, value in enumerate(row[:len(columns)]):\n                    # Escape single quotes\n                    escaped_value = value.replace(\"'\", \"''\")\n                    # Handle NULL values\n                    if not escaped_value.strip():\n                        values.append('NULL')\n                    # Handle boolean values\n                    elif escaped_value.lower() in ['true', 'false']:\n                        values.append(escaped_value.upper())\n                    # Handle numeric values\n                    elif re.match(r'^-?\\d+(\\.\\d+)?$', escaped_value):\n                        values.append(escaped_value)\n                    # Default to string\n                    else:\n                        values.append(f\"'{escaped_value}'\")\n                \n                values_parts.append(f'  ({', '.join(values)})')\n            \n            insert_sql = f\"INSERT INTO {table_name} ({', '.join(clean_columns)}) VALUES\\n{',\\n'.join(values_parts)};\"\n            insert_statements.append(insert_sql)\n        \n        return insert_statements\n    \n    def create_column_mapping(original_columns):\n        \"\"\"Create mapping between original and SQL column names\"\"\"\n        mapping = {}\n        for col in original_columns:\n            mapping[col] = clean_column_name(col)\n        return mapping\n    \n    def generate_sample_queries(table_name, columns, primary_key):\n        \"\"\"Generate sample SQL queries for the imported data\"\"\"\n        clean_table = table_name\n        queries = []\n        \n        # Basic SELECT\n        queries.append({\n            'name': 'Select all records',\n            'sql': f'SELECT * FROM {clean_table};'\n        })\n        \n        # SELECT with LIMIT\n        queries.append({\n            'name': 'Select first 10 records',\n            'sql': f'SELECT * FROM {clean_table} LIMIT 10;'\n        })\n        \n        # COUNT query\n        queries.append({\n            'name': 'Count total records',\n            'sql': f'SELECT COUNT(*) AS total_records FROM {clean_table};'\n        })\n        \n        # If there's a primary key\n        if primary_key and primary_key in columns:\n            clean_pk = clean_column_name(primary_key)\n            queries.append({\n                'name': f'Find record by {primary_key}',\n                'sql': f\"SELECT * FROM {clean_table} WHERE {clean_pk} = 'VALUE';\"\n            })\n        \n        # If there are numeric columns\n        numeric_cols = [col for col, dtype in data_types.items() \n                       if any(t in dtype.upper() for t in ['INT', 'DECIMAL', 'FLOAT', 'NUMERIC'])]\n        if numeric_cols:\n            clean_col = clean_column_name(numeric_cols[0])\n            queries.append({\n                'name': f'Calculate sum of {numeric_cols[0]}',\n                'sql': f'SELECT SUM({clean_col}) AS total FROM {clean_table};'\n            })\n            queries.append({\n                'name': f'Find records where {numeric_cols[0]} > value',\n                'sql': f'SELECT * FROM {clean_table} WHERE {clean_col} > 100;'\n            })\n        \n        # If there are date columns\n        date_cols = [col for col, dtype in data_types.items() \n                    if any(t in dtype.upper() for t in ['DATE', 'TIMESTAMP'])]\n        if date_cols:\n            clean_col = clean_column_name(date_cols[0])\n            queries.append({\n                'name': f'Filter by date range',\n                'sql': f\"SELECT * FROM {clean_table} WHERE {clean_col} BETWEEN '2024-01-01' AND '2024-12-31';\"\n            })\n        \n        # GROUP BY example\n        if len(columns) > 1:\n            # Find a non-primary key column for grouping\n            group_col = None\n            for col in columns:\n                if col != primary_key and 'VARCHAR' in data_types.get(col, ''):\n                    group_col = col\n                    break\n            \n            if group_col:\n                clean_group = clean_column_name(group_col)\n                queries.append({\n                    'name': f'Group by {group_col}',\n                    'sql': f'SELECT {clean_group}, COUNT(*) as count FROM {clean_table} GROUP BY {clean_group};'\n                })\n        \n        return queries\n    \n    def generate_index_suggestions(table_name, columns, data_types):\n        \"\"\"Suggest indexes for better query performance\"\"\"\n        suggestions = []\n        \n        # Suggest indexes for foreign key-like columns\n        for col in columns:\n            if any(suffix in col.lower() for suffix in ['_id', 'id_', 'code', 'email']):\n                clean_col = clean_column_name(col)\n                suggestions.append({\n                    'column': col,\n                    'reason': 'Likely used in JOINs or lookups',\n                    'sql': f'CREATE INDEX idx_{table_name}_{clean_col} ON {table_name}({clean_col});'\n                })\n        \n        # Suggest indexes for date columns\n        for col, dtype in data_types.items():\n            if any(t in dtype.upper() for t in ['DATE', 'TIMESTAMP']):\n                clean_col = clean_column_name(col)\n                suggestions.append({\n                    'column': col,\n                    'reason': 'Date columns often used in range queries',\n                    'sql': f'CREATE INDEX idx_{table_name}_{clean_col} ON {table_name}({clean_col});'\n                })\n        \n        return suggestions\n    \n    # Main execution\n    try:\n        # Parse CSV data\n        data_rows = parse_csv_data(spreadsheet_data)\n        \n        if not data_rows or len(data_rows) < 2:\n            return json.dumps({\n                'error': 'Insufficient data rows',\n                'message': 'Please provide at least a header row and one data row'\n            }, indent=2)\n        \n        # Extract columns from header row\n        original_columns = data_rows[0]\n        \n        # Detect or use provided data types\n        if data_types == 'Auto-detect':\n            detected_types = detect_data_types(data_rows)\n        else:\n            # Parse user-provided types (simplified for this example)\n            detected_types = {col: 'VARCHAR(255)' for col in original_columns}\n        \n        # Create column mapping\n        column_name_mapping = create_column_mapping(original_columns)\n        \n        # Generate SQL statements\n        create_table_sql = generate_create_table_sql(table_name, original_columns, detected_types, primary_key)\n        insert_statements = generate_insert_sql(table_name, original_columns, data_rows)\n        \n        # Generate sample queries\n        sample_queries = generate_sample_queries(table_name, original_columns, primary_key)\n        \n        # Generate index suggestions\n        index_suggestions = generate_index_suggestions(table_name, original_columns, detected_types)\n        \n        # Data statistics\n        data_stats = {\n            'total_rows': len(data_rows) - 1,  # Exclude header\n            'total_columns': len(original_columns),\n            'estimated_size': sum(len(str(cell)) for row in data_rows for cell in row),\n            'null_count': sum(1 for row in data_rows[1:] for cell in row if not cell.strip())\n        }\n        \n        # Create comprehensive output\n        output = {\n            'database_import_results': {\n                'import_summary': {\n                    'source_format': 'CSV/Excel',\n                    'table_name': table_name,\n                    'import_mode': import_mode,\n                    'rows_to_import': data_stats['total_rows'],\n                    'columns_detected': data_stats['total_columns']\n                },\n                'schema_analysis': {\n                    'original_columns': original_columns,\n                    'sql_column_names': list(column_name_mapping.values()),\n                    'column_mapping': column_name_mapping,\n                    'detected_data_types': detected_types,\n                    'primary_key': primary_key if primary_key else 'None specified'\n                },\n                'data_statistics': data_stats\n            },\n            'sql_statements': {\n                'create_table': create_table_sql,\n                'insert_statements': insert_statements[:3],  # Show first 3 batches\n                'total_insert_batches': len(insert_statements),\n                'records_per_batch': 100\n            },\n            'sample_queries': sample_queries,\n            'optimization_suggestions': {\n                'indexes': index_suggestions,\n                'best_practices': [\n                    'Consider adding constraints (UNIQUE, CHECK) for data integrity',\n                    'Create foreign key relationships if this table references others',\n                    'Regularly update statistics for query optimization',\n                    'Consider partitioning for very large tables'\n                ]\n            },\n            'migration_guide': {\n                'pre_import_checklist': [\n                    'Backup existing database',\n                    'Verify target database has sufficient space',\n                    'Check user permissions for CREATE TABLE and INSERT',\n                    'Test with a small sample first'\n                ],\n                'import_steps': [\n                    '1. Execute CREATE TABLE statement',\n                    '2. Run INSERT statements in batches',\n                    '3. Verify row count matches source',\n                    '4. Create indexes for performance',\n                    '5. Update table statistics'\n                ],\n                'post_import_validation': [\n                    'Compare row counts',\n                    'Spot-check data accuracy',\n                    'Test sample queries',\n                    'Monitor query performance'\n                ]\n            },\n            'compatibility_notes': {\n                'mysql': 'All statements compatible with MySQL 5.7+',\n                'postgresql': 'Minor syntax adjustments may be needed for PostgreSQL',\n                'sqlite': 'Remove explicit type lengths for SQLite compatibility',\n                'sql_server': 'Use square brackets for column names in SQL Server'\n            }\n        }\n        \n        return json.dumps(output, indent=2)\n        \n    except Exception as e:\n        return json.dumps({\n            'error': f'Error in Excel to database conversion: {str(e)}',\n            'troubleshooting': {\n                'common_issues': [\n                    'Ensure CSV format is correct',\n                    'Check for special characters in data',\n                    'Verify column names are unique',\n                    'Remove any summary rows from data'\n                ]\n            }\n        }, indent=2)\n\n# Execute the function\nresult = convert_excel_to_database()\nprint(result)"
          }
        },
        "formInput": {
          "__version": "1.0",
          "schema": {
            "containers": [],
            "elements": [
              {
                "fieldName": "spreadsheet_data",
                "id": "spreadsheet_data_input",
                "label": "Spreadsheet Data (CSV Format)",
                "type": "Text",
                "isRequired": false,
                "props": {
                  "placeholder": "Paste your spreadsheet data in CSV format:\ncustomer_id,name,email,purchase_date,amount,status\n1001,John Doe,john@email.com,2024-01-15,250.00,completed\n1002,Jane Smith,jane@email.com,2024-01-16,175.50,completed\n\nOr leave blank to use sample data.",
                  "minRows": 8
                }
              },
              {
                "fieldName": "table_name",
                "id": "table_name_input",
                "label": "Table Name",
                "type": "Text",
                "isRequired": true,
                "props": {
                  "placeholder": "Enter database table name (e.g., customers, sales_data)",
                  "defaultValue": "imported_data"
                }
              },
              {
                "fieldName": "column_mapping",
                "id": "column_mapping_input",
                "label": "Column Name Mapping (Optional)",
                "type": "Text",
                "isRequired": false,
                "props": {
                  "placeholder": "Map original column names to SQL names (one per line):\nCustomer ID -> customer_id\nPurchase Date -> purchase_date\n\nLeave blank for automatic conversion.",
                  "minRows": 3
                }
              },
              {
                "fieldName": "data_types",
                "id": "data_types_input",
                "label": "Data Type Detection",
                "type": "Selector",
                "isRequired": true,
                "props": {
                  "options": ["Auto-detect", "All VARCHAR", "Custom mapping"],
                  "defaultValue": "Auto-detect"
                }
              },
              {
                "fieldName": "primary_key",
                "id": "primary_key_input",
                "label": "Primary Key Column",
                "type": "Text",
                "isRequired": false,
                "props": {
                  "placeholder": "Enter column name to use as primary key (optional)"
                }
              },
              {
                "fieldName": "import_mode",
                "id": "import_mode_input",
                "label": "Import Mode",
                "type": "Selector",
                "isRequired": true,
                "props": {
                  "options": ["Create new table", "Replace existing table", "Append to existing table"],
                  "defaultValue": "Create new table"
                }
              },
              {
                "fieldName": "database_type",
                "id": "database_type_input",
                "label": "Target Database Type",
                "type": "Selector",
                "isRequired": true,
                "props": {
                  "options": ["MySQL", "PostgreSQL", "SQLite", "SQL Server", "Generic SQL"],
                  "defaultValue": "MySQL"
                }
              }
            ]
          }
        },
        "formOutput": {
          "__version": "1.0",
          "schema": {
            "containers": [],
            "elements": [
              {
                "fieldName": "response",
                "id": "complete_import_results_output",
                "label": "Complete Import Results",
                "type": "Text"
              },
              {
                "fieldName": "sqlStatements",
                "id": "sql_statements_output",
                "label": "SQL Statements",
                "type": "Text"
              },
              {
                "fieldName": "sampleQueries",
                "id": "sample_queries_output",
                "label": "Sample Queries",
                "type": "Text"
              },
              {
                "fieldName": "migrationGuide",
                "id": "migration_guide_output",
                "label": "Migration Guide & Best Practices",
                "type": "Text"
              }
            ]
          }
        },
        "executionStrategy": "deterministic"
      },
      "translations": [
        {
          "id": "7829564732190847209",
          "language": "en",
          "name": "Excel-to-Database Importer",
          "description": "Converts spreadsheet data into queryable database format. Generates CREATE TABLE and INSERT statements with automatic data type detection and optimization suggestions.",
          "instructions": "1. Paste spreadsheet data in CSV format (or use sample)\n2. Enter desired table name\n3. Map column names if needed (optional)\n4. Choose data type detection method\n5. Specify primary key column (optional)\n6. Select import mode\n7. Choose target database type\n\nThe importer will:\n- Parse and analyze your spreadsheet data\n- Detect appropriate SQL data types\n- Clean column names for SQL compatibility\n- Generate CREATE TABLE statement\n- Create INSERT statements in batches\n- Provide sample queries for your data\n- Suggest indexes for optimization\n- Include migration guide and best practices\n\nPerfect for data analysts, developers, and anyone who needs to convert Excel/CSV data into a proper database structure for SQL querying."
        }
      ]
    }
  ]
}