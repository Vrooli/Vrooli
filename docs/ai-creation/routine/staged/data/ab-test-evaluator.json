{
  "id": "7829564732190847204",
  "publicId": "ab-test-eval-v1",
  "resourceType": "Routine",
  "isPrivate": false,
  "permissions": "{}",
  "isInternal": false,
  "tags": [],
  "versions": [
    {
      "id": "7829564732190847205",
      "publicId": "test-evaluator-v1",
      "versionLabel": "1.0.0",
      "versionNotes": "Initial version",
      "isComplete": true,
      "isPrivate": false,
      "versionIndex": 0,
      "isAutomatable": true,
      "resourceSubType": "RoutineCode",
      "config": {
        "__version": "1.0",
        "callDataCode": {
          "__version": "1.0",
          "schema": {
            "language": "python",
            "code": "import json\nimport math\nfrom scipy import stats\nimport numpy as np\n\ndef analyze_ab_test():\n    # Get inputs\n    variant_a_data = globals().get('variant_a_data', '')\n    variant_b_data = globals().get('variant_b_data', '')\n    success_metric = globals().get('success_metric', 'conversion_rate')\n    confidence_level = globals().get('confidence_level', '95%')\n    test_type = globals().get('test_type', 'Two-proportion z-test')\n    minimum_effect_size = globals().get('minimum_effect_size', '5%')\n    \n    def parse_variant_data(data_text, variant_name):\n        \"\"\"Parse variant data from input text\"\"\"\n        if not data_text.strip():\n            # Sample data for demonstration\n            if variant_name == 'A':\n                return {\n                    'visitors': 1000,\n                    'conversions': 120,\n                    'conversion_rate': 12.0,\n                    'revenue': 24000,\n                    'avg_order_value': 200\n                }\n            else:\n                return {\n                    'visitors': 1050,\n                    'conversions': 147,\n                    'conversion_rate': 14.0,\n                    'revenue': 27930,\n                    'avg_order_value': 190\n                }\n        \n        data = {}\n        lines = data_text.strip().split('\\n')\n        \n        for line in lines:\n            if ':' in line:\n                key, value = line.split(':', 1)\n                key = key.strip().lower().replace(' ', '_')\n                value = value.strip()\n                \n                # Extract numeric values\n                if any(char.isdigit() for char in value):\n                    # Remove common non-numeric characters\n                    numeric_value = ''.join(c for c in value if c.isdigit() or c == '.')\n                    try:\n                        if '.' in numeric_value:\n                            data[key] = float(numeric_value)\n                        else:\n                            data[key] = int(numeric_value)\n                    except:\n                        data[key] = value\n                else:\n                    data[key] = value\n        \n        # Calculate derived metrics if not provided\n        if 'visitors' in data and 'conversions' in data:\n            if 'conversion_rate' not in data:\n                data['conversion_rate'] = (data['conversions'] / data['visitors']) * 100\n        \n        if 'revenue' in data and 'conversions' in data and data['conversions'] > 0:\n            if 'avg_order_value' not in data:\n                data['avg_order_value'] = data['revenue'] / data['conversions']\n        \n        return data\n    \n    def perform_statistical_test(variant_a, variant_b, metric, test_method, conf_level):\n        \"\"\"Perform appropriate statistical test\"\"\"\n        results = {\n            'test_type': test_method,\n            'confidence_level': conf_level,\n            'statistically_significant': False,\n            'p_value': None,\n            'confidence_interval': None,\n            'effect_size': None,\n            'power': None\n        }\n        \n        # Extract confidence level as decimal\n        conf_decimal = float(conf_level.replace('%', '')) / 100\n        alpha = 1 - conf_decimal\n        \n        if metric == 'conversion_rate' and test_method == 'Two-proportion z-test':\n            # Two-proportion z-test\n            n1 = variant_a.get('visitors', 0)\n            n2 = variant_b.get('visitors', 0)\n            x1 = variant_a.get('conversions', 0)\n            x2 = variant_b.get('conversions', 0)\n            \n            if n1 > 0 and n2 > 0:\n                p1 = x1 / n1\n                p2 = x2 / n2\n                \n                # Pooled proportion\n                p_pooled = (x1 + x2) / (n1 + n2)\n                \n                # Standard error\n                se = math.sqrt(p_pooled * (1 - p_pooled) * (1/n1 + 1/n2))\n                \n                # Z-statistic\n                if se > 0:\n                    z_stat = (p2 - p1) / se\n                    \n                    # Two-tailed p-value\n                    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n                    \n                    # Effect size (difference in proportions)\n                    effect_size = p2 - p1\n                    \n                    # Confidence interval for difference\n                    se_diff = math.sqrt((p1 * (1 - p1) / n1) + (p2 * (1 - p2) / n2))\n                    margin_error = stats.norm.ppf(1 - alpha/2) * se_diff\n                    ci_lower = effect_size - margin_error\n                    ci_upper = effect_size + margin_error\n                    \n                    results.update({\n                        'p_value': round(p_value, 6),\n                        'z_statistic': round(z_stat, 4),\n                        'effect_size': round(effect_size, 6),\n                        'effect_size_percent': round(effect_size * 100, 2),\n                        'confidence_interval': [round(ci_lower, 6), round(ci_upper, 6)],\n                        'confidence_interval_percent': [round(ci_lower * 100, 2), round(ci_upper * 100, 2)],\n                        'statistically_significant': p_value < alpha\n                    })\n        \n        elif metric == 'revenue' and test_method == 'Two-sample t-test':\n            # Simplified t-test (assuming we have individual data points)\n            # For demonstration, we'll use approximation\n            rev_a = variant_a.get('revenue', 0)\n            rev_b = variant_b.get('revenue', 0)\n            visitors_a = variant_a.get('visitors', 1)\n            visitors_b = variant_b.get('visitors', 1)\n            \n            avg_rev_a = rev_a / visitors_a\n            avg_rev_b = rev_b / visitors_b\n            \n            # Simplified calculation (in real scenario, need individual data points)\n            effect_size = avg_rev_b - avg_rev_a\n            effect_size_percent = (effect_size / avg_rev_a) * 100 if avg_rev_a > 0 else 0\n            \n            results.update({\n                'effect_size': round(effect_size, 2),\n                'effect_size_percent': round(effect_size_percent, 2),\n                'note': 'Simplified calculation - full t-test requires individual data points'\n            })\n        \n        return results\n    \n    def calculate_sample_size_needed(variant_a, variant_b, min_effect, conf_level, power=0.8):\n        \"\"\"Calculate required sample size for desired power\"\"\"\n        try:\n            alpha = 1 - (float(conf_level.replace('%', '')) / 100)\n            min_effect_decimal = float(min_effect.replace('%', '')) / 100\n            \n            # Current conversion rates\n            p1 = variant_a.get('conversions', 0) / max(variant_a.get('visitors', 1), 1)\n            p2 = p1 + min_effect_decimal  # Expected improvement\n            \n            # Sample size calculation for two-proportion test\n            z_alpha = stats.norm.ppf(1 - alpha/2)\n            z_beta = stats.norm.ppf(power)\n            \n            p_avg = (p1 + p2) / 2\n            \n            numerator = (z_alpha * math.sqrt(2 * p_avg * (1 - p_avg)) + z_beta * math.sqrt(p1 * (1 - p1) + p2 * (1 - p2))) ** 2\n            denominator = (p2 - p1) ** 2\n            \n            n_per_group = math.ceil(numerator / denominator) if denominator > 0 else 0\n            \n            return {\n                'sample_size_per_group': n_per_group,\n                'total_sample_size': n_per_group * 2,\n                'assumptions': {\n                    'baseline_conversion': round(p1 * 100, 2),\n                    'target_conversion': round(p2 * 100, 2),\n                    'minimum_detectable_effect': min_effect,\n                    'power': power,\n                    'confidence_level': conf_level\n                }\n            }\n        except:\n            return {'sample_size_per_group': 'Unable to calculate', 'note': 'Check input parameters'}\n    \n    def assess_practical_significance(variant_a, variant_b, stat_results, min_effect):\n        \"\"\"Assess practical significance beyond statistical significance\"\"\"\n        assessment = {\n            'is_practically_significant': False,\n            'business_impact': 'Low',\n            'recommendation': 'Insufficient evidence for implementation',\n            'considerations': []\n        }\n        \n        try:\n            min_effect_decimal = float(min_effect.replace('%', '')) / 100\n            \n            if 'effect_size' in stat_results:\n                effect_size = abs(stat_results['effect_size'])\n                \n                # Check if effect size meets minimum threshold\n                if effect_size >= min_effect_decimal:\n                    assessment['is_practically_significant'] = True\n                    \n                    # Assess business impact based on effect size\n                    if effect_size >= min_effect_decimal * 3:\n                        assessment['business_impact'] = 'High'\n                        assessment['recommendation'] = 'Strong recommendation for implementation'\n                    elif effect_size >= min_effect_decimal * 2:\n                        assessment['business_impact'] = 'Medium'\n                        assessment['recommendation'] = 'Moderate recommendation for implementation'\n                    else:\n                        assessment['business_impact'] = 'Low to Medium'\n                        assessment['recommendation'] = 'Consider implementation with monitoring'\n                \n                # Add considerations\n                if stat_results.get('statistically_significant', False):\n                    assessment['considerations'].append('Statistically significant result')\n                else:\n                    assessment['considerations'].append('Not statistically significant - results may be due to chance')\n                \n                if assessment['is_practically_significant']:\n                    assessment['considerations'].append('Effect size meets minimum business threshold')\n                else:\n                    assessment['considerations'].append('Effect size below minimum business threshold')\n        except:\n            assessment['considerations'].append('Unable to assess practical significance')\n        \n        return assessment\n    \n    def generate_recommendations(variant_a, variant_b, stat_results, practical_assessment, sample_size_analysis):\n        \"\"\"Generate actionable recommendations\"\"\"\n        recommendations = {\n            'primary_recommendation': '',\n            'confidence_level': 'Low',\n            'next_steps': [],\n            'risk_factors': [],\n            'implementation_notes': []\n        }\n        \n        is_stat_sig = stat_results.get('statistically_significant', False)\n        is_practical_sig = practical_assessment.get('is_practically_significant', False)\n        \n        if is_stat_sig and is_practical_sig:\n            recommendations['primary_recommendation'] = 'Implement Variant B - both statistically and practically significant'\n            recommendations['confidence_level'] = 'High'\n            recommendations['next_steps'] = [\n                'Plan rollout strategy for Variant B',\n                'Monitor key metrics during implementation',\n                'Set up long-term tracking for sustained impact'\n            ]\n        elif is_stat_sig and not is_practical_sig:\n            recommendations['primary_recommendation'] = 'Exercise caution - statistically significant but small effect size'\n            recommendations['confidence_level'] = 'Medium'\n            recommendations['next_steps'] = [\n                'Consider cost-benefit analysis of implementation',\n                'Test with larger sample size for more precision',\n                'Evaluate if small gains justify implementation costs'\n            ]\n        elif not is_stat_sig and is_practical_sig:\n            recommendations['primary_recommendation'] = 'Insufficient evidence - large effect size but not statistically significant'\n            recommendations['confidence_level'] = 'Low'\n            recommendations['next_steps'] = [\n                'Continue testing with larger sample size',\n                'Check for confounding factors',\n                'Consider extending test duration'\n            ]\n        else:\n            recommendations['primary_recommendation'] = 'No clear winner - insufficient evidence for change'\n            recommendations['confidence_level'] = 'Low'\n            recommendations['next_steps'] = [\n                'Keep current variant (A)',\n                'Design new test with different approach',\n                'Increase sample size for future tests'\n            ]\n        \n        # Add risk factors\n        current_sample_a = variant_a.get('visitors', 0)\n        current_sample_b = variant_b.get('visitors', 0)\n        needed_sample = sample_size_analysis.get('sample_size_per_group', 0)\n        \n        if isinstance(needed_sample, int) and current_sample_a < needed_sample:\n            recommendations['risk_factors'].append('Underpowered test - sample size too small for reliable results')\n        \n        if stat_results.get('p_value', 1) > 0.05 and stat_results.get('p_value', 1) < 0.1:\n            recommendations['risk_factors'].append('Marginally significant result - interpret with caution')\n        \n        # Implementation notes\n        if recommendations['primary_recommendation'].startswith('Implement'):\n            recommendations['implementation_notes'] = [\n                'Monitor for novelty effect in first few weeks',\n                'Validate results across different user segments',\n                'Plan A/A test to verify measurement accuracy'\n            ]\n        \n        return recommendations\n    \n    # Main execution\n    try:\n        # Parse input data\n        variant_a = parse_variant_data(variant_a_data, 'A')\n        variant_b = parse_variant_data(variant_b_data, 'B')\n        \n        # Perform statistical test\n        statistical_results = perform_statistical_test(\n            variant_a, variant_b, success_metric, test_type, confidence_level\n        )\n        \n        # Calculate required sample size\n        sample_size_analysis = calculate_sample_size_needed(\n            variant_a, variant_b, minimum_effect_size, confidence_level\n        )\n        \n        # Assess practical significance\n        practical_significance = assess_practical_significance(\n            variant_a, variant_b, statistical_results, minimum_effect_size\n        )\n        \n        # Generate recommendations\n        recommendations = generate_recommendations(\n            variant_a, variant_b, statistical_results, practical_significance, sample_size_analysis\n        )\n        \n        # Calculate additional metrics\n        lift_calculation = {\n            'relative_lift': 0,\n            'absolute_lift': 0\n        }\n        \n        if success_metric == 'conversion_rate':\n            rate_a = variant_a.get('conversion_rate', 0)\n            rate_b = variant_b.get('conversion_rate', 0)\n            \n            if rate_a > 0:\n                lift_calculation['relative_lift'] = round(((rate_b - rate_a) / rate_a) * 100, 2)\n            lift_calculation['absolute_lift'] = round(rate_b - rate_a, 2)\n        \n        # Create comprehensive output\n        output = {\n            'ab_test_analysis': {\n                'test_summary': {\n                    'variant_a_performance': variant_a,\n                    'variant_b_performance': variant_b,\n                    'success_metric': success_metric,\n                    'test_duration': 'Not specified',\n                    'sample_size_adequacy': 'See sample size analysis section'\n                },\n                'statistical_analysis': statistical_results,\n                'practical_significance': practical_significance,\n                'lift_metrics': lift_calculation\n            },\n            'sample_size_analysis': sample_size_analysis,\n            'recommendations': recommendations,\n            'detailed_interpretation': {\n                'result_interpretation': {\n                    'winner': 'Variant B' if statistical_results.get('effect_size', 0) > 0 else 'Variant A',\n                    'confidence_in_result': recommendations['confidence_level'],\n                    'key_findings': [\n                        f\"Variant B {'outperformed' if statistical_results.get('effect_size', 0) > 0 else 'underperformed'} Variant A\",\n                        f\"Effect size: {statistical_results.get('effect_size_percent', 'N/A')}%\",\n                        f\"Statistical significance: {'Yes' if statistical_results.get('statistically_significant') else 'No'}\",\n                        f\"Practical significance: {'Yes' if practical_significance.get('is_practically_significant') else 'No'}\"\n                    ]\n                },\n                'statistical_concepts': {\n                    'p_value_explanation': 'Probability that observed difference occurred by chance',\n                    'confidence_interval_explanation': 'Range of plausible values for true effect size',\n                    'practical_significance_explanation': 'Whether effect size is large enough to matter for business',\n                    'type_i_error': 'Risk of concluding difference exists when it doesn\\'t',\n                    'type_ii_error': 'Risk of missing real difference due to insufficient power'\n                }\n            },\n            'next_steps_guide': {\n                'immediate_actions': recommendations['next_steps'],\n                'long_term_considerations': [\n                    'Design follow-up experiments to validate findings',\n                    'Monitor long-term impact if implementing changes',\n                    'Document learnings for future test design',\n                    'Consider segmented analysis for different user groups'\n                ],\n                'test_improvement_suggestions': [\n                    'Ensure randomization was proper',\n                    'Check for external factors during test period',\n                    'Validate measurement accuracy',\n                    'Consider multi-variate testing for next iteration'\n                ]\n            }\n        }\n        \n        return json.dumps(output, indent=2)\n        \n    except Exception as e:\n        return json.dumps({\n            'error': f'Error in A/B test analysis: {str(e)}',\n            'basic_guidance': {\n                'required_data': 'Visitors and conversions for both variants',\n                'analysis_steps': [\n                    'Collect sufficient sample size',\n                    'Ensure proper randomization',\n                    'Choose appropriate statistical test',\n                    'Interpret both statistical and practical significance'\n                ]\n            }\n        }, indent=2)\n\n# Execute the function\nresult = analyze_ab_test()\nprint(result)"
          }
        },
        "formInput": {
          "__version": "1.0",
          "schema": {
            "containers": [],
            "elements": [
              {
                "fieldName": "variant_a_data",
                "id": "variant_a_input",
                "label": "Variant A Data",
                "type": "Text",
                "isRequired": false,
                "props": {
                  "placeholder": "Enter Variant A performance data (one per line):\nVisitors: 1000\nConversions: 120\nRevenue: 24000\n\nOr leave blank to use sample data.",
                  "minRows": 4
                }
              },
              {
                "fieldName": "variant_b_data",
                "id": "variant_b_input",
                "label": "Variant B Data",
                "type": "Text",
                "isRequired": false,
                "props": {
                  "placeholder": "Enter Variant B performance data (one per line):\nVisitors: 1050\nConversions: 147\nRevenue: 27930\n\nOr leave blank to use sample data.",
                  "minRows": 4
                }
              },
              {
                "fieldName": "success_metric",
                "id": "success_metric_input",
                "label": "Primary Success Metric",
                "type": "Selector",
                "isRequired": true,
                "props": {
                  "options": ["conversion_rate", "revenue", "click_through_rate", "engagement_rate", "retention_rate"],
                  "defaultValue": "conversion_rate"
                }
              },
              {
                "fieldName": "confidence_level",
                "id": "confidence_level_input",
                "label": "Confidence Level",
                "type": "Selector",
                "isRequired": true,
                "props": {
                  "options": ["90%", "95%", "99%"],
                  "defaultValue": "95%"
                }
              },
              {
                "fieldName": "test_type",
                "id": "test_type_input",
                "label": "Statistical Test Type",
                "type": "Selector",
                "isRequired": true,
                "props": {
                  "options": ["Two-proportion z-test", "Two-sample t-test", "Chi-square test", "Mann-Whitney U test"],
                  "defaultValue": "Two-proportion z-test"
                }
              },
              {
                "fieldName": "minimum_effect_size",
                "id": "minimum_effect_size_input",
                "label": "Minimum Detectable Effect Size",
                "type": "Selector",
                "isRequired": true,
                "props": {
                  "options": ["1%", "2%", "5%", "10%", "15%", "20%"],
                  "defaultValue": "5%"
                }
              },
              {
                "fieldName": "testDuration",
                "id": "test_duration_input",
                "label": "Test Duration",
                "type": "Selector",
                "isRequired": false,
                "props": {
                  "options": ["1 week", "2 weeks", "1 month", "2 months", "3+ months", "Ongoing"],
                  "defaultValue": "2 weeks"
                }
              },
              {
                "fieldName": "businessContext",
                "id": "business_context_input",
                "label": "Business Context (Optional)",
                "type": "Text",
                "isRequired": false,
                "props": {
                  "placeholder": "Any business context that should influence the interpretation? (seasonality, marketing campaigns, etc.)",
                  "minRows": 2
                }
              }
            ]
          }
        },
        "formOutput": {
          "__version": "1.0",
          "schema": {
            "containers": [],
            "elements": [
              {
                "fieldName": "response",
                "id": "complete_analysis_output",
                "label": "Complete A/B Test Analysis",
                "type": "Text"
              },
              {
                "fieldName": "statisticalResults",
                "id": "statistical_results_output",
                "label": "Statistical Analysis Results",
                "type": "Text"
              },
              {
                "fieldName": "recommendations",
                "id": "recommendations_output",
                "label": "Recommendations & Next Steps",
                "type": "Text"
              },
              {
                "fieldName": "sampleSizeAnalysis",
                "id": "sample_size_analysis_output",
                "label": "Sample Size & Power Analysis",
                "type": "Text"
              }
            ]
          }
        },
        "executionStrategy": "deterministic"
      },
      "translations": [
        {
          "id": "7829564732190847206",
          "language": "en",
          "name": "A/B Test Evaluator",
          "description": "Analyzes A/B test results for statistical significance. Performs appropriate statistical tests, calculates confidence intervals, and provides winner recommendations with clear explanations.",
          "instructions": "1. Enter Variant A performance data (or use sample data)\n2. Enter Variant B performance data (or use sample data)\n3. Select primary success metric\n4. Choose confidence level\n5. Select statistical test type\n6. Set minimum detectable effect size\n7. Add test duration (optional)\n8. Provide business context (optional)\n\nThe evaluator will:\n- Perform appropriate statistical tests (z-test, t-test, etc.)\n- Calculate statistical significance and p-values\n- Assess practical significance vs statistical significance\n- Provide confidence intervals and effect sizes\n- Calculate required sample sizes for proper power\n- Generate clear recommendations with rationale\n- Explain statistical concepts in plain language\n- Suggest next steps and implementation guidance\n\nPerfect for marketers, product managers, and data analysts who need to make data-driven decisions from A/B test results."
        }
      ]
    }
  ]
}