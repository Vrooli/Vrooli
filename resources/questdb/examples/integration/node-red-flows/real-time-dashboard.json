[
  {
    "id": "flow_1",
    "type": "tab",
    "label": "QuestDB Real-time Dashboard",
    "disabled": false,
    "info": "Real-time monitoring dashboard using QuestDB data"
  },
  {
    "id": "inject_1",
    "type": "inject",
    "z": "flow_1",
    "name": "Every 10 seconds",
    "props": [{"p": "payload"}],
    "repeat": "10",
    "crontab": "",
    "once": true,
    "onceDelay": 0.1,
    "topic": "",
    "payload": "",
    "payloadType": "date",
    "x": 130,
    "y": 100,
    "wires": [["http_request_1"]]
  },
  {
    "id": "http_request_1",
    "type": "http request",
    "z": "flow_1",
    "name": "Query AI Metrics",
    "method": "GET",
    "ret": "obj",
    "paytoqs": "ignore",
    "url": "http://localhost:9009/exec?query=SELECT%20model%2C%20COUNT(*)%20as%20requests%2C%20AVG(response_time_ms)%20as%20avg_time%2C%20SUM(tokens_input%20%2B%20tokens_output)%20as%20total_tokens%20FROM%20ai_inference%20WHERE%20timestamp%20%3E%20now()%20-%201h%20GROUP%20BY%20model&fmt=json",
    "tls": "",
    "persist": false,
    "proxy": "",
    "x": 330,
    "y": 100,
    "wires": [["function_1", "chart_1"]]
  },
  {
    "id": "function_1",
    "type": "function",
    "z": "flow_1",
    "name": "Process Metrics",
    "func": "// Process QuestDB response for dashboard\nif (msg.payload && msg.payload.dataset) {\n    const metrics = {};\n    \n    msg.payload.dataset.forEach(row => {\n        const [model, requests, avgTime, totalTokens] = row;\n        metrics[model] = {\n            requests: requests || 0,\n            avgResponseTime: avgTime ? avgTime.toFixed(2) : 0,\n            totalTokens: totalTokens || 0\n        };\n    });\n    \n    // Send to dashboard\n    msg.payload = metrics;\n    msg.topic = \"ai_metrics\";\n    \n    // Also create chart data\n    const chartData = Object.keys(metrics).map(model => ({\n        x: model,\n        y: metrics[model].avgResponseTime\n    }));\n    \n    return [msg, {payload: chartData, topic: \"Response Time by Model\"}];\n}\n\nreturn null;",
    "outputs": 2,
    "noerr": 0,
    "x": 540,
    "y": 100,
    "wires": [["dashboard_1"], ["chart_2"]]
  },
  {
    "id": "dashboard_1",
    "type": "ui_text",
    "z": "flow_1",
    "group": "dashboard_group",
    "order": 1,
    "width": 0,
    "height": 0,
    "name": "AI Metrics Display",
    "label": "AI Model Performance",
    "format": "{{msg.payload}}",
    "layout": "col-center",
    "x": 750,
    "y": 80,
    "wires": []
  },
  {
    "id": "chart_1",
    "type": "ui_chart",
    "z": "flow_1",
    "name": "Request Volume",
    "group": "dashboard_group",
    "order": 2,
    "width": 0,
    "height": 0,
    "label": "Requests per Model",
    "chartType": "bar",
    "x": 540,
    "y": 180,
    "wires": [[]]
  },
  {
    "id": "chart_2",
    "type": "ui_chart",
    "z": "flow_1",
    "name": "Response Times",
    "group": "dashboard_group",
    "order": 3,
    "width": 0,
    "height": 0,
    "label": "Avg Response Time (ms)",
    "chartType": "line",
    "x": 750,
    "y": 140,
    "wires": [[]]
  },
  {
    "id": "inject_2",
    "type": "inject",
    "z": "flow_1",
    "name": "Send Test Metric",
    "props": [{"p": "payload"}],
    "repeat": "",
    "crontab": "",
    "once": false,
    "onceDelay": 0.1,
    "topic": "",
    "payload": "true",
    "payloadType": "bool",
    "x": 120,
    "y": 300,
    "wires": [["function_2"]]
  },
  {
    "id": "function_2",
    "type": "function",
    "z": "flow_1",
    "name": "Generate ILP Metric",
    "func": "// Generate InfluxDB Line Protocol metric\nconst models = ['llama3.2', 'gpt-4', 'claude-3', 'mistral-7b'];\nconst model = models[Math.floor(Math.random() * models.length)];\nconst responseTime = 100 + Math.random() * 400;\nconst tokensIn = Math.floor(50 + Math.random() * 200);\nconst tokensOut = Math.floor(100 + Math.random() * 500);\n\nmsg.payload = `ai_inference,model=${model},task=chat response_time_ms=${responseTime},tokens_input=${tokensIn}i,tokens_output=${tokensOut}i ${Date.now()}000000`;\nmsg.url = 'http://localhost:9003/write';\nmsg.method = 'POST';\n\nreturn msg;",
    "outputs": 1,
    "noerr": 0,
    "x": 330,
    "y": 300,
    "wires": [["http_request_2"]]
  },
  {
    "id": "http_request_2",
    "type": "http request",
    "z": "flow_1",
    "name": "Send to QuestDB ILP",
    "method": "use",
    "ret": "txt",
    "paytoqs": "ignore",
    "url": "",
    "tls": "",
    "persist": false,
    "proxy": "",
    "x": 560,
    "y": 300,
    "wires": [["debug_1"]]
  },
  {
    "id": "debug_1",
    "type": "debug",
    "z": "flow_1",
    "name": "ILP Response",
    "active": true,
    "tosidebar": true,
    "console": false,
    "tostatus": false,
    "complete": "true",
    "targetType": "full",
    "x": 760,
    "y": 300,
    "wires": []
  },
  {
    "id": "comment_1",
    "type": "comment",
    "z": "flow_1",
    "name": "Real-time Monitoring",
    "info": "This section queries QuestDB every 10 seconds for AI metrics and displays them on a dashboard",
    "x": 120,
    "y": 40,
    "wires": []
  },
  {
    "id": "comment_2",
    "type": "comment",
    "z": "flow_1",
    "name": "Metric Ingestion",
    "info": "This section demonstrates sending metrics to QuestDB using InfluxDB Line Protocol",
    "x": 110,
    "y": 240,
    "wires": []
  },
  {
    "id": "dashboard_group",
    "type": "ui_group",
    "name": "QuestDB Metrics",
    "tab": "dashboard_tab",
    "order": 1,
    "disp": true,
    "width": "6",
    "collapse": false
  },
  {
    "id": "dashboard_tab",
    "type": "ui_tab",
    "name": "QuestDB Dashboard",
    "icon": "dashboard",
    "order": 1,
    "disabled": false,
    "hidden": false
  }
]