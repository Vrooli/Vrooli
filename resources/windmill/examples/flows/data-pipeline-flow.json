{
  "name": "Data Pipeline Flow",
  "description": "Example ETL pipeline that fetches data from an API, transforms it, validates it, and loads it into a database",
  "flow_structure": {
    "nodes": [
      {
        "id": "extract_data",
        "type": "script",
        "path": "f/etl/extract_from_api",
        "summary": "Extract data from external API",
        "inputs": {
          "api_endpoint": "${flow.input.source_endpoint}",
          "api_key": "${resources.api_key}",
          "date_range": {
            "start": "${flow.input.start_date}",
            "end": "${flow.input.end_date}"
          }
        }
      },
      {
        "id": "validate_data",
        "type": "script",
        "path": "f/etl/validate_data",
        "summary": "Validate extracted data",
        "inputs": {
          "data": "${nodes.extract_data.output}",
          "validation_rules": {
            "required_fields": ["id", "timestamp", "value"],
            "value_range": {"min": 0, "max": 1000}
          }
        }
      },
      {
        "id": "transform_data",
        "type": "script",
        "path": "f/etl/transform_data",
        "summary": "Transform and enrich data",
        "inputs": {
          "raw_data": "${nodes.validate_data.output.valid_records}",
          "transformation_config": {
            "add_metadata": true,
            "normalize_values": true,
            "format": "database"
          }
        },
        "condition": "${nodes.validate_data.output.is_valid}"
      },
      {
        "id": "load_to_database",
        "type": "script",
        "path": "f/etl/load_to_postgres",
        "summary": "Load transformed data to database",
        "inputs": {
          "records": "${nodes.transform_data.output}",
          "table_name": "${flow.input.target_table}",
          "batch_size": 100
        },
        "condition": "${nodes.transform_data.output.length > 0}"
      },
      {
        "id": "send_notification",
        "type": "script",
        "path": "f/notifications/send_email",
        "summary": "Send completion notification",
        "inputs": {
          "to": "${flow.input.notification_email}",
          "subject": "ETL Pipeline Complete",
          "body": "Pipeline processed ${nodes.load_to_database.output.records_inserted} records successfully"
        }
      },
      {
        "id": "handle_error",
        "type": "script",
        "path": "f/notifications/send_alert",
        "summary": "Send error alert",
        "inputs": {
          "to": "${flow.input.notification_email}",
          "error": "${flow.error}",
          "context": {
            "pipeline": "data_pipeline",
            "timestamp": "${flow.start_time}"
          }
        },
        "condition": "${flow.has_error}"
      }
    ],
    "error_handling": {
      "retry_policy": {
        "max_attempts": 3,
        "backoff_seconds": [10, 30, 60]
      },
      "on_error": "handle_error"
    }
  },
  "input_schema": {
    "type": "object",
    "properties": {
      "source_endpoint": {
        "type": "string",
        "description": "API endpoint to extract data from"
      },
      "start_date": {
        "type": "string",
        "format": "date",
        "description": "Start date for data extraction"
      },
      "end_date": {
        "type": "string",
        "format": "date",
        "description": "End date for data extraction"
      },
      "target_table": {
        "type": "string",
        "description": "Database table to load data into"
      },
      "notification_email": {
        "type": "string",
        "format": "email",
        "description": "Email for notifications"
      }
    },
    "required": ["source_endpoint", "start_date", "end_date", "target_table", "notification_email"]
  },
  "schedule": {
    "cron": "0 2 * * *",
    "timezone": "UTC",
    "description": "Run daily at 2 AM UTC"
  },
  "required_scripts": [
    {
      "path": "f/etl/extract_from_api",
      "language": "typescript",
      "description": "Fetches data from external API with pagination support"
    },
    {
      "path": "f/etl/validate_data",
      "language": "typescript",
      "description": "Validates data structure and content"
    },
    {
      "path": "f/etl/transform_data",
      "language": "python",
      "description": "Transforms and enriches data for database storage"
    },
    {
      "path": "f/etl/load_to_postgres",
      "language": "typescript",
      "description": "Bulk loads data into PostgreSQL"
    },
    {
      "path": "f/notifications/send_email",
      "language": "typescript",
      "description": "Sends email notifications"
    },
    {
      "path": "f/notifications/send_alert",
      "language": "typescript",
      "description": "Sends error alerts with context"
    }
  ],
  "usage_example": {
    "trigger": "schedule",
    "input": {
      "source_endpoint": "/api/v1/sales",
      "start_date": "2025-01-01",
      "end_date": "2025-01-31",
      "target_table": "monthly_sales",
      "notification_email": "data-team@company.com"
    }
  }
}