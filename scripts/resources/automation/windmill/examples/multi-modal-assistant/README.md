# Multi-Modal AI Assistant Tutorial

**Build a complete voice-to-visual-to-action AI interface using Windmill + Vrooli resources**

A step-by-step guide to creating a professional multi-modal AI assistant that combines **Whisper** (voice), **Ollama** (intelligence), **ComfyUI** (visuals), and **Agent-S2** (automation) into a seamless user interface.

## ðŸŽ¯ What You'll Build

### **Complete AI Assistant Interface**
- **Voice Input**: Upload audio files or type text commands
- **Intelligence Layer**: AI understanding and response generation
- **Visual Creation**: Automated image generation based on commands
- **Screen Automation**: File management and application control
- **Professional UI**: Dashboard with real-time progress tracking

### **Business Value**
- **Revenue Potential**: $10,000-25,000 per project
- **Target Markets**: Accessibility services, creative professionals, enterprise productivity
- **Unique Selling Point**: Complete voice-to-visual-to-action automation

## ðŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Windmill UI   â”‚    â”‚  AI Orchestrator â”‚    â”‚   AI Resources  â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ - Audio Upload  â”‚â”€â”€â”€â–¶â”‚ - Workflow Mgmt  â”‚â”€â”€â”€â–¶â”‚ - Whisper API   â”‚
â”‚ - Progress View â”‚    â”‚ - State Tracking â”‚    â”‚ - Ollama LLM    â”‚
â”‚ - Results Panel â”‚â—„â”€â”€â”€â”‚ - Error Handling â”‚â—„â”€â”€â”€â”‚ - ComfyUI Gen   â”‚
â”‚ - Export Tools  â”‚    â”‚ - Context Memory â”‚    â”‚ - Agent-S2 Auto â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“‹ Prerequisites

### **Required Resources**
```bash
# Verify all required services are running
./scripts/resources/index.sh --action discover

# Should show these services as healthy:
# âœ… whisper is running on port 8090
# âœ… ollama is running on port 11434  
# âœ… comfyui is running on port 8188
# âœ… agent-s2 is running on port 4113
# âœ… windmill is running on port 5681
```

### **Windmill Access**
```bash
# Access Windmill interface
open http://localhost:5681

# Login with default credentials (change in production)
# Username: admin@windmill.dev
# Password: changeme
```

## ðŸš€ Step-by-Step Implementation

### **Step 1: Create App Foundation (15 minutes)**

**1.1 Create New App**
```typescript
// Navigate to Windmill UI â†’ Apps â†’ Create New App
// App Configuration:
{
  name: "Multi-Modal AI Assistant",
  description: "Voice-to-visual-to-action AI workflow",
  path: "multi_modal_assistant"
}
```

**1.2 Setup App Layout**
```typescript
// Layout Configuration (in App Builder):
const appLayout = {
  type: "tabs",
  tabs: [
    {
      name: "AI Assistant",
      components: [
        { type: "container", id: "input_section", width: 4 },
        { type: "container", id: "processing_section", width: 4 },
        { type: "container", id: "results_section", width: 4 }
      ]
    },
    {
      name: "Dashboard", 
      components: [
        { type: "container", id: "metrics_section", width: 12 }
      ]
    }
  ]
}
```

**1.3 Add Core Components**
```typescript
// Input Section Components:
// - File Input (audio upload)
// - Text Input (alternative to voice)
// - Processing Button
// - Clear/Reset Button

// Processing Section Components:
// - Progress Stepper
// - Status Messages
// - Live Logs

// Results Section Components:
// - Generated Images Gallery
// - Text Responses
// - Download/Export Options
```

### **Step 2: Backend Scripts (30 minutes)**

**2.1 Audio Processing Script**
```python
# scripts/process_audio.py
import requests
import base64
import json
from typing import Dict, Any, Optional

def process_audio(audio_file: bytes, file_name: str = "audio.wav") -> Dict[str, Any]:
    """
    Process audio file through Whisper API
    
    Args:
        audio_file: Raw audio file bytes
        file_name: Name of the audio file
        
    Returns:
        Dict containing transcription results or error
    """
    try:
        # Prepare file for Whisper API
        files = {
            'audio': (file_name, audio_file, 'audio/wav')
        }
        
        data = {
            'response_format': 'json',
            'language': 'en'  # Auto-detect if not specified
        }
        
        # Call Whisper API
        response = requests.post(
            'http://localhost:8090/transcribe',
            files=files,
            data=data,
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            return {
                "success": True,
                "transcription": result.get("text", ""),
                "language": result.get("language", "en"),
                "confidence": result.get("confidence", 0.0),
                "duration": result.get("duration", 0),
                "timestamp": response.headers.get("Date")
            }
        else:
            return {
                "success": False,
                "error": f"Whisper API error: {response.status_code}",
                "details": response.text
            }
            
    except requests.exceptions.RequestException as e:
        return {
            "success": False,
            "error": f"Request failed: {str(e)}"
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"Processing failed: {str(e)}"
        }

# Test the function
if __name__ == "__main__":
    # Test with dummy data
    test_result = process_audio(b"dummy_audio_data", "test.wav")
    print(json.dumps(test_result, indent=2))
```

**2.2 Intent Analysis Script**
```typescript
// scripts/analyze_intent.ts
interface IntentAnalysis {
  intent: string;
  parameters: Record<string, any>;
  confidence: number;
  next_actions: string[];
  visual_requirements?: {
    generate_image: boolean;
    style_parameters: Record<string, any>;
  };
  automation_requirements?: {
    screen_actions: boolean;
    file_operations: string[];
  };
}

export async function analyzeIntent(transcription: string): Promise<IntentAnalysis> {
  try {
    const analysisPrompt = `
Analyze this voice command and extract structured information:

COMMAND: "${transcription}"

Please provide a JSON response with:
1. intent: Primary action (create, modify, search, automate, etc.)
2. parameters: Key details (colors, styles, targets, etc.)
3. confidence: How confident you are (0-100)
4. next_actions: Array of specific steps to take
5. visual_requirements: If image generation is needed
6. automation_requirements: If screen/file actions are needed

Focus on actionable, specific analysis for a multi-modal AI assistant.
`;

    const response = await fetch('http://localhost:11434/api/generate', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'llama3.1:8b',
        prompt: analysisPrompt,
        stream: false,
        format: 'json'
      })
    });

    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.status}`);
    }

    const result = await response.json();
    
    // Parse the AI response
    let analysis: IntentAnalysis;
    try {
      analysis = JSON.parse(result.response);
    } catch (parseError) {
      // Fallback parsing if JSON is malformed
      analysis = {
        intent: extractIntent(transcription),
        parameters: extractParameters(transcription),
        confidence: 75,
        next_actions: ["process_with_fallback_method"],
        visual_requirements: {
          generate_image: transcription.toLowerCase().includes('create') || 
                          transcription.toLowerCase().includes('generate'),
          style_parameters: {}
        }
      };
    }

    return analysis;
    
  } catch (error) {
    throw new Error(`Intent analysis failed: ${error.message}`);
  }
}

function extractIntent(text: string): string {
  const lowerText = text.toLowerCase();
  
  if (lowerText.includes('create') || lowerText.includes('generate') || lowerText.includes('make')) {
    return 'create';
  } else if (lowerText.includes('modify') || lowerText.includes('change') || lowerText.includes('edit')) {
    return 'modify';
  } else if (lowerText.includes('search') || lowerText.includes('find')) {
    return 'search';
  } else if (lowerText.includes('automate') || lowerText.includes('do')) {
    return 'automate';
  } else {
    return 'unknown';
  }
}

function extractParameters(text: string): Record<string, any> {
  const params: Record<string, any> = {};
  
  // Extract colors
  const colors = text.match(/\b(red|blue|green|yellow|purple|orange|black|white|gray|pink|brown)\b/ig);
  if (colors) params.colors = colors;
  
  // Extract styles
  const styles = text.match(/\b(modern|classic|minimalist|professional|artistic|bold|elegant)\b/ig);
  if (styles) params.styles = styles;
  
  // Extract dimensions or sizes
  const sizes = text.match(/\b(small|medium|large|tiny|huge|banner|logo|icon)\b/ig);
  if (sizes) params.sizes = sizes;
  
  return params;
}
```

**2.3 Visual Generation Script**
```python
# scripts/generate_visuals.py
import requests
import json
import time
from typing import Dict, Any, Optional

def generate_image(
    prompt: str,
    style_params: Dict[str, Any] = None,
    dimensions: tuple = (512, 512)
) -> Dict[str, Any]:
    """
    Generate image using ComfyUI API
    
    Args:
        prompt: Text description for image generation
        style_params: Additional style parameters
        dimensions: Image dimensions (width, height)
        
    Returns:
        Dict containing generation results or error
    """
    try:
        # Build ComfyUI workflow
        workflow = create_comfyui_workflow(prompt, style_params, dimensions)
        
        # Submit workflow to ComfyUI
        response = requests.post(
            'http://localhost:8188/prompt',
            json={"prompt": workflow},
            timeout=10
        )
        
        if response.status_code == 200:
            result = response.json()
            prompt_id = result.get('prompt_id')
            
            if prompt_id:
                # Monitor generation progress
                return monitor_generation_progress(prompt_id)
            else:
                return {
                    "success": False,
                    "error": "No prompt ID returned from ComfyUI"
                }
        else:
            return {
                "success": False,
                "error": f"ComfyUI API error: {response.status_code}",
                "details": response.text
            }
            
    except Exception as e:
        return {
            "success": False,
            "error": f"Image generation failed: {str(e)}"
        }

def create_comfyui_workflow(
    prompt: str, 
    style_params: Dict[str, Any] = None,
    dimensions: tuple = (512, 512)
) -> Dict[str, Any]:
    """Create ComfyUI workflow JSON"""
    
    # Default style parameters
    if style_params is None:
        style_params = {
            "steps": 20,
            "cfg_scale": 7.0,
            "sampler": "euler_a",
            "scheduler": "normal"
        }
    
    # Enhanced prompt with style
    enhanced_prompt = f"{prompt}, high quality, professional"
    if style_params.get('style'):
        enhanced_prompt += f", {style_params['style']}"
    
    negative_prompt = "blur, low quality, distorted, unprofessional, nsfw"
    
    workflow = {
        "1": {
            "class_type": "CheckpointLoaderSimple",
            "inputs": {
                "ckpt_name": "sd_xl_base_1.0.safetensors"
            }
        },
        "2": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": enhanced_prompt,
                "clip": ["1", 1]
            }
        },
        "3": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": negative_prompt,
                "clip": ["1", 1]
            }
        },
        "4": {
            "class_type": "EmptyLatentImage",
            "inputs": {
                "width": dimensions[0],
                "height": dimensions[1],
                "batch_size": 1
            }
        },
        "5": {
            "class_type": "KSampler",
            "inputs": {
                "seed": -1,  # Random seed
                "steps": style_params.get('steps', 20),
                "cfg": style_params.get('cfg_scale', 7.0),
                "sampler_name": style_params.get('sampler', 'euler_a'),
                "scheduler": style_params.get('scheduler', 'normal'),
                "denoise": 1.0,
                "model": ["1", 0],
                "positive": ["2", 0],
                "negative": ["3", 0],
                "latent_image": ["4", 0]
            }
        },
        "6": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["5", 0],
                "vae": ["1", 2]
            }
        },
        "7": {
            "class_type": "SaveImage",
            "inputs": {
                "filename_prefix": "multimodal_assistant",
                "images": ["6", 0]
            }
        }
    }
    
    return workflow

def monitor_generation_progress(prompt_id: str, max_wait: int = 120) -> Dict[str, Any]:
    """Monitor ComfyUI generation progress"""
    
    start_time = time.time()
    
    while (time.time() - start_time) < max_wait:
        try:
            # Check queue status
            queue_response = requests.get('http://localhost:8188/queue')
            if queue_response.status_code == 200:
                queue_data = queue_response.json()
                
                # Check if our job is still in queue
                running = queue_data.get('queue_running', [])
                pending = queue_data.get('queue_pending', [])
                
                job_in_queue = any(
                    job[1] == prompt_id for job in running + pending
                )
                
                if not job_in_queue:
                    # Job completed, get results
                    history_response = requests.get(f'http://localhost:8188/history/{prompt_id}')
                    if history_response.status_code == 200:
                        history_data = history_response.json()
                        return {
                            "success": True,
                            "prompt_id": prompt_id,
                            "images": extract_image_paths(history_data),
                            "generation_time": time.time() - start_time
                        }
            
            time.sleep(2)  # Wait 2 seconds before checking again
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Progress monitoring failed: {str(e)}"
            }
    
    return {
        "success": False,
        "error": f"Generation timeout after {max_wait} seconds"
    }

def extract_image_paths(history_data: Dict[str, Any]) -> list:
    """Extract generated image paths from ComfyUI history"""
    images = []
    
    try:
        for node_id, node_data in history_data.items():
            if 'outputs' in node_data:
                for output_name, output_data in node_data['outputs'].items():
                    if 'images' in output_data:
                        for image_info in output_data['images']:
                            if 'filename' in image_info:
                                images.append({
                                    'filename': image_info['filename'],
                                    'subfolder': image_info.get('subfolder', ''),
                                    'type': image_info.get('type', 'output')
                                })
    except Exception as e:
        print(f"Error extracting image paths: {e}")
    
    return images
```

**2.4 Screen Automation Script**
```typescript
// scripts/automate_screen.ts
interface AutomationAction {
  type: 'click' | 'type' | 'screenshot' | 'file_operation';
  parameters: Record<string, any>;
}

interface AutomationResult {
  success: boolean;
  actions_completed: number;
  results: any[];
  error?: string;
}

export async function executeScreenAutomation(
  actions: AutomationAction[]
): Promise<AutomationResult> {
  const results: any[] = [];
  let actionsCompleted = 0;

  try {
    for (const action of actions) {
      const result = await executeAction(action);
      results.push(result);
      
      if (result.success) {
        actionsCompleted++;
      } else {
        // Stop on first failure for safety
        break;
      }
      
      // Small delay between actions
      await new Promise(resolve => setTimeout(resolve, 500));
    }

    return {
      success: actionsCompleted === actions.length,
      actions_completed: actionsCompleted,
      results: results
    };

  } catch (error) {
    return {
      success: false,
      actions_completed: actionsCompleted,
      results: results,
      error: error.message
    };
  }
}

async function executeAction(action: AutomationAction): Promise<any> {
  const baseUrl = 'http://localhost:4113';
  
  try {
    switch (action.type) {
      case 'screenshot':
        return await takeScreenshot();
        
      case 'click':
        return await clickMouse(action.parameters.x, action.parameters.y);
        
      case 'type':
        return await typeText(action.parameters.text);
        
      case 'file_operation':
        return await handleFileOperation(action.parameters);
        
      default:
        throw new Error(`Unknown action type: ${action.type}`);
    }
  } catch (error) {
    return {
      success: false,
      action: action.type,
      error: error.message
    };
  }
}

async function takeScreenshot(): Promise<any> {
  const response = await fetch('http://localhost:4113/screenshot?format=png&response_format=base64');
  
  if (response.ok) {
    const screenshotData = await response.text();
    return {
      success: true,
      action: 'screenshot',
      data: screenshotData,
      timestamp: new Date().toISOString()
    };
  } else {
    throw new Error(`Screenshot failed: ${response.status}`);
  }
}

async function clickMouse(x: number, y: number): Promise<any> {
  const response = await fetch('http://localhost:4113/mouse/click', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ x, y })
  });
  
  if (response.ok) {
    return {
      success: true,
      action: 'click',
      coordinates: { x, y }
    };
  } else {
    throw new Error(`Click failed: ${response.status}`);
  }
}

async function typeText(text: string): Promise<any> {
  const response = await fetch('http://localhost:4113/keyboard/type', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ text, delay: 50 })
  });
  
  if (response.ok) {
    return {
      success: true,
      action: 'type',
      text: text
    };
  } else {
    throw new Error(`Typing failed: ${response.status}`);
  }
}

async function handleFileOperation(params: any): Promise<any> {
  // Implement file operations like save, move, copy
  // This would depend on the specific file operation needed
  
  switch (params.operation) {
    case 'save_image':
      // Logic to save generated images to specified location
      return { success: true, action: 'save_image', path: params.path };
      
    case 'create_folder':
      // Logic to create folders
      return { success: true, action: 'create_folder', path: params.path };
      
    default:
      throw new Error(`Unknown file operation: ${params.operation}`);
  }
}
```

### **Step 3: UI Components (25 minutes)**

**3.1 Main Processing Interface**
```typescript
// UI Component Configuration in Windmill App Builder

// Audio Input Component
const audioInput = {
  type: "file_input",
  id: "audio_upload",
  configuration: {
    accept: [".wav", ".mp3", ".m4a", ".ogg"],
    multiple: false,
    placeholder: "Upload audio file or drop here",
    max_size_mb: 50
  },
  styling: {
    container: { marginBottom: "20px" }
  }
}

// Alternative Text Input
const textInput = {
  type: "textarea",
  id: "text_command", 
  configuration: {
    placeholder: "Or type your command here...",
    rows: 3
  },
  styling: {
    container: { marginBottom: "20px" }
  }
}

// Processing Button
const processButton = {
  type: "button",
  id: "process_command",
  configuration: {
    label: "Process Command",
    color: "blue",
    size: "lg",
    disabled: "${processing_status.output === 'processing'}"
  },
  event_handlers: {
    onClick: "start_processing_workflow"
  }
}

// Progress Stepper
const progressStepper = {
  type: "stepper",
  id: "processing_progress",
  configuration: {
    steps: [
      { label: "Audio Processing", icon: "ðŸŽ¤" },
      { label: "Intent Analysis", icon: "ðŸ§ " },
      { label: "Visual Generation", icon: "ðŸŽ¨" },
      { label: "Screen Automation", icon: "ðŸ¤–" }
    ],
    current_step: "${current_step.output}",
    show_numbers: true
  }
}

// Results Gallery
const resultsGallery = {
  type: "image_gallery",
  id: "generated_images",
  configuration: {
    images: "${generated_images.output}",
    columns: 2,
    height: "300px",
    downloadable: true
  }
}

// Conversation History
const conversationHistory = {
  type: "table",
  id: "conversation_log",
  configuration: {
    data: "${conversation_history.output}",
    columns: [
      { field: "timestamp", header: "Time", width: "150px" },
      { field: "input", header: "Input", width: "300px" },
      { field: "response", header: "AI Response", flex: 1 },
      { field: "status", header: "Status", width: "100px" }
    ],
    pagination: { enabled: true, page_size: 10 }
  }
}
```

**3.2 Real-time Status Dashboard**
```typescript
// Status Dashboard Components

// Resource Health Monitor
const resourceHealthMonitor = {
  type: "metrics_cards",
  id: "resource_health",
  configuration: {
    cards: [
      {
        title: "Whisper API",
        value: "${whisper_status.output}",
        icon: "ðŸŽ¤",
        color: "${whisper_status.output === 'healthy' ? 'green' : 'red'}"
      },
      {
        title: "Ollama LLM", 
        value: "${ollama_status.output}",
        icon: "ðŸ§ ",
        color: "${ollama_status.output === 'healthy' ? 'green' : 'red'}"
      },
      {
        title: "ComfyUI",
        value: "${comfyui_status.output}",
        icon: "ðŸŽ¨", 
        color: "${comfyui_status.output === 'healthy' ? 'green' : 'red'}"
      },
      {
        title: "Agent-S2",
        value: "${agent_s2_status.output}",
        icon: "ðŸ¤–",
        color: "${agent_s2_status.output === 'healthy' ? 'green' : 'red'}"
      }
    ]
  }
}

// Processing Metrics Chart
const processingMetrics = {
  type: "line_chart",
  id: "processing_metrics",
  configuration: {
    data: "${processing_metrics.output}",
    x_field: "timestamp",
    y_fields: ["response_time", "success_rate"],
    title: "AI Processing Performance",
    height: "300px"
  }
}
```

### **Step 4: Workflow Orchestration (20 minutes)**

**4.1 Main Processing Workflow**
```typescript
// scripts/main_workflow.ts
import { process_audio } from './process_audio.py';
import { analyzeIntent } from './analyze_intent.ts';
import { generate_image } from './generate_visuals.py';
import { executeScreenAutomation } from './automate_screen.ts';

interface WorkflowInput {
  audio_file?: any;
  text_command?: string;
  user_preferences?: Record<string, any>;
  session_id: string;
}

interface WorkflowOutput {
  success: boolean;
  results: {
    transcription?: string;
    intent_analysis?: any;
    generated_images?: any[];
    automation_results?: any;
  };
  error?: string;
  processing_time: number;
}

export async function executeMainWorkflow(input: WorkflowInput): Promise<WorkflowOutput> {
  const startTime = Date.now();
  const results: any = {};
  
  try {
    // Step 1: Process Audio Input (if provided)
    let commandText = input.text_command || '';
    
    if (input.audio_file) {
      console.log("Step 1: Processing audio...");
      const audioResult = await process_audio(input.audio_file, 'user_command.wav');
      
      if (audioResult.success) {
        commandText = audioResult.transcription;
        results.transcription = audioResult.transcription;
      } else {
        throw new Error(`Audio processing failed: ${audioResult.error}`);
      }
    }
    
    if (!commandText) {
      throw new Error("No command text available for processing");
    }
    
    // Step 2: Analyze Intent
    console.log("Step 2: Analyzing intent...");
    const intentAnalysis = await analyzeIntent(commandText);
    results.intent_analysis = intentAnalysis;
    
    // Step 3: Generate Visuals (if required)
    if (intentAnalysis.visual_requirements?.generate_image) {
      console.log("Step 3: Generating visuals...");
      
      const visualPrompt = createVisualPrompt(commandText, intentAnalysis);
      const imageResult = await generate_image(
        visualPrompt,
        intentAnalysis.visual_requirements.style_parameters
      );
      
      if (imageResult.success) {
        results.generated_images = imageResult.images;
      } else {
        console.warn(`Image generation failed: ${imageResult.error}`);
      }
    }
    
    // Step 4: Execute Screen Automation (if required)
    if (intentAnalysis.automation_requirements?.screen_actions) {
      console.log("Step 4: Executing screen automation...");
      
      const automationActions = createAutomationActions(
        intentAnalysis,
        results.generated_images
      );
      
      const automationResult = await executeScreenAutomation(automationActions);
      results.automation_results = automationResult;
    }
    
    return {
      success: true,
      results: results,
      processing_time: Date.now() - startTime
    };
    
  } catch (error) {
    return {
      success: false,
      results: results,
      error: error.message,
      processing_time: Date.now() - startTime
    };
  }
}

function createVisualPrompt(commandText: string, intentAnalysis: any): string {
  let prompt = commandText;
  
  // Enhance prompt with intent analysis
  if (intentAnalysis.parameters?.colors) {
    prompt += `, colors: ${intentAnalysis.parameters.colors.join(', ')}`;
  }
  
  if (intentAnalysis.parameters?.styles) {
    prompt += `, style: ${intentAnalysis.parameters.styles.join(', ')}`;
  }
  
  // Add quality modifiers
  prompt += ', high quality, professional, detailed';
  
  return prompt;
}

function createAutomationActions(intentAnalysis: any, generatedImages: any[]): any[] {
  const actions = [];
  
  // Take screenshot for context
  actions.push({
    type: 'screenshot',
    parameters: {}
  });
  
  // Save generated images if any
  if (generatedImages && generatedImages.length > 0) {
    generatedImages.forEach((image, index) => {
      actions.push({
        type: 'file_operation',
        parameters: {
          operation: 'save_image',
          source: image.filename,
          destination: `~/Downloads/ai_generated_${index + 1}.png`
        }
      });
    });
  }
  
  return actions;
}
```

**4.2 Workflow State Management**
```typescript
// scripts/state_manager.ts
interface AppState {
  session_id: string;
  current_step: number;
  processing_status: 'idle' | 'processing' | 'completed' | 'error';
  conversation_history: ConversationEntry[];
  generated_content: any[];
  user_preferences: Record<string, any>;
}

interface ConversationEntry {
  timestamp: string;
  input: string;
  response: string;
  status: 'success' | 'error';
  processing_time: number;
}

export class StateManager {
  private state: AppState;
  
  constructor(sessionId: string) {
    this.state = {
      session_id: sessionId,
      current_step: 0,
      processing_status: 'idle',
      conversation_history: [],
      generated_content: [],
      user_preferences: {}
    };
  }
  
  updateProcessingStep(step: number): void {
    this.state.current_step = step;
    this.state.processing_status = 'processing';
  }
  
  addConversationEntry(entry: ConversationEntry): void {
    this.state.conversation_history.push(entry);
    
    // Keep only last 50 entries
    if (this.state.conversation_history.length > 50) {
      this.state.conversation_history = this.state.conversation_history.slice(-50);
    }
  }
  
  setProcessingStatus(status: AppState['processing_status']): void {
    this.state.processing_status = status;
  }
  
  getState(): AppState {
    return { ...this.state };
  }
  
  exportState(): string {
    return JSON.stringify(this.state, null, 2);
  }
}

// Global state instance
export const appState = new StateManager(`session_${Date.now()}`);
```

### **Step 5: Testing and Deployment (10 minutes)**

**5.1 Test the Complete Workflow**
```bash
# Test individual components first
curl -X POST http://localhost:5681/api/scripts/test \
  -H "Content-Type: application/json" \
  -d '{"script": "process_audio", "args": {"test": true}}'

curl -X POST http://localhost:5681/api/scripts/test \
  -H "Content-Type: application/json" \
  -d '{"script": "analyze_intent", "args": {"test": true}}'

# Test complete workflow
curl -X POST http://localhost:5681/api/jobs/run \
  -H "Content-Type: application/json" \
  -d '{
    "script": "main_workflow",
    "args": {
      "text_command": "Create a professional logo with blue colors",
      "session_id": "test_session"
    }
  }'
```

**5.2 Deploy to Production**
```bash
# Set up production environment
windmill app deploy multi_modal_assistant --environment production

# Configure permissions
windmill permissions set --app multi_modal_assistant --role ai_operator

# Set up monitoring
windmill monitoring enable --app multi_modal_assistant --metrics all
```

## ðŸ“Š Usage and Testing

### **Testing the Complete System**
```bash
# Run the complete test scenario
./scripts/resources/tests/run.sh --scenarios "scenario=multi-modal-ai-assistant"

# This will test:
# âœ… Voice command processing
# âœ… Visual content generation  
# âœ… Screen interaction automation
# âœ… Multi-modal context understanding
# âœ… End-to-end workflow execution
```

### **Accessing Your Assistant**
```bash
# Access the complete interface
open http://localhost:5681/apps/multi_modal_assistant

# Monitor real-time status
open http://localhost:1880/ui/  # Node-RED dashboard for monitoring
```

## ðŸŽ¯ Business Applications

### **Accessibility Solutions ($10K-25K)**
- **Voice-controlled creative workflows** for users with mobility limitations
- **Screen reader integration** with AI-generated content descriptions
- **Alternative input methods** for users who cannot use traditional interfaces

### **Creative Professional Tools ($8K-20K)**
- **AI-assisted design workflows** that understand verbal creative direction
- **Automated asset generation** with voice commands
- **Brand consistency automation** across visual content

### **Enterprise Productivity ($5K-15K)**
- **Meeting assistance** with automated note-taking and visual summaries
- **Document processing** with voice commands and automated filing
- **Report generation** from verbal specifications

## ðŸ”§ Customization and Extensions

### **Adding New AI Services**
```typescript
// Extend the workflow to include new services
async function extendWorkflow(newServices: string[]) {
  for (const service of newServices) {
    switch (service) {
      case 'tts':
        // Add text-to-speech capabilities
        await setupTTSIntegration();
        break;
      case 'translation':
        // Add translation services  
        await setupTranslationService();
        break;
      case 'sentiment':
        // Add sentiment analysis
        await setupSentimentAnalysis();
        break;
    }
  }
}
```

### **Custom UI Themes**
```css
/* Accessibility-focused theme */
.accessibility-theme {
  --primary-color: #2563eb;
  --secondary-color: #64748b;
  --success-color: #059669;
  --warning-color: #d97706;
  --error-color: #dc2626;
  
  /* High contrast for visibility */
  --text-contrast: 8.5:1;
  --focus-outline: 3px solid var(--primary-color);
  
  /* Large touch targets */
  --button-min-height: 44px;
  --input-min-height: 44px;
}

/* Professional business theme */
.business-theme {
  --primary-color: #1e293b;
  --secondary-color: #64748b;
  --accent-color: #0ea5e9;
  
  /* Corporate styling */
  --border-radius: 8px;
  --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.1);
}
```

## ðŸ“ˆ Performance Optimization

### **Caching Strategy**
```typescript
// Implement intelligent caching for AI responses
const cacheManager = {
  // Cache frequent prompts
  promptCache: new Map<string, any>(),
  
  // Cache generated images
  imageCache: new Map<string, string>(),
  
  // Cache intent analysis for similar commands
  intentCache: new Map<string, any>(),
  
  getCachedResponse(key: string, type: 'prompt' | 'image' | 'intent'): any {
    switch (type) {
      case 'prompt':
        return this.promptCache.get(key);
      case 'image':
        return this.imageCache.get(key);
      case 'intent':
        return this.intentCache.get(key);
    }
  },
  
  setCachedResponse(key: string, value: any, type: 'prompt' | 'image' | 'intent'): void {
    switch (type) {
      case 'prompt':
        this.promptCache.set(key, value);
        break;
      case 'image':
        this.imageCache.set(key, value);
        break;
      case 'intent':
        this.intentCache.set(key, value);
        break;
    }
  }
};
```

### **Async Processing**
```typescript
// Process multiple AI services in parallel when possible
async function parallelProcessing(input: WorkflowInput): Promise<any> {
  const tasks = [];
  
  // Start all independent tasks simultaneously
  if (input.audio_file) {
    tasks.push(process_audio(input.audio_file));
  }
  
  if (input.context_analysis_required) {
    tasks.push(analyzeContext(input.session_id));
  }
  
  // Wait for all to complete
  const results = await Promise.allSettled(tasks);
  
  // Process results and handle any failures
  return processParallelResults(results);
}
```

## ðŸ”— Integration with Other Tools

### **Export to Professional Tools**
```typescript
// Export results to common professional formats
export async function exportResults(results: any, format: string): Promise<string> {
  switch (format) {
    case 'figma':
      return await exportToFigma(results.generated_images);
    case 'photoshop':
      return await exportToPhotoshop(results.generated_images);
    case 'pdf_report':
      return await generatePDFReport(results);
    case 'powerpoint':
      return await createPowerPointPresentation(results);
    default:
      throw new Error(`Unsupported export format: ${format}`);
  }
}
```

## ðŸ“š Additional Resources

- **Complete Code Repository**: [examples/multi-modal-assistant/src/](./src/)
- **Video Tutorial**: [Multi-Modal AI Assistant Walkthrough](./docs/video-tutorial.md)
- **Troubleshooting Guide**: [Common Issues and Solutions](./docs/troubleshooting.md)
- **API Reference**: [Complete API Documentation](./docs/api-reference.md)
- **Business Templates**: [Client Proposal Templates](./docs/business-templates/)

---

**ðŸŽ‰ Congratulations! You've built a complete multi-modal AI assistant that demonstrates the full power of Vrooli's resource ecosystem orchestrated through professional Windmill interfaces.**

**Next Steps**: Deploy to production, customize for specific client needs, and explore additional AI service integrations to expand capabilities and increase project value.