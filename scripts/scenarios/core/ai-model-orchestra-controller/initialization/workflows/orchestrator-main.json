[
  {
    "id": "http-in-select-model",
    "type": "http in",
    "z": "main-flow",
    "name": "Model Selection API",
    "url": "/flow/select-model",
    "method": "post",
    "upload": false,
    "swaggerDoc": "",
    "x": 160,
    "y": 100,
    "wires": [["validate-select-request"]]
  },
  {
    "id": "http-in-route-request",
    "type": "http in", 
    "z": "main-flow",
    "name": "Route AI Request",
    "url": "/flow/route-request",
    "method": "post",
    "upload": false,
    "swaggerDoc": "",
    "x": 160,
    "y": 200,
    "wires": [["validate-route-request"]]
  },
  {
    "id": "validate-select-request",
    "type": "function",
    "z": "main-flow",
    "name": "Validate Selection Request",
    "func": "// Validate required fields for model selection\nconst { taskType, requirements } = msg.payload;\n\nif (!taskType) {\n    msg.statusCode = 400;\n    msg.payload = { error: 'taskType is required' };\n    return [null, msg];\n}\n\n// Set defaults for requirements\nmsg.payload.requirements = {\n    complexity: 'moderate',\n    priority: 'normal',\n    maxTokens: 2048,\n    costLimit: null,\n    qualityRequirement: 'balanced',\n    ...requirements\n};\n\n// Add request metadata\nmsg.requestId = `req_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\nmsg.startTime = Date.now();\n\nreturn [msg, null];",
    "outputs": 2,
    "noerr": 0,
    "initialize": "",
    "finalize": "",
    "libs": [],
    "x": 400,
    "y": 100,
    "wires": [["get-system-metrics"], ["http-response-error"]]
  },
  {
    "id": "validate-route-request",
    "type": "function",
    "z": "main-flow", 
    "name": "Validate Route Request",
    "func": "// Validate required fields for request routing\nconst { taskType, prompt, requirements } = msg.payload;\n\nif (!taskType || !prompt) {\n    msg.statusCode = 400;\n    msg.payload = { error: 'taskType and prompt are required' };\n    return [null, msg];\n}\n\n// Set defaults\nmsg.payload.requirements = {\n    complexity: 'moderate',\n    priority: 'normal', \n    maxTokens: 2048,\n    costLimit: null,\n    qualityRequirement: 'balanced',\n    retryAttempts: 3,\n    ...requirements\n};\n\n// Add request metadata\nmsg.requestId = `req_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\nmsg.startTime = Date.now();\n\nreturn [msg, null];",
    "outputs": 2,
    "noerr": 0,
    "initialize": "",
    "finalize": "",
    "libs": [],
    "x": 400,
    "y": 200,
    "wires": [["get-system-metrics"], ["http-response-error"]]
  },
  {
    "id": "get-system-metrics",
    "type": "function",
    "z": "main-flow",
    "name": "Get System Metrics",
    "func": "// Get current system resource metrics\nconst fs = require('fs');\n\ntry {\n    // Get memory info\n    const memInfo = fs.readFileSync('/proc/meminfo', 'utf8');\n    const memTotal = parseInt(memInfo.match(/MemTotal:\\s+(\\d+)/)[1]) * 1024;\n    const memFree = parseInt(memInfo.match(/MemFree:\\s+(\\d+)/)[1]) * 1024;\n    const memAvailable = parseInt(memInfo.match(/MemAvailable:\\s+(\\d+)/)[1]) * 1024;\n    \n    // Get CPU load\n    const loadAvg = fs.readFileSync('/proc/loadavg', 'utf8');\n    const cpuLoad = parseFloat(loadAvg.split(' ')[0]);\n    \n    msg.systemMetrics = {\n        memory: {\n            total: memTotal,\n            free: memFree,\n            available: memAvailable,\n            total_gb: memTotal / 1024 / 1024 / 1024,\n            free_gb: memFree / 1024 / 1024 / 1024,\n            available_gb: memAvailable / 1024 / 1024 / 1024\n        },\n        cpu: {\n            load: cpuLoad,\n            usage: Math.min(cpuLoad * 20, 100)\n        }\n    };\n    \n    // Calculate memory pressure\n    msg.memoryPressure = 1 - (msg.systemMetrics.memory.available_gb / msg.systemMetrics.memory.total_gb);\n    \n    return msg;\n} catch (error) {\n    msg.statusCode = 500;\n    msg.payload = { error: 'Failed to get system metrics', details: error.message };\n    return [null, msg];\n}",
    "outputs": 2,
    "noerr": 0,
    "initialize": "",
    "finalize": "",
    "libs": [],
    "x": 640,
    "y": 150,
    "wires": [["load-model-capabilities"], ["http-response-error"]]
  },
  {
    "id": "load-model-capabilities",
    "type": "function",
    "z": "main-flow",
    "name": "Load Model Capabilities",
    "func": "// Load model capabilities from configuration\nconst fs = require('fs');\nconst path = require('path');\n\ntry {\n    const capabilitiesPath = path.join(__dirname, '../configuration/model-capabilities.json');\n    const capabilities = JSON.parse(fs.readFileSync(capabilitiesPath, 'utf8'));\n    \n    msg.modelCapabilities = capabilities;\n    \n    return msg;\n} catch (error) {\n    msg.statusCode = 500;\n    msg.payload = { error: 'Failed to load model capabilities', details: error.message };\n    return [null, msg];\n}",
    "outputs": 2,
    "noerr": 0,
    "initialize": "",
    "finalize": "",
    "libs": [],
    "x": 880,
    "y": 150,
    "wires": [["check-model-health"], ["http-response-error"]]
  },
  {
    "id": "check-model-health",
    "type": "http request",
    "z": "main-flow",
    "name": "Check Ollama Models",
    "method": "GET",
    "ret": "obj",
    "paytoqs": "ignore",
    "url": "http://localhost:11434/api/tags",
    "tls": "",
    "persist": false,
    "proxy": "",
    "authType": "",
    "senderr": false,
    "x": 1120,
    "y": 150,
    "wires": [["process-model-health"], ["handle-ollama-error"]]
  },
  {
    "id": "process-model-health",
    "type": "function",
    "z": "main-flow",
    "name": "Process Model Health",
    "func": "// Process Ollama model health response\nconst ollamaResponse = msg.payload;\n\nmsg.availableModels = {};\n\nif (ollamaResponse && ollamaResponse.models) {\n    ollamaResponse.models.forEach(model => {\n        msg.availableModels[model.name] = {\n            healthy: true,\n            size_gb: model.size ? model.size / (1024 * 1024 * 1024) : 0,\n            last_check: new Date().toISOString()\n        };\n    });\n}\n\nreturn msg;",
    "outputs": 1,
    "noerr": 0,
    "initialize": "",
    "finalize": "",
    "libs": [],
    "x": 1360,
    "y": 150,
    "wires": [["select-optimal-model"]]
  },
  {
    "id": "handle-ollama-error",
    "type": "function",
    "z": "main-flow",
    "name": "Handle Ollama Error",
    "func": "// Handle Ollama connection errors gracefully\nnode.warn('Ollama health check failed: ' + (msg.payload.message || 'Unknown error'));\n\n// Continue with empty model list - will trigger fallback\nmsg.availableModels = {};\n\nreturn msg;",
    "outputs": 1,
    "noerr": 0,
    "initialize": "",
    "finalize": "",
    "libs": [],
    "x": 1360,
    "y": 100,
    "wires": [["select-optimal-model"]]
  },
  {
    "id": "select-optimal-model",
    "type": "function",
    "z": "main-flow",
    "name": "Select Optimal Model",
    "func": "// Intelligent model selection algorithm\nfunction selectOptimalModel(taskType, requirements, capabilities, systemMetrics, availableModels) {\n    const {\n        complexity = 'moderate',\n        priority = 'normal',\n        maxTokens = 2048,\n        costLimit = null,\n        qualityRequirement = 'balanced'\n    } = requirements;\n    \n    // Get models capable of this task type\n    const capableModels = Object.entries(capabilities.models)\n        .filter(([name, model]) => model.capabilities.includes(taskType))\n        .filter(([name, model]) => availableModels[name] && availableModels[name].healthy)\n        .map(([name, model]) => ({ name, ...model }));\n    \n    if (capableModels.length === 0) {\n        return null;\n    }\n    \n    // Filter by resource availability\n    const memoryPressure = 1 - (systemMetrics.memory.available_gb / systemMetrics.memory.total_gb);\n    const resourceViableModels = capableModels.filter(model => {\n        const ramNeeded = model.ram_required_gb + 1.5; // Safety buffer\n        return systemMetrics.memory.available_gb > ramNeeded;\n    });\n    \n    if (resourceViableModels.length === 0) {\n        // Fallback to smallest capable model\n        const smallest = capableModels.reduce((prev, curr) => \n            curr.ram_required_gb < prev.ram_required_gb ? curr : prev\n        );\n        return { model: smallest.name, fallback: true, reason: 'resource_pressure' };\n    }\n    \n    // Score models based on requirements\n    const scoredModels = resourceViableModels.map(model => {\n        let score = 0;\n        \n        // Speed scoring (higher is better)\n        const speedScore = model.speed === 'very_fast' ? 4 : \n                          model.speed === 'fast' ? 3 : \n                          model.speed === 'moderate' ? 2 : \n                          model.speed === 'slow' ? 1 : 0;\n        \n        // Cost scoring (lower cost is better)\n        const costScore = costLimit ? \n            Math.max(0, 3 - (model.cost_per_1k_tokens / costLimit * 3)) : 2;\n        \n        // Quality scoring based on quality tier\n        const qualityScore = model.quality_tier === 'exceptional' ? 4 :\n                           model.quality_tier === 'excellent' ? 3 :\n                           model.quality_tier === 'very_good' ? 2 :\n                           model.quality_tier === 'good' ? 1 : 0;\n        \n        // Resource efficiency scoring\n        const resourceScore = memoryPressure > 0.7 ? \n            Math.max(0, 3 - (model.ram_required_gb / 10)) : 2;\n        \n        // Weight scores based on requirements\n        const weights = {\n            speed: priority === 'critical' ? 0.6 : priority === 'high' ? 0.4 : 0.2,\n            cost: costLimit ? 0.4 : 0.1,\n            quality: complexity === 'complex' ? 0.5 : qualityRequirement === 'high' ? 0.4 : 0.2,\n            resource: memoryPressure > 0.5 ? 0.3 : 0.1\n        };\n        \n        score = (speedScore * weights.speed) + \n                (costScore * weights.cost) + \n                (qualityScore * weights.quality) + \n                (resourceScore * weights.resource);\n        \n        return { model, score };\n    });\n    \n    // Sort by score and return best model\n    scoredModels.sort((a, b) => b.score - a.score);\n    return { \n        model: scoredModels[0].model.name, \n        fallback: false, \n        score: scoredModels[0].score,\n        alternatives: scoredModels.slice(1, 3).map(m => m.model.name)\n    };\n}\n\n// Perform selection\nconst selection = selectOptimalModel(\n    msg.payload.taskType,\n    msg.payload.requirements,\n    msg.modelCapabilities,\n    msg.systemMetrics,\n    msg.availableModels\n);\n\nif (!selection || !selection.model) {\n    msg.statusCode = 404;\n    msg.payload = { \n        error: 'No suitable model found',\n        taskType: msg.payload.taskType,\n        availableModels: Object.keys(msg.availableModels),\n        memoryPressure: msg.memoryPressure\n    };\n    return [null, msg];\n}\n\nmsg.selectedModel = selection.model;\nmsg.selectionInfo = selection;\n\n// Prepare response for model selection endpoint\nif (msg.req.originalUrl.includes('/select-model')) {\n    msg.payload = {\n        requestId: msg.requestId,\n        selectedModel: selection.model,\n        taskType: msg.payload.taskType,\n        fallbackUsed: selection.fallback || false,\n        alternatives: selection.alternatives || [],\n        systemMetrics: {\n            memoryPressure: msg.memoryPressure,\n            availableMemoryGb: msg.systemMetrics.memory.available_gb,\n            cpuUsage: msg.systemMetrics.cpu.usage\n        },\n        modelInfo: msg.modelCapabilities.models[selection.model] || null\n    };\n    return [null, msg]; // Go to response\n}\n\n// For route requests, continue to execution\nreturn [msg, null];",
    "outputs": 2,
    "noerr": 0,
    "initialize": "",
    "finalize": "",
    "libs": [],
    "x": 1600,
    "y": 150,
    "wires": [["execute-ai-request"], ["http-response-success"]]
  },
  {
    "id": "execute-ai-request",
    "type": "function",
    "z": "main-flow",
    "name": "Prepare Ollama Request",
    "func": "// Prepare request for Ollama\nconst { prompt, requirements } = msg.payload;\n\nmsg.url = 'http://localhost:11434/api/generate';\nmsg.method = 'POST';\nmsg.headers = { 'Content-Type': 'application/json' };\nmsg.payload = {\n    model: msg.selectedModel,\n    prompt: prompt,\n    stream: false,\n    options: {\n        num_predict: requirements.maxTokens || 2048,\n        temperature: requirements.temperature || 0.7,\n        top_p: requirements.topP || 0.9\n    }\n};\n\nreturn msg;",
    "outputs": 1,
    "noerr": 0,\n    "initialize": "",\n    "finalize": "",\n    "libs": [],\n    "x": 1840,\n    "y": 100,\n    "wires": [["execute-ollama-request"]]\n  },\n  {\n    "id": "execute-ollama-request",\n    "type": "http request",\n    "z": "main-flow",\n    "name": "Execute Ollama Request",\n    "method": "use",\n    "ret": "obj",\n    "paytoqs": "ignore",\n    "url": "",\n    "tls": "",\n    "persist": false,\n    "proxy": "",\n    "authType": "",\n    "senderr": false,\n    "x": 2080,\n    "y": 100,\n    "wires": [["process-ai-response"], ["handle-ai-error"]]\n  },\n  {\n    "id": "process-ai-response",\n    "type": "function",\n    "z": "main-flow",\n    "name": "Process AI Response",\n    "func": "// Process successful AI response\nconst responseTime = Date.now() - msg.startTime;\nconst ollamaResponse = msg.payload;\n\n// Prepare final response\nmsg.payload = {\n    requestId: msg.requestId,\n    selectedModel: msg.selectedModel,\n    response: ollamaResponse.response,\n    fallbackUsed: msg.selectionInfo.fallback || false,\n    metrics: {\n        responseTimeMs: responseTime,\n        memoryPressure: msg.memoryPressure,\n        modelUsed: msg.selectedModel,\n        tokensGenerated: ollamaResponse.eval_count || 0,\n        promptTokens: ollamaResponse.prompt_eval_count || 0\n    }\n};\n\n// Store metrics for analytics\nmsg.metrics = {\n    requestId: msg.requestId,\n    selectedModel: msg.selectedModel,\n    responseTime: responseTime,\n    success: true,\n    memoryPressure: msg.memoryPressure\n};\n\nreturn msg;",\n    "outputs": 1,\n    "noerr": 0,\n    "initialize": "",\n    "finalize": "",\n    "libs": [],\n    "x": 2320,\n    "y": 100,\n    "wires": [["store-metrics"]]\n  },\n  {\n    "id": "handle-ai-error",\n    "type": "function",\n    "z": "main-flow",\n    "name": "Handle AI Error",\n    "func": "// Handle AI request errors with potential fallback\nconst responseTime = Date.now() - msg.startTime;\nconst error = msg.payload;\n\n// Check if we should try fallback model\nif (!msg.fallbackAttempted && msg.selectionInfo.alternatives && msg.selectionInfo.alternatives.length > 0) {\n    msg.fallbackAttempted = true;\n    msg.selectedModel = msg.selectionInfo.alternatives[0];\n    \n    // Retry with fallback model\n    msg.url = 'http://localhost:11434/api/generate';\n    msg.method = 'POST';\n    msg.headers = { 'Content-Type': 'application/json' };\n    msg.payload = {\n        model: msg.selectedModel,\n        prompt: msg.originalPayload.prompt,\n        stream: false,\n        options: {\n            num_predict: msg.originalPayload.requirements.maxTokens || 2048\n        }\n    };\n    \n    return [msg, null]; // Retry\n}\n\n// No more fallbacks available\nmsg.statusCode = 500;\nmsg.payload = {\n    error: 'AI request failed',\n    requestId: msg.requestId,\n    selectedModel: msg.selectedModel,\n    message: error.message || 'Unknown error',\n    fallbackAttempted: msg.fallbackAttempted || false\n};\n\n// Store error metrics\nmsg.metrics = {\n    requestId: msg.requestId,\n    selectedModel: msg.selectedModel,\n    responseTime: responseTime,\n    success: false,\n    error: error.message,\n    memoryPressure: msg.memoryPressure\n};\n\nreturn [null, msg]; // Go to error response",\n    "outputs": 2,\n    "noerr": 0,\n    "initialize": "",\n    "finalize": "",\n    "libs": [],\n    "x": 2320,\n    "y": 150,\n    "wires": [["execute-ollama-request"], ["store-metrics"]]\n  },\n  {\n    "id": "store-metrics",\n    "type": "function",\n    "z": "main-flow",\n    "name": "Store Metrics",\n    "func": "// Store metrics in Redis for analytics\nif (msg.metrics) {\n    // Publish metrics event\n    const metricsEvent = {\n        type: 'orchestrator_metrics',\n        timestamp: new Date().toISOString(),\n        ...msg.metrics\n    };\n    \n    // Store in context for Redis publishing (would need Redis node)\n    context.set('lastMetrics', metricsEvent);\n    \n    // Log metrics\n    node.log(`Request ${msg.metrics.requestId}: ${msg.metrics.selectedModel} - ${msg.metrics.responseTime}ms - ${msg.metrics.success ? 'SUCCESS' : 'ERROR'}`);\n}\n\nreturn msg;",\n    "outputs": 1,\n    "noerr": 0,\n    "initialize": "",\n    "finalize": "",\n    "libs": [],\n    "x": 2560,\n    "y": 125,\n    "wires": [["http-response-success"]]\n  },\n  {\n    "id": "http-response-success",\n    "type": "http response",\n    "z": "main-flow",\n    "name": "Success Response",\n    "statusCode": "200",\n    "headers": {},\n    "x": 2780,\n    "y": 125,\n    "wires": []\n  },\n  {\n    "id": "http-response-error",\n    "type": "http response",\n    "z": "main-flow",\n    "name": "Error Response", \n    "statusCode": "",\n    "headers": {},\n    "x": 2780,\n    "y": 200,\n    "wires": []\n  },\n  {\n    "id": "main-flow",\n    "type": "tab",\n    "label": "AI Model Orchestra Controller",\n    "disabled": false,\n    "info": "Main orchestration flow for intelligent AI model routing and load balancing\\n\\nFeatures:\\n- Intelligent model selection based on task type and requirements\\n- Resource-aware routing (memory, CPU)\\n- Automatic fallback handling\\n- Performance metrics collection\\n- Health monitoring integration\\n\\nEndpoints:\\n- POST /flow/select-model - Select optimal model for task\\n- POST /flow/route-request - Full AI request routing with execution"\n  }\n]