-- Resource Experimenter Seed Data
-- Auto-generated by Resource Experimenter

-- Insert default resource templates
INSERT INTO resource_templates (name, type, description, docker_compose_template, env_template, success_rate, times_used, applicable_to) VALUES
('ai-llm-service', 'ai', 'Template for Large Language Model API services', 
 'version: ''3.8''
services:
  {{resource_name}}:
    image: {{docker_image}}
    ports:
      - "${port}:{{internal_port}}"
    environment:
      - MODEL_NAME=${MODEL_NAME:-default}
    volumes:
      - ./data:/data
    restart: unless-stopped', 
 '{"MODEL_NAME": "default", "API_KEY": "generated", "MAX_TOKENS": "4096"}', 
 0.85, 0, ARRAY['ollama', 'text-generation-webui', 'oobabooga']),

('automation-workflow', 'automation', 'Template for workflow automation engines',
 'version: ''3.8''
services:
  {{resource_name}}:
    image: {{docker_image}}
    ports:
      - "${port}:{{internal_port}}"
    environment:
      - NODE_ENV=production
      - DB_TYPE=sqlite
    volumes:
      - ./data:/data
      - ./workflows:/workflows
    restart: unless-stopped',
 '{"NODE_ENV": "production", "DB_TYPE": "sqlite", "EXECUTIONS_PROCESS": "main"}',
 0.78, 0, ARRAY['n8n', 'nodered', 'huginn']),

('storage-database', 'storage', 'Template for database services',
 'version: ''3.8''
services:
  {{resource_name}}:
    image: {{docker_image}}
    ports:
      - "${port}:{{internal_port}}"
    environment:
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_DB=${DB_NAME}
    volumes:
      - ./data:/var/lib/postgresql/data
    restart: unless-stopped',
 '{"DB_USER": "vrooli", "DB_PASSWORD": "generated", "DB_NAME": "vrooli_db"}',
 0.92, 0, ARRAY['postgres', 'mysql', 'mariadb', 'mongodb']),

('monitoring-metrics', 'monitoring', 'Template for metrics collection services',
 'version: ''3.8''
services:
  {{resource_name}}:
    image: {{docker_image}}
    ports:
      - "${port}:{{internal_port}}"
    environment:
      - STORAGE_RETENTION=${RETENTION_PERIOD}
    volumes:
      - ./data:/prometheus
      - ./config:/etc/prometheus
    restart: unless-stopped',
 '{"RETENTION_PERIOD": "15d", "STORAGE_RETENTION_SIZE": "10GB"}',
 0.88, 0, ARRAY['prometheus', 'influxdb', 'grafana']),

('search-engine', 'search', 'Template for search engine services',
 'version: ''3.8''
services:
  {{resource_name}}:
    image: {{docker_image}}
    ports:
      - "${port}:{{internal_port}}"
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xmx1g -Xms1g
    volumes:
      - ./data:/usr/share/elasticsearch/data
    restart: unless-stopped',
 '{"discovery.type": "single-node", "ES_JAVA_OPTS": "-Xmx1g -Xms1g"}',
 0.75, 0, ARRAY['elasticsearch', 'opensearch', 'meilisearch', 'typesense']),

('execution-runner', 'execution', 'Template for code execution services',
 'version: ''3.8''
services:
  {{resource_name}}:
    image: {{docker_image}}
    ports:
      - "${port}:{{internal_port}}"
    environment:
      - ENABLE_BATCHED_SUBMISSIONS=true
    volumes:
      - /tmp:/tmp
    restart: unless-stopped
    privileged: true',
 '{"ENABLE_BATCHED_SUBMISSIONS": "true", "ENABLE_WAIT_RESULT": "true"}',
 0.70, 0, ARRAY['judge0', 'code-server', 'coderunner']);

-- Insert sample discovery sources (these will be replaced by actual configuration)
INSERT INTO discovery_queue (name, source_url, type, priority, discovered_by, tags, processed, notes) VALUES
('ollama', 'https://hub.docker.com/r/ollama/ollama', 'ai', 9, 'manual', ARRAY['ai', 'llm', 'local'], false, 'Popular local LLM runtime'),
('n8n', 'https://hub.docker.com/r/n8nio/n8n', 'automation', 8, 'manual', ARRAY['automation', 'workflow', 'nocode'], false, 'Workflow automation platform'),
('grafana', 'https://hub.docker.com/r/grafana/grafana', 'monitoring', 8, 'manual', ARRAY['monitoring', 'visualization', 'dashboards'], false, 'Monitoring and observability platform'),
('meilisearch', 'https://hub.docker.com/r/getmeili/meilisearch', 'search', 7, 'manual', ARRAY['search', 'full-text', 'fast'], false, 'Fast, typo-tolerant search engine'),
('redis', 'https://hub.docker.com/_/redis', 'storage', 9, 'manual', ARRAY['cache', 'database', 'inmemory'], false, 'In-memory data structure store'),
('minio', 'https://hub.docker.com/r/minio/minio', 'storage', 8, 'manual', ARRAY['storage', 's3-compatible', 'object-storage'], false, 'High Performance Object Storage'),
('jupyter', 'https://hub.docker.com/r/jupyter/base-notebook', 'execution', 7, 'manual', ARRAY['notebook', 'python', 'data-science'], false, 'Jupyter notebook server'),
('nextcloud', 'https://hub.docker.com/_/nextcloud', 'other', 6, 'manual', ARRAY['cloud', 'files', 'collaboration'], false, 'Self-hosted cloud platform'),
('gitea', 'https://hub.docker.com/r/gitea/gitea', 'automation', 7, 'manual', ARRAY['git', 'repository', 'devops'], false, 'Git service with web interface'),
('wordpress', 'https://hub.docker.com/_/wordpress', 'other', 5, 'manual', ARRAY['cms', 'blog', 'website'], false, 'Content management system');

-- Insert some sample integration patterns
INSERT INTO integration_attempts (resource_id, version, status, integration_approach, files_changed, lines_added, reviewed_by, approved)
SELECT 
    uuid_generate_v4() as resource_id,
    '1.0.0' as version,
    'successful' as status,
    'Automated generation via resource-experimenter' as integration_approach,
    ARRAY['Generated complete resource provider structure'] as files_changed,
    (random() * 500 + 100)::integer as lines_added,
    'resource-experimenter-bot' as reviewed_by,
    true as approved
FROM generate_series(1, 5);

-- Create some example experiment test results
INSERT INTO experiment_tests (experiment_id, test_name, test_type, result, execution_time_ms, executed_at)
SELECT 
    uuid_generate_v4() as experiment_id,
    'health-check-' || generate_series as test_name,
    'connectivity' as test_type,
    CASE WHEN random() > 0.2 THEN 'pass' ELSE 'fail' END as result,
    (random() * 5000 + 100)::integer as execution_time_ms,
    NOW() - (random() * interval '7 days') as executed_at
FROM generate_series(1, 20);

INSERT INTO experiment_tests (experiment_id, test_name, test_type, result, execution_time_ms, executed_at)
SELECT 
    uuid_generate_v4() as experiment_id,
    'api-test-' || generate_series as test_name,
    'api' as test_type,
    CASE WHEN random() > 0.3 THEN 'pass' ELSE 'fail' END as result,
    (random() * 3000 + 200)::integer as execution_time_ms,
    NOW() - (random() * interval '7 days') as executed_at
FROM generate_series(1, 15);

INSERT INTO experiment_tests (experiment_id, test_name, test_type, result, execution_time_ms, executed_at)
SELECT 
    uuid_generate_v4() as experiment_id,
    'integration-test-' || generate_series as test_name,
    'integration' as test_type,
    CASE WHEN random() > 0.4 THEN 'pass' ELSE 'fail' END as result,
    (random() * 10000 + 500)::integer as execution_time_ms,
    NOW() - (random() * interval '7 days') as executed_at
FROM generate_series(1, 10);

-- Update template usage counts based on dummy data
UPDATE resource_templates SET 
    times_used = (random() * 10)::integer,
    success_rate = 0.7 + (random() * 0.25);

-- Create some sample sandboxes (these would normally be created during experiments)
INSERT INTO sandboxes (experiment_id, sandbox_path, docker_network, port_range_start, port_range_end, active, created_at)
SELECT 
    uuid_generate_v4() as experiment_id,
    '/app/sandbox/experiment-' || generate_series as sandbox_path,
    'vrooli-network' as docker_network,
    9000 + (generate_series * 10) as port_range_start,
    9000 + (generate_series * 10) + 10 as port_range_end,
    false as active,  -- All sample sandboxes are inactive
    NOW() - (random() * interval '30 days') as created_at
FROM generate_series(1, 5);

-- Add some realistic discovery queue items with AI-like categorization
INSERT INTO discovery_queue (name, source_url, type, priority, discovered_by, tags, processed, notes) VALUES
('portainer', 'https://hub.docker.com/r/portainer/portainer-ce', 'monitoring', 7, 'crawler', ARRAY['docker', 'management', 'ui'], false, 'Docker container management UI'),
('traefik', 'https://hub.docker.com/_/traefik', 'other', 8, 'crawler', ARRAY['proxy', 'loadbalancer', 'routing'], false, 'Cloud native application proxy'),
('ghost', 'https://hub.docker.com/_/ghost', 'other', 6, 'crawler', ARRAY['blog', 'cms', 'publishing'], false, 'Professional publishing platform'),
('code-server', 'https://hub.docker.com/r/codercom/code-server', 'execution', 7, 'crawler', ARRAY['ide', 'vscode', 'remote'], false, 'VS Code in the browser'),
('bitwarden', 'https://hub.docker.com/r/bitwarden/self-host', 'other', 8, 'crawler', ARRAY['password', 'security', 'vault'], false, 'Password manager'),
('homeassistant', 'https://hub.docker.com/r/homeassistant/home-assistant', 'automation', 6, 'crawler', ARRAY['iot', 'smart-home', 'automation'], false, 'Home automation platform'),
('photoprism', 'https://hub.docker.com/r/photoprism/photoprism', 'other', 5, 'crawler', ARRAY['photos', 'ai', 'gallery'], false, 'AI-powered photo management'),
('uptime-kuma', 'https://hub.docker.com/r/louislam/uptime-kuma', 'monitoring', 7, 'crawler', ARRAY['uptime', 'monitoring', 'status'], false, 'Uptime monitoring tool'),
('paperless-ngx', 'https://hub.docker.com/r/ghcr.io/paperless-ngx/paperless-ngx', 'other', 6, 'crawler', ARRAY['documents', 'scanning', 'ocr'], false, 'Document management system'),
('immich', 'https://hub.docker.com/r/ghcr.io/immich-app/immich-server', 'other', 7, 'crawler', ARRAY['photos', 'backup', 'self-hosted'], false, 'Self-hosted photo and video backup solution');

-- Add creation timestamps to older records if missing
UPDATE discovery_queue SET discovered_at = NOW() - (random() * interval '30 days') 
WHERE discovered_at IS NULL;

-- Create indexes for better query performance
CREATE INDEX IF NOT EXISTS idx_discovery_queue_priority_date ON discovery_queue(priority DESC, discovered_at ASC);
CREATE INDEX IF NOT EXISTS idx_experiment_tests_executed_at ON experiment_tests(executed_at DESC);
CREATE INDEX IF NOT EXISTS idx_integration_attempts_created_at ON integration_attempts(created_at DESC);

-- Add some helpful views for common queries
CREATE OR REPLACE VIEW discovery_stats AS
SELECT 
    type,
    COUNT(*) as total_discovered,
    COUNT(*) FILTER (WHERE processed = false) as pending,
    COUNT(*) FILTER (WHERE processed = true) as processed,
    AVG(priority) as avg_priority,
    MAX(discovered_at) as last_discovery
FROM discovery_queue
GROUP BY type
ORDER BY avg_priority DESC;

CREATE OR REPLACE VIEW test_results_summary AS
SELECT 
    test_type,
    COUNT(*) as total_tests,
    COUNT(*) FILTER (WHERE result = 'pass') as passed,
    COUNT(*) FILTER (WHERE result = 'fail') as failed,
    COUNT(*) FILTER (WHERE result = 'skip') as skipped,
    AVG(execution_time_ms) as avg_execution_time_ms,
    MAX(executed_at) as last_test_run
FROM experiment_tests
GROUP BY test_type
ORDER BY total_tests DESC;

CREATE OR REPLACE VIEW resource_template_usage AS
SELECT 
    name,
    type,
    times_used,
    success_rate,
    CASE 
        WHEN times_used > 10 THEN 'popular'
        WHEN times_used > 5 THEN 'moderate'
        WHEN times_used > 0 THEN 'used'
        ELSE 'unused'
    END as usage_category,
    created_at
FROM resource_templates
ORDER BY times_used DESC, success_rate DESC;

-- Grant appropriate permissions (adjust as needed for your setup)
-- GRANT SELECT ON ALL TABLES IN SCHEMA public TO readonly_user;
-- GRANT SELECT, INSERT, UPDATE ON discovery_queue TO experimenter_user;
-- GRANT SELECT, INSERT, UPDATE ON experiments TO experimenter_user;

-- Log the completion of seed data insertion
INSERT INTO experiment_tests (experiment_id, test_name, test_type, result, executed_at) 
VALUES (uuid_generate_v4(), 'seed-data-initialization', 'setup', 'pass', NOW());

-- Display summary of what was seeded
SELECT 
    'Seed data initialization completed' as status,
    (SELECT COUNT(*) FROM resource_templates) as templates_created,
    (SELECT COUNT(*) FROM discovery_queue) as resources_queued,
    (SELECT COUNT(*) FROM experiment_tests) as test_records_created,
    (SELECT COUNT(*) FROM integration_attempts) as integration_records_created,
    NOW() as completed_at;