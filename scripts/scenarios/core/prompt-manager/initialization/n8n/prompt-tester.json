{
  "name": "Prompt Tester and Evaluator",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "test-prompt",
        "responseMode": "responseNode",
        "options": {}
      },
      "id": "webhook_trigger",
      "name": "Webhook Trigger",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1.1,
      "position": [200, 300]
    },
    {
      "parameters": {},
      "id": "manual_trigger",
      "name": "Manual Trigger",
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [200, 150]
    },
    {
      "parameters": {
        "values": {
          "string": [
            {
              "name": "prompt_id",
              "value": "test-prompt-001"
            },
            {
              "name": "prompt",
              "value": "Generate a marketing tagline for an eco-friendly water bottle"
            },
            {
              "name": "test_cases",
              "value": "[{\"input\":\"plastic waste reduction\",\"expected_keywords\":[\"eco\",\"sustainable\",\"green\"]},{\"input\":\"target millennials\",\"expected_tone\":\"casual\"}]"
            },
            {
              "name": "models",
              "value": "[\"phi3.5:3.8b\",\"llama3.2\"]"
            },
            {
              "name": "iterations",
              "value": "3"
            }
          ]
        },
        "options": {}
      },
      "id": "manual_defaults",
      "name": "Manual Test Defaults",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [450, 150]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Prepare test configuration\nconst input = $input.item.json;\n\n// Parse and validate inputs\nlet testCases = [];\ntry {\n  testCases = typeof input.test_cases === 'string' ? JSON.parse(input.test_cases) : input.test_cases || [];\n} catch (e) {\n  testCases = [];\n}\n\nlet models = [];\ntry {\n  models = typeof input.models === 'string' ? JSON.parse(input.models) : input.models || ['phi3.5:3.8b'];\n} catch (e) {\n  models = ['phi3.5:3.8b'];\n}\n\nconst iterations = parseInt(input.iterations) || 1;\nconst prompt = input.prompt || '';\nconst promptId = input.prompt_id || 'unknown';\n\nif (!prompt) {\n  throw new Error('Prompt is required for testing');\n}\n\n// Create test matrix\nconst testMatrix = [];\nfor (const model of models) {\n  for (let i = 0; i < iterations; i++) {\n    for (const testCase of testCases.length > 0 ? testCases : [{}]) {\n      testMatrix.push({\n        prompt_id: promptId,\n        prompt: prompt,\n        model: model,\n        iteration: i + 1,\n        test_case: testCase,\n        test_id: `${promptId}_${model}_iter${i + 1}_case${testCases.indexOf(testCase) + 1}`,\n        timestamp: new Date().toISOString()\n      });\n    }\n  }\n}\n\nreturn testMatrix;"
      },
      "id": "prepare_tests",
      "name": "Prepare Test Matrix",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [650, 300]
    },
    {
      "parameters": {
        "batchSize": 1,
        "batchInterval": 500,
        "options": {}
      },
      "id": "split_batches",
      "name": "Split Into Batches",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [850, 300]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Build prompt for testing\nconst test = $input.item.json;\n\n// Construct the full prompt with test case context\nlet fullPrompt = test.prompt;\n\nif (test.test_case && test.test_case.input) {\n  fullPrompt = `${test.prompt}\\n\\nAdditional context: ${test.test_case.input}`;\n}\n\nreturn {\n  request: {\n    prompt: fullPrompt,\n    model: test.model,\n    type: 'reasoning',\n    quiet: true,\n    timeout_seconds: 30\n  },\n  test_metadata: test\n};"
      },
      "id": "build_test_request",
      "name": "Build Test Request",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [1050, 300]
    },
    {
      "parameters": {
        "url": "http://localhost:5678/webhook/ollama",
        "method": "POST",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify($json.request) }}",
        "options": {
          "timeout": 30000
        }
      },
      "id": "execute_test",
      "name": "Execute Test with Ollama",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [1250, 300],
      "continueOnFail": true
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Evaluate test results\nconst ollamaResponse = $input.item.json;\nconst testData = $('Build Test Request').item.json.test_metadata;\n\nconst result = {\n  test_id: testData.test_id,\n  prompt_id: testData.prompt_id,\n  model: testData.model,\n  iteration: testData.iteration,\n  test_case: testData.test_case,\n  timestamp: testData.timestamp,\n  completed_at: new Date().toISOString()\n};\n\nif (ollamaResponse.success) {\n  const response = ollamaResponse.response || '';\n  \n  // Basic metrics\n  result.response = response;\n  result.response_length = response.length;\n  result.response_time_ms = ollamaResponse.execution?.execution_time_ms || 0;\n  result.success = true;\n  \n  // Evaluate against test case expectations\n  const evaluation = {\n    passed: true,\n    checks: []\n  };\n  \n  if (testData.test_case) {\n    // Check for expected keywords\n    if (testData.test_case.expected_keywords) {\n      const keywords = testData.test_case.expected_keywords;\n      const foundKeywords = keywords.filter(kw => \n        response.toLowerCase().includes(kw.toLowerCase())\n      );\n      \n      evaluation.checks.push({\n        type: 'keywords',\n        expected: keywords,\n        found: foundKeywords,\n        passed: foundKeywords.length > 0,\n        score: foundKeywords.length / keywords.length\n      });\n      \n      if (foundKeywords.length === 0) {\n        evaluation.passed = false;\n      }\n    }\n    \n    // Check for expected tone\n    if (testData.test_case.expected_tone) {\n      const toneIndicators = {\n        casual: ['hey', 'cool', 'awesome', '!', 'you\\'ll', 'let\\'s'],\n        formal: ['therefore', 'furthermore', 'consequently', 'shall', 'would'],\n        technical: ['algorithm', 'implementation', 'architecture', 'interface', 'protocol'],\n        friendly: ['welcome', 'happy', 'glad', 'please', 'thank you', 'ðŸ˜Š']\n      };\n      \n      const expectedIndicators = toneIndicators[testData.test_case.expected_tone] || [];\n      const foundIndicators = expectedIndicators.filter(ind => \n        response.toLowerCase().includes(ind.toLowerCase())\n      );\n      \n      evaluation.checks.push({\n        type: 'tone',\n        expected: testData.test_case.expected_tone,\n        indicators_found: foundIndicators.length,\n        passed: foundIndicators.length > 0,\n        score: foundIndicators.length > 0 ? 1 : 0\n      });\n    }\n    \n    // Check response length expectations\n    if (testData.test_case.max_length) {\n      const withinLimit = response.length <= testData.test_case.max_length;\n      evaluation.checks.push({\n        type: 'length',\n        max_expected: testData.test_case.max_length,\n        actual: response.length,\n        passed: withinLimit,\n        score: withinLimit ? 1 : 0\n      });\n      \n      if (!withinLimit) {\n        evaluation.passed = false;\n      }\n    }\n  }\n  \n  // Calculate overall score\n  if (evaluation.checks.length > 0) {\n    const totalScore = evaluation.checks.reduce((sum, check) => sum + (check.score || 0), 0);\n    evaluation.overall_score = totalScore / evaluation.checks.length;\n  } else {\n    evaluation.overall_score = 1; // No specific checks, so consider it passed\n  }\n  \n  result.evaluation = evaluation;\n  \n  // Add quality metrics\n  result.quality_metrics = {\n    has_punctuation: /[.!?]/.test(response),\n    has_structure: response.includes('\\n') || response.length > 100,\n    word_count: response.split(/\\s+/).length,\n    unique_words: new Set(response.toLowerCase().split(/\\s+/)).size,\n    avg_word_length: response.replace(/\\s+/g, '').length / response.split(/\\s+/).length\n  };\n  \n} else {\n  result.success = false;\n  result.error = ollamaResponse.error || 'Test execution failed';\n  result.evaluation = {\n    passed: false,\n    overall_score: 0,\n    checks: []\n  };\n}\n\nreturn result;"
      },
      "id": "evaluate_results",
      "name": "Evaluate Results",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [1450, 300]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO prompt_test_results (test_id, prompt_id, model, iteration, test_case, response, response_time_ms, evaluation, quality_metrics, success, created_at) VALUES ('{{ $json.test_id }}', '{{ $json.prompt_id }}', '{{ $json.model }}', {{ $json.iteration }}, '{{ JSON.stringify($json.test_case) }}', '{{ $json.response }}', {{ $json.response_time_ms }}, '{{ JSON.stringify($json.evaluation) }}', '{{ JSON.stringify($json.quality_metrics) }}', {{ $json.success }}, '{{ $json.completed_at }}') ON CONFLICT (test_id) DO UPDATE SET response = EXCLUDED.response, evaluation = EXCLUDED.evaluation, quality_metrics = EXCLUDED.quality_metrics",
        "additionalFields": {}
      },
      "id": "store_results",
      "name": "Store Test Results",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [1650, 300],
      "continueOnFail": true
    },
    {
      "parameters": {
        "aggregate": "aggregateAllItemData",
        "destinationFieldName": "all_results",
        "options": {}
      },
      "id": "aggregate_results",
      "name": "Aggregate All Results",
      "type": "n8n-nodes-base.aggregate",
      "typeVersion": 1,
      "position": [1850, 300]
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Generate comprehensive test report\nconst allResults = $input.item.json.all_results || [];\n\nif (allResults.length === 0) {\n  return {\n    success: false,\n    error: 'No test results to analyze'\n  };\n}\n\n// Group results by model\nconst modelGroups = {};\nallResults.forEach(result => {\n  if (!modelGroups[result.model]) {\n    modelGroups[result.model] = [];\n  }\n  modelGroups[result.model].push(result);\n});\n\n// Calculate statistics for each model\nconst modelStats = {};\nfor (const [model, results] of Object.entries(modelGroups)) {\n  const successfulTests = results.filter(r => r.success);\n  const avgResponseTime = successfulTests.reduce((sum, r) => sum + (r.response_time_ms || 0), 0) / successfulTests.length;\n  const avgScore = successfulTests.reduce((sum, r) => sum + (r.evaluation?.overall_score || 0), 0) / successfulTests.length;\n  const passRate = successfulTests.filter(r => r.evaluation?.passed).length / results.length;\n  \n  modelStats[model] = {\n    total_tests: results.length,\n    successful_tests: successfulTests.length,\n    failed_tests: results.length - successfulTests.length,\n    avg_response_time_ms: Math.round(avgResponseTime),\n    avg_score: avgScore.toFixed(2),\n    pass_rate: (passRate * 100).toFixed(1) + '%',\n    avg_response_length: Math.round(successfulTests.reduce((sum, r) => sum + (r.response_length || 0), 0) / successfulTests.length),\n    avg_word_count: Math.round(successfulTests.reduce((sum, r) => sum + (r.quality_metrics?.word_count || 0), 0) / successfulTests.length)\n  };\n}\n\n// Find best performing model\nlet bestModel = null;\nlet bestScore = -1;\nfor (const [model, stats] of Object.entries(modelStats)) {\n  if (parseFloat(stats.avg_score) > bestScore) {\n    bestScore = parseFloat(stats.avg_score);\n    bestModel = model;\n  }\n}\n\n// Generate consistency analysis\nconst responseVariations = {};\nallResults.forEach(result => {\n  if (result.success && result.response) {\n    const key = `${result.prompt_id}_${result.test_case?.input || 'default'}`;\n    if (!responseVariations[key]) {\n      responseVariations[key] = [];\n    }\n    responseVariations[key].push({\n      model: result.model,\n      iteration: result.iteration,\n      response: result.response.substring(0, 100) // First 100 chars for comparison\n    });\n  }\n});\n\n// Check consistency\nconst consistencyScores = {};\nfor (const [key, variations] of Object.entries(responseVariations)) {\n  const uniqueResponses = new Set(variations.map(v => v.response.toLowerCase().trim()));\n  consistencyScores[key] = {\n    variation_count: uniqueResponses.size,\n    total_tests: variations.length,\n    consistency_score: ((variations.length - uniqueResponses.size + 1) / variations.length).toFixed(2)\n  };\n}\n\n// Generate final report\nconst report = {\n  success: true,\n  test_summary: {\n    total_tests_run: allResults.length,\n    successful_tests: allResults.filter(r => r.success).length,\n    failed_tests: allResults.filter(r => !r.success).length,\n    models_tested: Object.keys(modelGroups).length,\n    unique_test_cases: new Set(allResults.map(r => JSON.stringify(r.test_case))).size,\n    test_duration_ms: Math.max(...allResults.map(r => new Date(r.completed_at) - new Date(r.timestamp)))\n  },\n  model_performance: modelStats,\n  best_performing_model: {\n    model: bestModel,\n    score: bestScore.toFixed(2),\n    stats: modelStats[bestModel]\n  },\n  consistency_analysis: consistencyScores,\n  recommendations: []\n};\n\n// Generate recommendations\nif (bestModel) {\n  report.recommendations.push(`Use ${bestModel} for best overall performance (score: ${bestScore.toFixed(2)})`);\n}\n\nfor (const [model, stats] of Object.entries(modelStats)) {\n  if (stats.avg_response_time_ms < 1000 && parseFloat(stats.avg_score) > 0.7) {\n    report.recommendations.push(`${model} offers good balance of speed and quality`);\n  }\n  if (parseFloat(stats.pass_rate) === 100) {\n    report.recommendations.push(`${model} passed all test cases successfully`);\n  }\n}\n\n// Add timestamp\nreport.generated_at = new Date().toISOString();\nreport.prompt_id = allResults[0]?.prompt_id || 'unknown';\n\nreturn report;"
      },
      "id": "generate_report",
      "name": "Generate Test Report",
      "type": "n8n-nodes-base.code",
      "typeVersion": 1,
      "position": [2050, 300]
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "respond_webhook",
      "name": "Respond to Webhook",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.1,
      "position": [2250, 300]
    }
  ],
  "connections": {
    "Manual Trigger": {
      "main": [
        [
          {
            "node": "Manual Test Defaults",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Manual Test Defaults": {
      "main": [
        [
          {
            "node": "Prepare Test Matrix",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Webhook Trigger": {
      "main": [
        [
          {
            "node": "Prepare Test Matrix",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Test Matrix": {
      "main": [
        [
          {
            "node": "Split Into Batches",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split Into Batches": {
      "main": [
        [
          {
            "node": "Build Test Request",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Test Request": {
      "main": [
        [
          {
            "node": "Execute Test with Ollama",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Execute Test with Ollama": {
      "main": [
        [
          {
            "node": "Evaluate Results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Evaluate Results": {
      "main": [
        [
          {
            "node": "Store Test Results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Store Test Results": {
      "main": [
        [
          {
            "node": "Split Into Batches",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Aggregate All Results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Aggregate All Results": {
      "main": [
        [
          {
            "node": "Generate Test Report",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Generate Test Report": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "prompt-tester-v1",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "prompt-manager"
  },
  "id": "prompt-tester",
  "tags": []
}